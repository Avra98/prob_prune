Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/7]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/200 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.4999e-06, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302984
Average KL loss: 17.598898
Average total loss: 19.901882
tensor(-3.5300, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.7670e-07, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302735
Average KL loss: 1.928126
Average total loss: 4.230861
tensor(-4.5024, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.0861e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302676
Average KL loss: 0.944790
Average total loss: 3.247466
tensor(-5.0321, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(6.4599e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302653
Average KL loss: 0.599491
Average total loss: 2.902144
tensor(-5.4216, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.3834e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302673
Average KL loss: 0.423229
Average total loss: 2.725902
tensor(-5.7320, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.2280e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302593
Average KL loss: 0.318702
Average total loss: 2.621295
tensor(-5.9907, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.4898e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302605
Average KL loss: 0.250731
Average total loss: 2.553336
tensor(-6.2129, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.9996e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302624
Average KL loss: 0.203578
Average total loss: 2.506202
tensor(-6.4080, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.6487e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302589
Average KL loss: 0.169304
Average total loss: 2.471893
tensor(-6.5820, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.3849e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302606
Average KL loss: 0.143489
Average total loss: 2.446095
tensor(-6.7392, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.1815e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302595
Average KL loss: 0.123466
Average total loss: 2.426061
tensor(-6.8827, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.0264e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302605
Average KL loss: 0.107569
Average total loss: 2.410173
tensor(-7.0149, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(8.9713e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302593
Average KL loss: 0.094708
Average total loss: 2.397301
tensor(-7.1375, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(7.9599e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302571
Average KL loss: 0.084130
Average total loss: 2.386701
tensor(-7.2518, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(7.0981e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302605
Average KL loss: 0.075310
Average total loss: 2.377916
tensor(-7.3590, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(6.3761e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302562
Average KL loss: 0.067862
Average total loss: 2.370424
tensor(-7.4600, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.7590e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302587
Average KL loss: 0.061509
Average total loss: 2.364096
tensor(-7.5556, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.2554e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302595
Average KL loss: 0.056038
Average total loss: 2.358634
tensor(-7.6463, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.7866e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302576
Average KL loss: 0.051290
Average total loss: 2.353865
tensor(-7.7326, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.3890e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302588
Average KL loss: 0.047139
Average total loss: 2.349727
tensor(-7.8151, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.0521e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302591
Average KL loss: 0.043484
Average total loss: 2.346076
tensor(-7.8940, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.7350e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302609
Average KL loss: 0.040248
Average total loss: 2.342857
tensor(-7.9697, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.4627e-09, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302586
Average KL loss: 0.037367
Average total loss: 2.339952
tensor(-8.0425, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.2191e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302590
Average KL loss: 0.034789
Average total loss: 2.337379
tensor(-8.1127, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.0019e-09, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302576
Average KL loss: 0.032472
Average total loss: 2.335048
tensor(-8.1803, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.8036e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302588
Average KL loss: 0.030382
Average total loss: 2.332970
tensor(-8.2458, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.6278e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302579
Average KL loss: 0.028486
Average total loss: 2.331066
tensor(-8.3091, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.4666e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302599
Average KL loss: 0.026764
Average total loss: 2.329363
tensor(-8.3705, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.3197e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.025192
Average total loss: 2.327777
tensor(-8.4302, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.1684e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302584
Average KL loss: 0.023754
Average total loss: 2.326338
tensor(-8.4881, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.0626e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302590
Average KL loss: 0.022434
Average total loss: 2.325024
tensor(-8.5445, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.9495e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302583
Average KL loss: 0.021220
Average total loss: 2.323803
tensor(-8.5994, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8454e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302581
Average KL loss: 0.020100
Average total loss: 2.322681
tensor(-8.6529, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.7492e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.019064
Average total loss: 2.321650
tensor(-8.7051, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.6606e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302584
Average KL loss: 0.018105
Average total loss: 2.320689
tensor(-8.7561, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.5777e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302583
Average KL loss: 0.017214
Average total loss: 2.319797
tensor(-8.8060, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.5009e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302576
Average KL loss: 0.016385
Average total loss: 2.318962
tensor(-8.8548, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.4295e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.015613
Average total loss: 2.318198
tensor(-8.9026, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.3507e-09, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.014891
Average total loss: 2.317477
tensor(-8.9494, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.3005e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302579
Average KL loss: 0.014217
Average total loss: 2.316796
tensor(-8.9953, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.2428e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302586
Average KL loss: 0.013585
Average total loss: 2.316171
tensor(-9.0403, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.1875e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.012993
Average total loss: 2.315578
tensor(-9.0845, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.1362e-09, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302586
Average KL loss: 0.012436
Average total loss: 2.315022
tensor(-9.1279, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.0880e-09, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302590
Average KL loss: 0.011912
Average total loss: 2.314502
tensor(-9.1705, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.0425e-09, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302584
Average KL loss: 0.011419
Average total loss: 2.314003
tensor(-9.2124, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.9974e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.010954
Average total loss: 2.313538
tensor(-9.2537, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.5937e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302582
Average KL loss: 0.010515
Average total loss: 2.313096
tensor(-9.2942, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.2459e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302584
Average KL loss: 0.010100
Average total loss: 2.312684
tensor(-9.3342, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.8628e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302580
Average KL loss: 0.009707
Average total loss: 2.312287
tensor(-9.3736, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.5099e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.009335
Average total loss: 2.311920
tensor(-9.4123, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.1865e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.008982
Average total loss: 2.311567
tensor(-9.4506, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.8793e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.008648
Average total loss: 2.311233
tensor(-9.4883, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.5878e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302584
Average KL loss: 0.008330
Average total loss: 2.310914
tensor(-9.5254, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.3048e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.008028
Average total loss: 2.310613
tensor(-9.5621, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.0474e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.007740
Average total loss: 2.310326
tensor(-9.5984, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.7967e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302589
Average KL loss: 0.007467
Average total loss: 2.310056
tensor(-9.6341, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.5664e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302587
Average KL loss: 0.007206
Average total loss: 2.309793
tensor(-9.6695, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.3301e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302590
Average KL loss: 0.006957
Average total loss: 2.309547
tensor(-9.7044, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.1130e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302586
Average KL loss: 0.006720
Average total loss: 2.309306
tensor(-9.7389, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.9182e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302590
Average KL loss: 0.006493
Average total loss: 2.309083
tensor(-9.7730, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.7070e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.006277
Average total loss: 2.308861
tensor(-9.8067, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.5185e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.006069
Average total loss: 2.308654
tensor(-9.8401, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.3374e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302588
Average KL loss: 0.005871
Average total loss: 2.308459
tensor(-9.8731, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.1642e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302586
Average KL loss: 0.005682
Average total loss: 2.308267
tensor(-9.9057, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.9982e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.005500
Average total loss: 2.308085
tensor(-9.9380, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.8393e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302586
Average KL loss: 0.005326
Average total loss: 2.307912
tensor(-9.9700, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.6869e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.005159
Average total loss: 2.307744
tensor(-10.0017, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.5407e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.004999
Average total loss: 2.307584
tensor(-10.0331, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.4005e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302580
Average KL loss: 0.004845
Average total loss: 2.307425
tensor(-10.0642, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.2658e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302581
Average KL loss: 0.004698
Average total loss: 2.307279
tensor(-10.0949, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.1365e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302584
Average KL loss: 0.004556
Average total loss: 2.307140
tensor(-10.1255, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.0122e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.004419
Average total loss: 2.307004
tensor(-10.1557, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.8927e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.004288
Average total loss: 2.306873
tensor(-10.1856, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.7778e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.004162
Average total loss: 2.306747
tensor(-10.2154, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.6673e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.004041
Average total loss: 2.306626
tensor(-10.2448, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.5608e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302584
Average KL loss: 0.003924
Average total loss: 2.306508
tensor(-10.2740, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.4585e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302583
Average KL loss: 0.003812
Average total loss: 2.306394
tensor(-10.3030, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.3597e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.003703
Average total loss: 2.306288
tensor(-10.3317, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.2645e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302583
Average KL loss: 0.003599
Average total loss: 2.306182
tensor(-10.3602, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.1730e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302587
Average KL loss: 0.003498
Average total loss: 2.306086
tensor(-10.3885, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.0843e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.003401
Average total loss: 2.305986
tensor(-10.4165, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.9990e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.003307
Average total loss: 2.305892
tensor(-10.4443, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.9166e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.003217
Average total loss: 2.305802
tensor(-10.4720, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.8372e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302583
Average KL loss: 0.003129
Average total loss: 2.305713
tensor(-10.4994, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.7604e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.003045
Average total loss: 2.305630
tensor(-10.5266, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.6864e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302584
Average KL loss: 0.002964
Average total loss: 2.305548
tensor(-10.5536, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.6148e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302583
Average KL loss: 0.002885
Average total loss: 2.305468
tensor(-10.5804, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.5457e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302584
Average KL loss: 0.002809
Average total loss: 2.305393
tensor(-10.6070, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.4793e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302590
Average KL loss: 0.002735
Average total loss: 2.305325
tensor(-10.6335, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.4141e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302590
Average KL loss: 0.002664
Average total loss: 2.305254
tensor(-10.6597, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.3515e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.002595
Average total loss: 2.305180
tensor(-10.6858, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.2910e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.002529
Average total loss: 2.305114
tensor(-10.7117, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.2325e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302584
Average KL loss: 0.002465
Average total loss: 2.305049
tensor(-10.7374, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.1760e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302583
Average KL loss: 0.002402
Average total loss: 2.304985
tensor(-10.7629, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.1211e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302586
Average KL loss: 0.002342
Average total loss: 2.304928
tensor(-10.7883, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.0680e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302584
Average KL loss: 0.002284
Average total loss: 2.304868
tensor(-10.8134, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.0577e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302584
Average KL loss: 0.002227
Average total loss: 2.304811
tensor(-10.8385, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.9667e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302588
Average KL loss: 0.002172
Average total loss: 2.304760
tensor(-10.8633, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.9183e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302589
Average KL loss: 0.002119
Average total loss: 2.304708
tensor(-10.8880, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.8714e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304653
tensor(-10.9125, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.8261e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302581
Average KL loss: 0.002017
Average total loss: 2.304598
tensor(-10.9369, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7827e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302590
Average KL loss: 0.001969
Average total loss: 2.304559
tensor(-10.9611, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7396e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.001922
Average total loss: 2.304507
tensor(-10.9852, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6982e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302586
Average KL loss: 0.001876
Average total loss: 2.304462
tensor(-11.0091, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6580e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302589
Average KL loss: 0.001832
Average total loss: 2.304421
tensor(-11.0329, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6190e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.001789
Average total loss: 2.304374
tensor(-11.0565, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5812e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.001747
Average total loss: 2.304332
tensor(-11.0799, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5446e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.001707
Average total loss: 2.304292
tensor(-11.1032, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5090e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-11.1264, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4744e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.001630
Average total loss: 2.304215
tensor(-11.1494, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4409e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.001593
Average total loss: 2.304178
tensor(-11.1722, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4083e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.001557
Average total loss: 2.304142
tensor(-11.1950, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3766e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-11.2175, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3459e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.001488
Average total loss: 2.304073
tensor(-11.2400, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3162e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.001455
Average total loss: 2.304040
tensor(-11.2623, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2870e-10, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302582
Average KL loss: 0.001423
Average total loss: 2.304005
tensor(-11.2844, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2588e-10, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302582
Average KL loss: 0.001392
Average total loss: 2.303975
tensor(-11.3064, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2314e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.001362
Average total loss: 2.303947
tensor(-11.3283, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2047e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.001332
Average total loss: 2.303917
tensor(-11.3501, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1788e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.001304
Average total loss: 2.303889
tensor(-11.3717, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1536e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302586
Average KL loss: 0.001276
Average total loss: 2.303862
tensor(-11.3931, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1291e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.001249
Average total loss: 2.303834
tensor(-11.4145, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1052e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.001223
Average total loss: 2.303808
tensor(-11.4356, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0821e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302588
Average KL loss: 0.001197
Average total loss: 2.303785
tensor(-11.4567, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0595e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302584
Average KL loss: 0.001172
Average total loss: 2.303757
tensor(-11.4776, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0376e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.001148
Average total loss: 2.303733
tensor(-11.4984, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.0162e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-11.5191, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.9540e-11, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.001102
Average total loss: 2.303686
tensor(-11.5396, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.7516e-11, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.001079
Average total loss: 2.303664
tensor(-11.5600, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.5546e-11, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303642
tensor(-11.5803, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.3628e-11, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.001036
Average total loss: 2.303621
tensor(-11.6004, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.1760e-11, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.001016
Average total loss: 2.303601
tensor(-11.6204, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.9942e-11, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302580
Average KL loss: 0.000996
Average total loss: 2.303576
tensor(-11.6403, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.8174e-11, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302589
Average KL loss: 0.000976
Average total loss: 2.303565
tensor(-11.6601, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.6446e-11, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.000957
Average total loss: 2.303542
tensor(-11.6797, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.4765e-11, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303523
tensor(-11.6992, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.3128e-11, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.000921
Average total loss: 2.303505
tensor(-11.7185, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1533e-11, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.000911
Average total loss: 2.303496
tensor(-11.7205, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1376e-11, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302584
Average KL loss: 0.000909
Average total loss: 2.303493
tensor(-11.7224, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1217e-11, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.000907
Average total loss: 2.303492
tensor(-11.7244, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1057e-11, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.000905
Average total loss: 2.303490
tensor(-11.7264, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0895e-11, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.000903
Average total loss: 2.303488
tensor(-11.7284, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0732e-11, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.000902
Average total loss: 2.303486
tensor(-11.7304, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0568e-11, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.000900
Average total loss: 2.303485
tensor(-11.7325, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0402e-11, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.000898
Average total loss: 2.303483
tensor(-11.7346, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0236e-11, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.000896
Average total loss: 2.303481
tensor(-11.7367, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0067e-11, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-11.7388, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9899e-11, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302586
Average KL loss: 0.000892
Average total loss: 2.303479
tensor(-11.7409, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9727e-11, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.000890
Average total loss: 2.303475
tensor(-11.7431, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9556e-11, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302584
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7433, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9538e-11, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7435, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9519e-11, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7438, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9500e-11, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7440, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9482e-11, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7442, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9463e-11, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7445, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9445e-11, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7447, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9426e-11, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7449, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9407e-11, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7452, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9389e-11, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7454, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9370e-11, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7456, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9352e-11, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7457, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9348e-11, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7457, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9344e-11, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7458, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9341e-11, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7458, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9337e-11, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7459, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9333e-11, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7459, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9329e-11, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7460, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9326e-11, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7460, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9322e-11, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7461, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9318e-11, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7461, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9315e-11, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302582
Average KL loss: 0.000887
Average total loss: 2.303468
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
 Percentile value: -11.746660232543945
Non-zero model percentage: 30.000001907348633%, Non-zero mask percentage: 30.000001907348633%

--- Pruning Level [1/7]: ---
conv1.weight         | nonzeros =     401 /    1728             ( 23.21%) | total_pruned =    1327 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
bn1.bias             | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5303 /   36864             ( 14.39%) | total_pruned =   31561 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   12390 /   36864             ( 33.61%) | total_pruned =   24474 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   12815 /   36864             ( 34.76%) | total_pruned =   24049 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      42 /      64             ( 65.62%) | total_pruned =      22 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   14966 /   36864             ( 40.60%) | total_pruned =   21898 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   33600 /   73728             ( 45.57%) | total_pruned =   40128 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   64194 /  147456             ( 43.53%) | total_pruned =   83262 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3577 /    8192             ( 43.66%) | total_pruned =    4615 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   49208 /  147456             ( 33.37%) | total_pruned =   98248 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   42147 /  147456             ( 28.58%) | total_pruned =  105309 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  127651 /  294912             ( 43.28%) | total_pruned =  167261 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     166 /     256             ( 64.84%) | total_pruned =      90 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  174200 /  589824             ( 29.53%) | total_pruned =  415624 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   10884 /   32768             ( 33.22%) | total_pruned =   21884 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     154 /     256             ( 60.16%) | total_pruned =     102 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  113042 /  589824             ( 19.17%) | total_pruned =  476782 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   86369 /  589824             ( 14.64%) | total_pruned =  503455 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     123 /     256             ( 48.05%) | total_pruned =     133 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  266759 / 1179648             ( 22.61%) | total_pruned =  912889 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     327 /     512             ( 63.87%) | total_pruned =     185 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  546739 / 2359296             ( 23.17%) | total_pruned = 1812557 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     297 /     512             ( 58.01%) | total_pruned =     215 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     371 /     512             ( 72.46%) | total_pruned =     141 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   31830 /  131072             ( 24.28%) | total_pruned =   99242 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     257 /     512             ( 50.20%) | total_pruned =     255 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     390 /     512             ( 76.17%) | total_pruned =     122 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  702379 / 2359296             ( 29.77%) | total_pruned = 1656917 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     290 /     512             ( 56.64%) | total_pruned =     222 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1043930 / 2359296             ( 44.25%) | total_pruned = 1315366 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3353629, pruned : 7825133, total: 11178762, Compression rate :       3.33x  ( 70.00% pruned)
Train Epoch: 67/200 Loss: 0.016586 Accuracy: 87.35 100.00 % Best test Accuracy: 87.37%
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000870
Average total loss: 2.303455
tensor(-11.7842, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(7.6299e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000838
Average total loss: 2.303422
tensor(-11.8208, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(7.3534e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8562, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(7.0970e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000780
Average total loss: 2.303365
tensor(-11.8904, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(6.8582e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000754
Average total loss: 2.303339
tensor(-11.9234, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.6350e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000730
Average total loss: 2.303315
tensor(-11.9554, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.4261e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303292
tensor(-11.9863, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.2299e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000686
Average total loss: 2.303271
tensor(-12.0164, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(6.0454e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000666
Average total loss: 2.303251
tensor(-12.0455, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.8715e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000647
Average total loss: 2.303232
tensor(-12.0739, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.7074e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000629
Average total loss: 2.303214
tensor(-12.1014, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.5523e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000613
Average total loss: 2.303197
tensor(-12.1282, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.4053e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000597
Average total loss: 2.303181
tensor(-12.1543, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.2660e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000581
Average total loss: 2.303166
tensor(-12.1798, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(5.1336e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000567
Average total loss: 2.303152
tensor(-12.2046, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(5.0078e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000553
Average total loss: 2.303138
tensor(-12.2288, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.8880e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000540
Average total loss: 2.303125
tensor(-12.2524, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.7738e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302584
Average KL loss: 0.000528
Average total loss: 2.303112
tensor(-12.2755, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.6648e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000516
Average total loss: 2.303101
tensor(-12.2981, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.5607e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000504
Average total loss: 2.303089
tensor(-12.3202, device='cuda:0') tensor(9.6478e-05, device='cuda:0') tensor(4.4611e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000493
Average total loss: 2.303078
tensor(-12.3418, device='cuda:0') tensor(9.1096e-05, device='cuda:0') tensor(4.3658e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000483
Average total loss: 2.303068
tensor(-12.3629, device='cuda:0') tensor(8.6177e-05, device='cuda:0') tensor(4.2744e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000477
Average total loss: 2.303062
tensor(-12.3650, device='cuda:0') tensor(8.5663e-05, device='cuda:0') tensor(4.2655e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000476
Average total loss: 2.303061
tensor(-12.3671, device='cuda:0') tensor(8.5185e-05, device='cuda:0') tensor(4.2567e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000475
Average total loss: 2.303060
tensor(-12.3691, device='cuda:0') tensor(8.4721e-05, device='cuda:0') tensor(4.2480e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000474
Average total loss: 2.303059
tensor(-12.3712, device='cuda:0') tensor(8.4261e-05, device='cuda:0') tensor(4.2392e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000473
Average total loss: 2.303058
tensor(-12.3732, device='cuda:0') tensor(8.3807e-05, device='cuda:0') tensor(4.2305e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000472
Average total loss: 2.303057
tensor(-12.3753, device='cuda:0') tensor(8.3357e-05, device='cuda:0') tensor(4.2218e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000471
Average total loss: 2.303056
tensor(-12.3773, device='cuda:0') tensor(8.2912e-05, device='cuda:0') tensor(4.2132e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000471
Average total loss: 2.303055
tensor(-12.3794, device='cuda:0') tensor(8.2473e-05, device='cuda:0') tensor(4.2045e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000470
Average total loss: 2.303054
tensor(-12.3815, device='cuda:0') tensor(8.2038e-05, device='cuda:0') tensor(4.1959e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000469
Average total loss: 2.303053
tensor(-12.3835, device='cuda:0') tensor(8.1607e-05, device='cuda:0') tensor(4.1872e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000468
Average total loss: 2.303052
tensor(-12.3856, device='cuda:0') tensor(8.1181e-05, device='cuda:0') tensor(4.1786e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3858, device='cuda:0') tensor(8.1132e-05, device='cuda:0') tensor(4.1779e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3859, device='cuda:0') tensor(8.1083e-05, device='cuda:0') tensor(4.1771e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3861, device='cuda:0') tensor(8.1034e-05, device='cuda:0') tensor(4.1763e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3863, device='cuda:0') tensor(8.0985e-05, device='cuda:0') tensor(4.1755e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3865, device='cuda:0') tensor(8.0935e-05, device='cuda:0') tensor(4.1747e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303051
tensor(-12.3867, device='cuda:0') tensor(8.0886e-05, device='cuda:0') tensor(4.1739e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303051
tensor(-12.3869, device='cuda:0') tensor(8.0837e-05, device='cuda:0') tensor(4.1732e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3871, device='cuda:0') tensor(8.0788e-05, device='cuda:0') tensor(4.1724e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3873, device='cuda:0') tensor(8.0739e-05, device='cuda:0') tensor(4.1716e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3874, device='cuda:0') tensor(8.0690e-05, device='cuda:0') tensor(4.1708e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0641e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0626e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0611e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0596e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0582e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0567e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0552e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0537e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0522e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0507e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0492e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
 Percentile value: -12.387662887573242
Non-zero model percentage: 9.000003814697266%, Non-zero mask percentage: 9.000003814697266%

--- Pruning Level [2/7]: ---
conv1.weight         | nonzeros =     374 /    1728             ( 21.64%) | total_pruned =    1354 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3310 /   36864             (  8.98%) | total_pruned =   33554 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6245 /   36864             ( 16.94%) | total_pruned =   30619 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6335 /   36864             ( 17.18%) | total_pruned =   30529 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    6888 /   36864             ( 18.68%) | total_pruned =   29976 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   18944 /   73728             ( 25.69%) | total_pruned =   54784 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   32027 /  147456             ( 21.72%) | total_pruned =  115429 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2205 /    8192             ( 26.92%) | total_pruned =    5987 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13648 /  147456             (  9.26%) | total_pruned =  133808 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10162 /  147456             (  6.89%) | total_pruned =  137294 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   64113 /  294912             ( 21.74%) | total_pruned =  230799 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     168 /     256             ( 65.62%) | total_pruned =      88 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   62855 /  589824             ( 10.66%) | total_pruned =  526969 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     142 /     256             ( 55.47%) | total_pruned =     114 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4075 /   32768             ( 12.44%) | total_pruned =   28693 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     148 /     256             ( 57.81%) | total_pruned =     108 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21778 /  589824             (  3.69%) | total_pruned =  568046 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   15649 /  589824             (  2.65%) | total_pruned =  574175 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     108 /     256             ( 42.19%) | total_pruned =     148 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   82715 / 1179648             (  7.01%) | total_pruned = 1096933 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     260 /     512             ( 50.78%) | total_pruned =     252 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     312 /     512             ( 60.94%) | total_pruned =     200 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  164432 / 2359296             (  6.97%) | total_pruned = 2194864 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     285 /     512             ( 55.66%) | total_pruned =     227 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     356 /     512             ( 69.53%) | total_pruned =     156 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    9790 /  131072             (  7.47%) | total_pruned =  121282 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  250523 / 2359296             ( 10.62%) | total_pruned = 2108773 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     307 /     512             ( 59.96%) | total_pruned =     205 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     275 /     512             ( 53.71%) | total_pruned =     237 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  219518 / 2359296             (  9.30%) | total_pruned = 2139778 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4817 /    5120             ( 94.08%) | total_pruned =     303 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1006089, pruned : 10172673, total: 11178762, Compression rate :      11.11x  ( 91.00% pruned)
Train Epoch: 72/200 Loss: 0.056892 Accuracy: 84.98 100.00 % Best test Accuracy: 86.25%
tensor(-12.3876, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000462
Average total loss: 2.303046
tensor(-12.4078, device='cuda:0') tensor(7.6146e-05, device='cuda:0') tensor(4.0866e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000452
Average total loss: 2.303037
tensor(-12.4276, device='cuda:0') tensor(7.2037e-05, device='cuda:0') tensor(4.0064e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000444
Average total loss: 2.303029
tensor(-12.4471, device='cuda:0') tensor(6.8466e-05, device='cuda:0') tensor(3.9293e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000435
Average total loss: 2.303020
tensor(-12.4661, device='cuda:0') tensor(6.5161e-05, device='cuda:0') tensor(3.8551e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000427
Average total loss: 2.303012
tensor(-12.4849, device='cuda:0') tensor(6.2094e-05, device='cuda:0') tensor(3.7836e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000419
Average total loss: 2.303004
tensor(-12.5032, device='cuda:0') tensor(5.9243e-05, device='cuda:0') tensor(3.7148e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000412
Average total loss: 2.302996
tensor(-12.5213, device='cuda:0') tensor(5.6588e-05, device='cuda:0') tensor(3.6484e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000404
Average total loss: 2.302989
tensor(-12.5390, device='cuda:0') tensor(5.4110e-05, device='cuda:0') tensor(3.5843e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000397
Average total loss: 2.302982
tensor(-12.5564, device='cuda:0') tensor(5.1794e-05, device='cuda:0') tensor(3.5225e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000390
Average total loss: 2.302975
tensor(-12.5735, device='cuda:0') tensor(4.9626e-05, device='cuda:0') tensor(3.4627e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000384
Average total loss: 2.302969
tensor(-12.5903, device='cuda:0') tensor(4.7594e-05, device='cuda:0') tensor(3.4050e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000378
Average total loss: 2.302962
tensor(-12.6068, device='cuda:0') tensor(4.5686e-05, device='cuda:0') tensor(3.3491e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000374
Average total loss: 2.302959
tensor(-12.6085, device='cuda:0') tensor(4.5503e-05, device='cuda:0') tensor(3.3437e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000373
Average total loss: 2.302958
tensor(-12.6101, device='cuda:0') tensor(4.5323e-05, device='cuda:0') tensor(3.3382e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000373
Average total loss: 2.302958
tensor(-12.6117, device='cuda:0') tensor(4.5144e-05, device='cuda:0') tensor(3.3327e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000372
Average total loss: 2.302957
tensor(-12.6134, device='cuda:0') tensor(4.4967e-05, device='cuda:0') tensor(3.3273e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000372
Average total loss: 2.302957
tensor(-12.6150, device='cuda:0') tensor(4.4792e-05, device='cuda:0') tensor(3.3219e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000371
Average total loss: 2.302956
tensor(-12.6166, device='cuda:0') tensor(4.4618e-05, device='cuda:0') tensor(3.3164e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000370
Average total loss: 2.302955
tensor(-12.6183, device='cuda:0') tensor(4.4446e-05, device='cuda:0') tensor(3.3110e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000370
Average total loss: 2.302955
tensor(-12.6199, device='cuda:0') tensor(4.4275e-05, device='cuda:0') tensor(3.3056e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000369
Average total loss: 2.302954
tensor(-12.6215, device='cuda:0') tensor(4.4103e-05, device='cuda:0') tensor(3.3002e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000369
Average total loss: 2.302954
tensor(-12.6231, device='cuda:0') tensor(4.3923e-05, device='cuda:0') tensor(3.2950e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6247, device='cuda:0') tensor(4.3744e-05, device='cuda:0') tensor(3.2897e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6249, device='cuda:0') tensor(4.3721e-05, device='cuda:0') tensor(3.2893e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6250, device='cuda:0') tensor(4.3698e-05, device='cuda:0') tensor(3.2888e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6251, device='cuda:0') tensor(4.3674e-05, device='cuda:0') tensor(3.2884e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6253, device='cuda:0') tensor(4.3651e-05, device='cuda:0') tensor(3.2879e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302952
tensor(-12.6254, device='cuda:0') tensor(4.3628e-05, device='cuda:0') tensor(3.2874e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6256, device='cuda:0') tensor(4.3605e-05, device='cuda:0') tensor(3.2870e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6257, device='cuda:0') tensor(4.3581e-05, device='cuda:0') tensor(3.2865e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6258, device='cuda:0') tensor(4.3558e-05, device='cuda:0') tensor(3.2861e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6260, device='cuda:0') tensor(4.3535e-05, device='cuda:0') tensor(3.2856e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6261, device='cuda:0') tensor(4.3512e-05, device='cuda:0') tensor(3.2851e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3489e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3484e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3478e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3473e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3468e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3463e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3457e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3452e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3447e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3442e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3436e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
 Percentile value: -12.625797271728516
Non-zero model percentage: 2.7000038623809814%, Non-zero mask percentage: 2.7000038623809814%

--- Pruning Level [3/7]: ---
conv1.weight         | nonzeros =     346 /    1728             ( 20.02%) | total_pruned =    1382 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1710 /   36864             (  4.64%) | total_pruned =   35154 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2865 /   36864             (  7.77%) | total_pruned =   33999 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2583 /   36864             (  7.01%) | total_pruned =   34281 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2760 /   36864             (  7.49%) | total_pruned =   34104 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    7925 /   73728             ( 10.75%) | total_pruned =   65803 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12118 /  147456             (  8.22%) | total_pruned =  135338 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1182 /    8192             ( 14.43%) | total_pruned =    7010 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2913 /  147456             (  1.98%) | total_pruned =  144543 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2193 /  147456             (  1.49%) | total_pruned =  145263 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   24571 /  294912             (  8.33%) | total_pruned =  270341 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   22541 /  589824             (  3.82%) | total_pruned =  567283 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     157 /     256             ( 61.33%) | total_pruned =      99 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1568 /   32768             (  4.79%) | total_pruned =   31200 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      93 /     256             ( 36.33%) | total_pruned =     163 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4219 /  589824             (  0.72%) | total_pruned =  585605 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      75 /     256             ( 29.30%) | total_pruned =     181 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      84 /     256             ( 32.81%) | total_pruned =     172 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3160 /  589824             (  0.54%) | total_pruned =  586664 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      73 /     256             ( 28.52%) | total_pruned =     183 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   26521 / 1179648             (  2.25%) | total_pruned = 1153127 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     239 /     512             ( 46.68%) | total_pruned =     273 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   45505 / 2359296             (  1.93%) | total_pruned = 2313791 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     258 /     512             ( 50.39%) | total_pruned =     254 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2928 /  131072             (  2.23%) | total_pruned =  128144 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     168 /     512             ( 32.81%) | total_pruned =     344 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     352 /     512             ( 68.75%) | total_pruned =     160 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   70491 / 2359296             (  2.99%) | total_pruned = 2288805 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     282 /     512             ( 55.08%) | total_pruned =     230 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     244 /     512             ( 47.66%) | total_pruned =     268 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   53737 / 2359296             (  2.28%) | total_pruned = 2305559 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     333 /     512             ( 65.04%) | total_pruned =     179 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4809 /    5120             ( 93.93%) | total_pruned =     311 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 301827, pruned : 10876935, total: 11178762, Compression rate :      37.04x  ( 97.30% pruned)
Train Epoch: 95/200 Loss: 0.060154 Accuracy: 80.06 99.96 % Best test Accuracy: 82.11%
tensor(-12.6263, device='cuda:0') tensor(4.3431e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000364
Average total loss: 2.302949
tensor(-12.6422, device='cuda:0') tensor(4.1759e-05, device='cuda:0') tensor(3.2327e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000359
Average total loss: 2.302943
tensor(-12.6579, device='cuda:0') tensor(4.0180e-05, device='cuda:0') tensor(3.1823e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000353
Average total loss: 2.302938
tensor(-12.6734, device='cuda:0') tensor(3.8690e-05, device='cuda:0') tensor(3.1334e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000348
Average total loss: 2.302933
tensor(-12.6886, device='cuda:0') tensor(3.7284e-05, device='cuda:0') tensor(3.0861e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000342
Average total loss: 2.302927
tensor(-12.7036, device='cuda:0') tensor(3.5954e-05, device='cuda:0') tensor(3.0401e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000337
Average total loss: 2.302922
tensor(-12.7184, device='cuda:0') tensor(3.4695e-05, device='cuda:0') tensor(2.9955e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000332
Average total loss: 2.302917
tensor(-12.7330, device='cuda:0') tensor(3.3502e-05, device='cuda:0') tensor(2.9522e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-12.7473, device='cuda:0') tensor(3.2370e-05, device='cuda:0') tensor(2.9101e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000323
Average total loss: 2.302908
tensor(-12.7615, device='cuda:0') tensor(3.1296e-05, device='cuda:0') tensor(2.8692e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000319
Average total loss: 2.302903
tensor(-12.7755, device='cuda:0') tensor(3.0276e-05, device='cuda:0') tensor(2.8294e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000314
Average total loss: 2.302899
tensor(-12.7892, device='cuda:0') tensor(2.9305e-05, device='cuda:0') tensor(2.7907e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000310
Average total loss: 2.302895
tensor(-12.8028, device='cuda:0') tensor(2.8381e-05, device='cuda:0') tensor(2.7531e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000308
Average total loss: 2.302892
tensor(-12.8041, device='cuda:0') tensor(2.8293e-05, device='cuda:0') tensor(2.7494e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000307
Average total loss: 2.302892
tensor(-12.8055, device='cuda:0') tensor(2.8207e-05, device='cuda:0') tensor(2.7456e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000307
Average total loss: 2.302892
tensor(-12.8069, device='cuda:0') tensor(2.8122e-05, device='cuda:0') tensor(2.7419e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000306
Average total loss: 2.302891
tensor(-12.8082, device='cuda:0') tensor(2.8037e-05, device='cuda:0') tensor(2.7382e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000306
Average total loss: 2.302891
tensor(-12.8096, device='cuda:0') tensor(2.7954e-05, device='cuda:0') tensor(2.7345e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8109, device='cuda:0') tensor(2.7870e-05, device='cuda:0') tensor(2.7308e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8123, device='cuda:0') tensor(2.7788e-05, device='cuda:0') tensor(2.7271e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8136, device='cuda:0') tensor(2.7700e-05, device='cuda:0') tensor(2.7235e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000304
Average total loss: 2.302889
tensor(-12.8149, device='cuda:0') tensor(2.7608e-05, device='cuda:0') tensor(2.7199e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000304
Average total loss: 2.302889
tensor(-12.8162, device='cuda:0') tensor(2.7518e-05, device='cuda:0') tensor(2.7163e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8175, device='cuda:0') tensor(2.7428e-05, device='cuda:0') tensor(2.7128e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8177, device='cuda:0') tensor(2.7422e-05, device='cuda:0') tensor(2.7124e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8178, device='cuda:0') tensor(2.7415e-05, device='cuda:0') tensor(2.7120e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8180, device='cuda:0') tensor(2.7409e-05, device='cuda:0') tensor(2.7117e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8181, device='cuda:0') tensor(2.7402e-05, device='cuda:0') tensor(2.7113e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8182, device='cuda:0') tensor(2.7396e-05, device='cuda:0') tensor(2.7109e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8184, device='cuda:0') tensor(2.7389e-05, device='cuda:0') tensor(2.7105e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8185, device='cuda:0') tensor(2.7383e-05, device='cuda:0') tensor(2.7101e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8187, device='cuda:0') tensor(2.7377e-05, device='cuda:0') tensor(2.7098e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8188, device='cuda:0') tensor(2.7370e-05, device='cuda:0') tensor(2.7094e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8189, device='cuda:0') tensor(2.7364e-05, device='cuda:0') tensor(2.7091e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
 Percentile value: -12.817139625549316
Non-zero model percentage: 0.8100091218948364%, Non-zero mask percentage: 0.8100091218948364%

--- Pruning Level [4/7]: ---
conv1.weight         | nonzeros =     300 /    1728             ( 17.36%) | total_pruned =    1428 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     837 /   36864             (  2.27%) | total_pruned =   36027 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1225 /   36864             (  3.32%) | total_pruned =   35639 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     908 /   36864             (  2.46%) | total_pruned =   35956 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1000 /   36864             (  2.71%) | total_pruned =   35864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2645 /   73728             (  3.59%) | total_pruned =   71083 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    3765 /  147456             (  2.55%) | total_pruned =  143691 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     621 /    8192             (  7.58%) | total_pruned =    7571 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     533 /  147456             (  0.36%) | total_pruned =  146923 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     451 /  147456             (  0.31%) | total_pruned =  147005 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    7989 /  294912             (  2.71%) | total_pruned =  286923 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    7828 /  589824             (  1.33%) | total_pruned =  581996 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     590 /   32768             (  1.80%) | total_pruned =   32178 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      60 /     256             ( 23.44%) | total_pruned =     196 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     717 /  589824             (  0.12%) | total_pruned =  589107 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      43 /     256             ( 16.80%) | total_pruned =     213 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      59 /     256             ( 23.05%) | total_pruned =     197 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     577 /  589824             (  0.10%) | total_pruned =  589247 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      48 /     256             ( 18.75%) | total_pruned =     208 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8374 / 1179648             (  0.71%) | total_pruned = 1171274 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     211 /     512             ( 41.21%) | total_pruned =     301 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   11326 / 2359296             (  0.48%) | total_pruned = 2347970 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     310 /     512             ( 60.55%) | total_pruned =     202 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     844 /  131072             (  0.64%) | total_pruned =  130228 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     123 /     512             ( 24.02%) | total_pruned =     389 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     328 /     512             ( 64.06%) | total_pruned =     184 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   17094 / 2359296             (  0.72%) | total_pruned = 2342202 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     193 /     512             ( 37.70%) | total_pruned =     319 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   13628 / 2359296             (  0.58%) | total_pruned = 2345668 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     312 /     512             ( 60.94%) | total_pruned =     200 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4774 /    5120             ( 93.24%) | total_pruned =     346 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 90549, pruned : 11088213, total: 11178762, Compression rate :     123.46x  ( 99.19% pruned)
Train Epoch: 199/200 Loss: 0.461399 Accuracy: 76.65 89.89 % Best test Accuracy: 78.14%
tensor(-12.8191, device='cuda:0') tensor(2.7357e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000301
Average total loss: 2.302886
tensor(-12.8323, device='cuda:0') tensor(2.6525e-05, device='cuda:0') tensor(2.6732e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000297
Average total loss: 2.302882
tensor(-12.8453, device='cuda:0') tensor(2.5729e-05, device='cuda:0') tensor(2.6386e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000293
Average total loss: 2.302878
tensor(-12.8581, device='cuda:0') tensor(2.4968e-05, device='cuda:0') tensor(2.6049e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000289
Average total loss: 2.302874
tensor(-12.8708, device='cuda:0') tensor(2.4242e-05, device='cuda:0') tensor(2.5721e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000286
Average total loss: 2.302871
tensor(-12.8833, device='cuda:0') tensor(2.3546e-05, device='cuda:0') tensor(2.5401e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000282
Average total loss: 2.302867
tensor(-12.8957, device='cuda:0') tensor(2.2881e-05, device='cuda:0') tensor(2.5089e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000279
Average total loss: 2.302864
tensor(-12.9079, device='cuda:0') tensor(2.2243e-05, device='cuda:0') tensor(2.4784e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-12.9200, device='cuda:0') tensor(2.1633e-05, device='cuda:0') tensor(2.4487e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000272
Average total loss: 2.302857
tensor(-12.9319, device='cuda:0') tensor(2.1047e-05, device='cuda:0') tensor(2.4197e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000269
Average total loss: 2.302854
tensor(-12.9437, device='cuda:0') tensor(2.0485e-05, device='cuda:0') tensor(2.3913e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000266
Average total loss: 2.302851
tensor(-12.9553, device='cuda:0') tensor(1.9945e-05, device='cuda:0') tensor(2.3636e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000263
Average total loss: 2.302848
tensor(-12.9668, device='cuda:0') tensor(1.9426e-05, device='cuda:0') tensor(2.3366e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000261
Average total loss: 2.302846
tensor(-12.9680, device='cuda:0') tensor(1.9372e-05, device='cuda:0') tensor(2.3340e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000261
Average total loss: 2.302846
tensor(-12.9691, device='cuda:0') tensor(1.9318e-05, device='cuda:0') tensor(2.3313e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9702, device='cuda:0') tensor(1.9264e-05, device='cuda:0') tensor(2.3287e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9713, device='cuda:0') tensor(1.9212e-05, device='cuda:0') tensor(2.3261e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9724, device='cuda:0') tensor(1.9159e-05, device='cuda:0') tensor(2.3235e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302844
tensor(-12.9736, device='cuda:0') tensor(1.9107e-05, device='cuda:0') tensor(2.3209e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9747, device='cuda:0') tensor(1.9071e-05, device='cuda:0') tensor(2.3183e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9758, device='cuda:0') tensor(1.9020e-05, device='cuda:0') tensor(2.3157e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9769, device='cuda:0') tensor(1.8970e-05, device='cuda:0') tensor(2.3131e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9781, device='cuda:0') tensor(1.8920e-05, device='cuda:0') tensor(2.3105e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9792, device='cuda:0') tensor(1.8870e-05, device='cuda:0') tensor(2.3079e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9793, device='cuda:0') tensor(1.8862e-05, device='cuda:0') tensor(2.3077e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9794, device='cuda:0') tensor(1.8854e-05, device='cuda:0') tensor(2.3075e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9795, device='cuda:0') tensor(1.8846e-05, device='cuda:0') tensor(2.3073e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9795, device='cuda:0') tensor(1.8838e-05, device='cuda:0') tensor(2.3071e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9796, device='cuda:0') tensor(1.8830e-05, device='cuda:0') tensor(2.3068e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9797, device='cuda:0') tensor(1.8822e-05, device='cuda:0') tensor(2.3066e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9798, device='cuda:0') tensor(1.8814e-05, device='cuda:0') tensor(2.3064e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9799, device='cuda:0') tensor(1.8806e-05, device='cuda:0') tensor(2.3062e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9800, device='cuda:0') tensor(1.8798e-05, device='cuda:0') tensor(2.3060e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9801, device='cuda:0') tensor(1.8790e-05, device='cuda:0') tensor(2.3058e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
 Percentile value: -12.966657638549805
Non-zero model percentage: 0.2430054396390915%, Non-zero mask percentage: 0.2430054396390915%

--- Pruning Level [5/7]: ---
conv1.weight         | nonzeros =     243 /    1728             ( 14.06%) | total_pruned =    1485 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
bn1.bias             | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     324 /   36864             (  0.88%) | total_pruned =   36540 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     458 /   36864             (  1.24%) | total_pruned =   36406 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     272 /   36864             (  0.74%) | total_pruned =   36592 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       6 /      64             (  9.38%) | total_pruned =      58 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     266 /   36864             (  0.72%) | total_pruned =   36598 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     583 /   73728             (  0.79%) | total_pruned =   73145 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     855 /  147456             (  0.58%) | total_pruned =  146601 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     289 /    8192             (  3.53%) | total_pruned =    7903 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      61 /  147456             (  0.04%) | total_pruned =  147395 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      70 /  147456             (  0.05%) | total_pruned =  147386 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      29 /     128             ( 22.66%) | total_pruned =      99 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    1992 /  294912             (  0.68%) | total_pruned =  292920 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     138 /     256             ( 53.91%) | total_pruned =     118 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    2274 /  589824             (  0.39%) | total_pruned =  587550 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      87 /     256             ( 33.98%) | total_pruned =     169 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     101 /     256             ( 39.45%) | total_pruned =     155 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     221 /   32768             (  0.67%) | total_pruned =   32547 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      48 /     256             ( 18.75%) | total_pruned =     208 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      88 /     256             ( 34.38%) | total_pruned =     168 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      75 /  589824             (  0.01%) | total_pruned =  589749 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      23 /     256             (  8.98%) | total_pruned =     233 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      28 /     256             ( 10.94%) | total_pruned =     228 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      65 /  589824             (  0.01%) | total_pruned =  589759 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      22 /     256             (  8.59%) | total_pruned =     234 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      93 /     256             ( 36.33%) | total_pruned =     163 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    2206 / 1179648             (  0.19%) | total_pruned = 1177442 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     163 /     512             ( 31.84%) | total_pruned =     349 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     196 /     512             ( 38.28%) | total_pruned =     316 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    2102 / 2359296             (  0.09%) | total_pruned = 2357194 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     182 /     512             ( 35.55%) | total_pruned =     330 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     292 /     512             ( 57.03%) | total_pruned =     220 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     213 /  131072             (  0.16%) | total_pruned =  130859 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      70 /     512             ( 13.67%) | total_pruned =     442 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     298 /     512             ( 58.20%) | total_pruned =     214 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    3199 / 2359296             (  0.14%) | total_pruned = 2356097 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     205 /     512             ( 40.04%) | total_pruned =     307 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     139 /     512             ( 27.15%) | total_pruned =     373 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    3025 / 2359296             (  0.13%) | total_pruned = 2356271 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     274 /     512             ( 53.52%) | total_pruned =     238 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4679 /    5120             ( 91.39%) | total_pruned =     441 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 27165, pruned : 11151597, total: 11178762, Compression rate :     411.51x  ( 99.76% pruned)
Train Epoch: 153/200 Loss: 0.909083 Accuracy: 63.43 65.70 % Best test Accuracy: 63.68%
tensor(-12.9802, device='cuda:0') tensor(1.8782e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000256
Average total loss: 2.302841
tensor(-12.9914, device='cuda:0') tensor(1.8308e-05, device='cuda:0') tensor(2.2798e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000253
Average total loss: 2.302838
tensor(-13.0025, device='cuda:0') tensor(1.7851e-05, device='cuda:0') tensor(2.2546e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000251
Average total loss: 2.302836
tensor(-13.0135, device='cuda:0') tensor(1.7411e-05, device='cuda:0') tensor(2.2300e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000248
Average total loss: 2.302833
tensor(-13.0244, device='cuda:0') tensor(1.6987e-05, device='cuda:0') tensor(2.2059e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000245
Average total loss: 2.302830
tensor(-13.0351, device='cuda:0') tensor(1.6573e-05, device='cuda:0') tensor(2.1823e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000243
Average total loss: 2.302828
tensor(-13.0458, device='cuda:0') tensor(1.6180e-05, device='cuda:0') tensor(2.1592e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000240
Average total loss: 2.302825
tensor(-13.0563, device='cuda:0') tensor(1.5804e-05, device='cuda:0') tensor(2.1366e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-13.0667, device='cuda:0') tensor(1.5438e-05, device='cuda:0') tensor(2.1145e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000235
Average total loss: 2.302820
tensor(-13.0770, device='cuda:0') tensor(1.5084e-05, device='cuda:0') tensor(2.0928e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000233
Average total loss: 2.302818
tensor(-13.0872, device='cuda:0') tensor(1.4742e-05, device='cuda:0') tensor(2.0716e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000230
Average total loss: 2.302815
tensor(-13.0973, device='cuda:0') tensor(1.4411e-05, device='cuda:0') tensor(2.0508e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000228
Average total loss: 2.302813
tensor(-13.1073, device='cuda:0') tensor(1.4092e-05, device='cuda:0') tensor(2.0304e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000227
Average total loss: 2.302812
tensor(-13.1083, device='cuda:0') tensor(1.4059e-05, device='cuda:0') tensor(2.0284e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000227
Average total loss: 2.302812
tensor(-13.1093, device='cuda:0') tensor(1.4027e-05, device='cuda:0') tensor(2.0264e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1102, device='cuda:0') tensor(1.3995e-05, device='cuda:0') tensor(2.0244e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1112, device='cuda:0') tensor(1.3962e-05, device='cuda:0') tensor(2.0224e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1122, device='cuda:0') tensor(1.3931e-05, device='cuda:0') tensor(2.0204e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1132, device='cuda:0') tensor(1.3899e-05, device='cuda:0') tensor(2.0184e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302810
tensor(-13.1142, device='cuda:0') tensor(1.3868e-05, device='cuda:0') tensor(2.0165e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1152, device='cuda:0') tensor(1.3837e-05, device='cuda:0') tensor(2.0145e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1161, device='cuda:0') tensor(1.3806e-05, device='cuda:0') tensor(2.0125e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1171, device='cuda:0') tensor(1.3776e-05, device='cuda:0') tensor(2.0105e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1181, device='cuda:0') tensor(1.3746e-05, device='cuda:0') tensor(2.0086e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302809
tensor(-13.1182, device='cuda:0') tensor(1.3742e-05, device='cuda:0') tensor(2.0084e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302809
tensor(-13.1183, device='cuda:0') tensor(1.3738e-05, device='cuda:0') tensor(2.0082e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1184, device='cuda:0') tensor(1.3734e-05, device='cuda:0') tensor(2.0080e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1185, device='cuda:0') tensor(1.3730e-05, device='cuda:0') tensor(2.0078e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1186, device='cuda:0') tensor(1.3726e-05, device='cuda:0') tensor(2.0076e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1187, device='cuda:0') tensor(1.3722e-05, device='cuda:0') tensor(2.0074e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1188, device='cuda:0') tensor(1.3718e-05, device='cuda:0') tensor(2.0072e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1188, device='cuda:0') tensor(1.3714e-05, device='cuda:0') tensor(2.0071e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1189, device='cuda:0') tensor(1.3710e-05, device='cuda:0') tensor(2.0069e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1190, device='cuda:0') tensor(1.3706e-05, device='cuda:0') tensor(2.0067e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3702e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
 Percentile value: -12.96953182220459
Non-zero model percentage: 0.07290609925985336%, Non-zero mask percentage: 0.07290609925985336%

--- Pruning Level [6/7]: ---
conv1.weight         | nonzeros =     122 /    1728             (  7.06%) | total_pruned =    1606 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =      48 /   36864             (  0.13%) | total_pruned =   36816 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =      69 /   36864             (  0.19%) | total_pruned =   36795 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =      24 /   36864             (  0.07%) | total_pruned =   36840 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =      29 /   36864             (  0.08%) | total_pruned =   36835 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =      14 /   73728             (  0.02%) | total_pruned =   73714 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       7 /     128             (  5.47%) | total_pruned =     121 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =      33 /  147456             (  0.02%) | total_pruned =  147423 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      39 /     128             ( 30.47%) | total_pruned =      89 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      16 /     128             ( 12.50%) | total_pruned =     112 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =      58 /    8192             (  0.71%) | total_pruned =    8134 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      27 /     128             ( 21.09%) | total_pruned =     101 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      17 /     128             ( 13.28%) | total_pruned =     111 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       2 /  147456             (  0.00%) | total_pruned =  147454 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       7 /     128             (  5.47%) | total_pruned =     121 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      10 /     128             (  7.81%) | total_pruned =     118 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     173 /  294912             (  0.06%) | total_pruned =  294739 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      86 /     256             ( 33.59%) | total_pruned =     170 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     210 /  589824             (  0.04%) | total_pruned =  589614 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      51 /     256             ( 19.92%) | total_pruned =     205 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      54 /     256             ( 21.09%) | total_pruned =     202 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =      40 /   32768             (  0.12%) | total_pruned =   32728 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      20 /     256             (  7.81%) | total_pruned =     236 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      60 /     256             ( 23.44%) | total_pruned =     196 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       2 /  589824             (  0.00%) | total_pruned =  589822 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       7 /     256             (  2.73%) | total_pruned =     249 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       3 /     256             (  1.17%) | total_pruned =     253 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      58 /     256             ( 22.66%) | total_pruned =     198 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     201 / 1179648             (  0.02%) | total_pruned = 1179447 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =      77 /     512             ( 15.04%) | total_pruned =     435 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     105 /     512             ( 20.51%) | total_pruned =     407 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     128 / 2359296             (  0.01%) | total_pruned = 2359168 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     100 /     512             ( 19.53%) | total_pruned =     412 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     243 /     512             ( 47.46%) | total_pruned =     269 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =      16 /  131072             (  0.01%) | total_pruned =  131056 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      21 /     512             (  4.10%) | total_pruned =     491 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     251 /     512             ( 49.02%) | total_pruned =     261 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     152 / 2359296             (  0.01%) | total_pruned = 2359144 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     125 /     512             ( 24.41%) | total_pruned =     387 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      68 /     512             ( 13.28%) | total_pruned =     444 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     260 / 2359296             (  0.01%) | total_pruned = 2359036 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     154 /     512             ( 30.08%) | total_pruned =     358 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4205 /    5120             ( 82.13%) | total_pruned =     915 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 8150, pruned : 11170612, total: 11178762, Compression rate :    1371.63x  ( 99.93% pruned)
Train Epoch: 56/200 Loss: 2.302480 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
