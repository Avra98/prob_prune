Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/7]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/200 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.5000e-05, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302701
Average KL loss: 175.345014
Average total loss: 177.647710
tensor(-3.5410, device='cuda:0') tensor(3.6329e-05, device='cuda:0') tensor(2.7375e-06, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302583
Average KL loss: 19.045783
Average total loss: 21.348366
tensor(-4.5139, device='cuda:0') tensor(2.8472e-05, device='cuda:0') tensor(1.0720e-06, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 9.321444
Average total loss: 11.624028
tensor(-5.0445, device='cuda:0') tensor(2.5220e-05, device='cuda:0') tensor(6.3626e-07, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302588
Average KL loss: 5.907176
Average total loss: 8.209764
tensor(-5.4350, device='cuda:0') tensor(2.4004e-05, device='cuda:0') tensor(4.3233e-07, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302590
Average KL loss: 4.165197
Average total loss: 6.467787
tensor(-5.7465, device='cuda:0') tensor(2.3219e-05, device='cuda:0') tensor(3.1738e-07, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302580
Average KL loss: 3.132825
Average total loss: 5.435405
tensor(-6.0063, device='cuda:0') tensor(2.2516e-05, device='cuda:0') tensor(2.4512e-07, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302589
Average KL loss: 2.461732
Average total loss: 4.764321
tensor(-6.2295, device='cuda:0') tensor(2.2062e-05, device='cuda:0') tensor(1.9627e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302587
Average KL loss: 1.996689
Average total loss: 4.299276
tensor(-6.4256, device='cuda:0') tensor(2.1662e-05, device='cuda:0') tensor(1.6144e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302584
Average KL loss: 1.658901
Average total loss: 3.961485
tensor(-6.6006, device='cuda:0') tensor(2.1463e-05, device='cuda:0') tensor(1.3559e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302584
Average KL loss: 1.404518
Average total loss: 3.707102
tensor(-6.7587, device='cuda:0') tensor(2.1390e-05, device='cuda:0') tensor(1.1580e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 1.207368
Average total loss: 3.509953
tensor(-6.9032, device='cuda:0') tensor(2.1283e-05, device='cuda:0') tensor(1.0025e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 1.050967
Average total loss: 3.353552
tensor(-7.0363, device='cuda:0') tensor(2.1191e-05, device='cuda:0') tensor(8.7782e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302583
Average KL loss: 0.924471
Average total loss: 3.227054
tensor(-7.1598, device='cuda:0') tensor(2.1366e-05, device='cuda:0') tensor(7.7601e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302587
Average KL loss: 0.820479
Average total loss: 3.123066
tensor(-7.2751, device='cuda:0') tensor(2.1426e-05, device='cuda:0') tensor(6.9166e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302577
Average KL loss: 0.733786
Average total loss: 3.036363
tensor(-7.3832, device='cuda:0') tensor(2.1299e-05, device='cuda:0') tensor(6.2085e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302587
Average KL loss: 0.660640
Average total loss: 2.963227
tensor(-7.4851, device='cuda:0') tensor(2.1203e-05, device='cuda:0') tensor(5.6076e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.598268
Average total loss: 2.900853
tensor(-7.5815, device='cuda:0') tensor(2.1132e-05, device='cuda:0') tensor(5.0926e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.544590
Average total loss: 2.847175
tensor(-7.6731, device='cuda:0') tensor(2.1127e-05, device='cuda:0') tensor(4.6473e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302584
Average KL loss: 0.498012
Average total loss: 2.800596
tensor(-7.7604, device='cuda:0') tensor(2.1582e-05, device='cuda:0') tensor(4.2594e-08, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.457297
Average total loss: 2.759882
tensor(-7.8437, device='cuda:0') tensor(2.1428e-05, device='cuda:0') tensor(3.9191e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.421470
Average total loss: 2.724055
tensor(-7.9235, device='cuda:0') tensor(2.1349e-05, device='cuda:0') tensor(3.6186e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302586
Average KL loss: 0.389756
Average total loss: 2.692342
tensor(-8.0002, device='cuda:0') tensor(2.1014e-05, device='cuda:0') tensor(3.3518e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302583
Average KL loss: 0.361529
Average total loss: 2.664112
tensor(-8.0739, device='cuda:0') tensor(2.0956e-05, device='cuda:0') tensor(3.1138e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302589
Average KL loss: 0.336283
Average total loss: 2.638871
tensor(-8.1450, device='cuda:0') tensor(2.0826e-05, device='cuda:0') tensor(2.9003e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.313599
Average total loss: 2.616184
tensor(-8.2136, device='cuda:0') tensor(2.0777e-05, device='cuda:0') tensor(2.7081e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.293134
Average total loss: 2.595719
tensor(-8.2799, device='cuda:0') tensor(2.0713e-05, device='cuda:0') tensor(2.5343e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.274597
Average total loss: 2.577182
tensor(-8.3442, device='cuda:0') tensor(2.0655e-05, device='cuda:0') tensor(2.3766e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.257749
Average total loss: 2.560334
tensor(-8.4066, device='cuda:0') tensor(2.0588e-05, device='cuda:0') tensor(2.2330e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.242385
Average total loss: 2.544970
tensor(-8.4671, device='cuda:0') tensor(2.0565e-05, device='cuda:0') tensor(2.1018e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.228331
Average total loss: 2.530916
tensor(-8.5260, device='cuda:0') tensor(2.0549e-05, device='cuda:0') tensor(1.9817e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.215438
Average total loss: 2.518023
tensor(-8.5834, device='cuda:0') tensor(2.0536e-05, device='cuda:0') tensor(1.8713e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302584
Average KL loss: 0.203580
Average total loss: 2.506163
tensor(-8.6392, device='cuda:0') tensor(2.0524e-05, device='cuda:0') tensor(1.7696e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.192645
Average total loss: 2.495230
tensor(-8.6938, device='cuda:0') tensor(2.0515e-05, device='cuda:0') tensor(1.6757e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.182539
Average total loss: 2.485124
tensor(-8.7470, device='cuda:0') tensor(2.0507e-05, device='cuda:0') tensor(1.5889e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302584
Average KL loss: 0.173177
Average total loss: 2.475762
tensor(-8.7991, device='cuda:0') tensor(2.0504e-05, device='cuda:0') tensor(1.5083e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.164487
Average total loss: 2.467072
tensor(-8.8500, device='cuda:0') tensor(2.0498e-05, device='cuda:0') tensor(1.4335e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.156406
Average total loss: 2.458991
tensor(-8.8998, device='cuda:0') tensor(2.0505e-05, device='cuda:0') tensor(1.3638e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.148877
Average total loss: 2.451462
tensor(-8.9486, device='cuda:0') tensor(2.0511e-05, device='cuda:0') tensor(1.2988e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302583
Average KL loss: 0.141849
Average total loss: 2.444431
tensor(-8.9965, device='cuda:0') tensor(2.0512e-05, device='cuda:0') tensor(1.2381e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.135279
Average total loss: 2.437863
tensor(-9.0435, device='cuda:0') tensor(2.0516e-05, device='cuda:0') tensor(1.1813e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.129126
Average total loss: 2.431711
tensor(-9.0896, device='cuda:0') tensor(2.0503e-05, device='cuda:0') tensor(1.1281e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.123356
Average total loss: 2.425941
tensor(-9.1349, device='cuda:0') tensor(2.0496e-05, device='cuda:0') tensor(1.0781e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.117937
Average total loss: 2.420523
tensor(-9.1794, device='cuda:0') tensor(2.0309e-05, device='cuda:0') tensor(1.0312e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302587
Average KL loss: 0.112842
Average total loss: 2.415429
tensor(-9.2232, device='cuda:0') tensor(2.0632e-05, device='cuda:0') tensor(9.8700e-09, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302586
Average KL loss: 0.108044
Average total loss: 2.410630
tensor(-9.2663, device='cuda:0') tensor(2.0703e-05, device='cuda:0') tensor(9.4539e-09, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.103521
Average total loss: 2.406106
tensor(-9.3088, device='cuda:0') tensor(2.0686e-05, device='cuda:0') tensor(9.0612e-09, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.099252
Average total loss: 2.401836
tensor(-9.3506, device='cuda:0') tensor(2.0670e-05, device='cuda:0') tensor(8.6903e-09, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.095218
Average total loss: 2.397803
tensor(-9.3917, device='cuda:0') tensor(2.0655e-05, device='cuda:0') tensor(8.3398e-09, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.091402
Average total loss: 2.393987
tensor(-9.4323, device='cuda:0') tensor(2.0640e-05, device='cuda:0') tensor(8.0080e-09, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.087789
Average total loss: 2.390374
tensor(-9.4724, device='cuda:0') tensor(2.0618e-05, device='cuda:0') tensor(7.6936e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.084365
Average total loss: 2.386950
tensor(-9.5119, device='cuda:0') tensor(2.0604e-05, device='cuda:0') tensor(7.3955e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.081116
Average total loss: 2.383701
tensor(-9.5509, device='cuda:0') tensor(2.0591e-05, device='cuda:0') tensor(7.1126e-09, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.078031
Average total loss: 2.380616
tensor(-9.5895, device='cuda:0') tensor(2.0578e-05, device='cuda:0') tensor(6.8439e-09, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.075100
Average total loss: 2.377685
tensor(-9.6275, device='cuda:0') tensor(2.0566e-05, device='cuda:0') tensor(6.5883e-09, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.072312
Average total loss: 2.374896
tensor(-9.6651, device='cuda:0') tensor(2.0554e-05, device='cuda:0') tensor(6.3452e-09, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.069657
Average total loss: 2.372242
tensor(-9.7023, device='cuda:0') tensor(2.0543e-05, device='cuda:0') tensor(6.1136e-09, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.067129
Average total loss: 2.369714
tensor(-9.7391, device='cuda:0') tensor(2.0532e-05, device='cuda:0') tensor(5.8930e-09, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.064718
Average total loss: 2.367303
tensor(-9.7754, device='cuda:0') tensor(2.0501e-05, device='cuda:0') tensor(5.6825e-09, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.062419
Average total loss: 2.365004
tensor(-9.8114, device='cuda:0') tensor(2.0490e-05, device='cuda:0') tensor(5.4817e-09, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.060223
Average total loss: 2.362808
tensor(-9.8470, device='cuda:0') tensor(2.0561e-05, device='cuda:0') tensor(5.2899e-09, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.058126
Average total loss: 2.360711
tensor(-9.8823, device='cuda:0') tensor(2.0551e-05, device='cuda:0') tensor(5.1066e-09, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.056121
Average total loss: 2.358706
tensor(-9.9172, device='cuda:0') tensor(2.0538e-05, device='cuda:0') tensor(4.9313e-09, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.054204
Average total loss: 2.356789
tensor(-9.9518, device='cuda:0') tensor(2.0527e-05, device='cuda:0') tensor(4.7637e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.052369
Average total loss: 2.354955
tensor(-9.9861, device='cuda:0') tensor(2.0493e-05, device='cuda:0') tensor(4.6032e-09, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.050612
Average total loss: 2.353197
tensor(-10.0201, device='cuda:0') tensor(2.0480e-05, device='cuda:0') tensor(4.4494e-09, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.048928
Average total loss: 2.351513
tensor(-10.0538, device='cuda:0') tensor(2.0468e-05, device='cuda:0') tensor(4.3021e-09, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.047315
Average total loss: 2.349900
tensor(-10.0872, device='cuda:0') tensor(2.0457e-05, device='cuda:0') tensor(4.1608e-09, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.045767
Average total loss: 2.348352
tensor(-10.1203, device='cuda:0') tensor(2.0446e-05, device='cuda:0') tensor(4.0252e-09, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302580
Average KL loss: 0.044282
Average total loss: 2.346862
tensor(-10.1531, device='cuda:0') tensor(2.1017e-05, device='cuda:0') tensor(3.8952e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.042856
Average total loss: 2.345441
tensor(-10.1857, device='cuda:0') tensor(2.1124e-05, device='cuda:0') tensor(3.7702e-09, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.041487
Average total loss: 2.344071
tensor(-10.2181, device='cuda:0') tensor(2.1067e-05, device='cuda:0') tensor(3.6502e-09, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.040171
Average total loss: 2.342756
tensor(-10.2502, device='cuda:0') tensor(2.1009e-05, device='cuda:0') tensor(3.5349e-09, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.038906
Average total loss: 2.341491
tensor(-10.2821, device='cuda:0') tensor(2.0958e-05, device='cuda:0') tensor(3.4240e-09, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.037689
Average total loss: 2.340274
tensor(-10.3137, device='cuda:0') tensor(2.0913e-05, device='cuda:0') tensor(3.3173e-09, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.036519
Average total loss: 2.339103
tensor(-10.3452, device='cuda:0') tensor(2.0888e-05, device='cuda:0') tensor(3.2146e-09, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.035392
Average total loss: 2.337977
tensor(-10.3764, device='cuda:0') tensor(2.0860e-05, device='cuda:0') tensor(3.1158e-09, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 0.034307
Average total loss: 2.336892
tensor(-10.4074, device='cuda:0') tensor(2.0826e-05, device='cuda:0') tensor(3.0206e-09, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.033263
Average total loss: 2.335848
tensor(-10.4382, device='cuda:0') tensor(2.0795e-05, device='cuda:0') tensor(2.9289e-09, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302583
Average KL loss: 0.032256
Average total loss: 2.334839
tensor(-10.4689, device='cuda:0') tensor(2.1489e-05, device='cuda:0') tensor(2.8406e-09, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.031286
Average total loss: 2.333871
tensor(-10.4993, device='cuda:0') tensor(2.1471e-05, device='cuda:0') tensor(2.7554e-09, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.030350
Average total loss: 2.332935
tensor(-10.5296, device='cuda:0') tensor(2.1330e-05, device='cuda:0') tensor(2.6733e-09, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.029448
Average total loss: 2.332033
tensor(-10.5597, device='cuda:0') tensor(2.1199e-05, device='cuda:0') tensor(2.5940e-09, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.028578
Average total loss: 2.331162
tensor(-10.5896, device='cuda:0') tensor(2.0849e-05, device='cuda:0') tensor(2.5176e-09, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 0.027737
Average total loss: 2.330322
tensor(-10.6194, device='cuda:0') tensor(2.0486e-05, device='cuda:0') tensor(2.4438e-09, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.026926
Average total loss: 2.329511
tensor(-10.6490, device='cuda:0') tensor(2.0474e-05, device='cuda:0') tensor(2.3725e-09, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 0.026143
Average total loss: 2.328728
tensor(-10.6784, device='cuda:0') tensor(2.0463e-05, device='cuda:0') tensor(2.3037e-09, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302585
Average KL loss: 0.025386
Average total loss: 2.327971
tensor(-10.7077, device='cuda:0') tensor(2.0453e-05, device='cuda:0') tensor(2.2372e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 0.024656
Average total loss: 2.327240
tensor(-10.7368, device='cuda:0') tensor(2.0442e-05, device='cuda:0') tensor(2.1729e-09, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.023949
Average total loss: 2.326534
tensor(-10.7658, device='cuda:0') tensor(2.0431e-05, device='cuda:0') tensor(2.1108e-09, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.023266
Average total loss: 2.325851
tensor(-10.7947, device='cuda:0') tensor(2.0420e-05, device='cuda:0') tensor(2.0508e-09, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.022606
Average total loss: 2.325191
tensor(-10.8234, device='cuda:0') tensor(2.0409e-05, device='cuda:0') tensor(1.9927e-09, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.021967
Average total loss: 2.324552
tensor(-10.8520, device='cuda:0') tensor(2.0398e-05, device='cuda:0') tensor(1.9365e-09, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302583
Average KL loss: 0.021349
Average total loss: 2.323932
tensor(-10.8805, device='cuda:0') tensor(2.0573e-05, device='cuda:0') tensor(1.8822e-09, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 0.020752
Average total loss: 2.323336
tensor(-10.9088, device='cuda:0') tensor(2.0558e-05, device='cuda:0') tensor(1.8296e-09, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 0.020173
Average total loss: 2.322758
tensor(-10.9370, device='cuda:0') tensor(2.0540e-05, device='cuda:0') tensor(1.7787e-09, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.019613
Average total loss: 2.322198
tensor(-10.9651, device='cuda:0') tensor(2.0522e-05, device='cuda:0') tensor(1.7294e-09, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 0.019071
Average total loss: 2.321655
tensor(-10.9931, device='cuda:0') tensor(2.0504e-05, device='cuda:0') tensor(1.6817e-09, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.018545
Average total loss: 2.321130
tensor(-11.0210, device='cuda:0') tensor(2.0507e-05, device='cuda:0') tensor(1.6355e-09, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 0.018037
Average total loss: 2.320622
tensor(-11.0487, device='cuda:0') tensor(2.0491e-05, device='cuda:0') tensor(1.5907e-09, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.017544
Average total loss: 2.320129
tensor(-11.0764, device='cuda:0') tensor(2.0473e-05, device='cuda:0') tensor(1.5474e-09, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 0.017066
Average total loss: 2.319651
tensor(-11.1039, device='cuda:0') tensor(2.0454e-05, device='cuda:0') tensor(1.5053e-09, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.016604
Average total loss: 2.319188
tensor(-11.1313, device='cuda:0') tensor(2.0436e-05, device='cuda:0') tensor(1.4646e-09, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.016155
Average total loss: 2.318740
tensor(-11.1587, device='cuda:0') tensor(2.0418e-05, device='cuda:0') tensor(1.4251e-09, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.015720
Average total loss: 2.318305
tensor(-11.1859, device='cuda:0') tensor(2.0400e-05, device='cuda:0') tensor(1.3868e-09, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.015298
Average total loss: 2.317883
tensor(-11.2131, device='cuda:0') tensor(2.0381e-05, device='cuda:0') tensor(1.3496e-09, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.014889
Average total loss: 2.317474
tensor(-11.2401, device='cuda:0') tensor(2.0363e-05, device='cuda:0') tensor(1.3136e-09, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.014492
Average total loss: 2.317077
tensor(-11.2671, device='cuda:0') tensor(2.0352e-05, device='cuda:0') tensor(1.2787e-09, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.014108
Average total loss: 2.316693
tensor(-11.2940, device='cuda:0') tensor(2.0334e-05, device='cuda:0') tensor(1.2448e-09, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.013734
Average total loss: 2.316319
tensor(-11.3207, device='cuda:0') tensor(2.0315e-05, device='cuda:0') tensor(1.2119e-09, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.013372
Average total loss: 2.315957
tensor(-11.3474, device='cuda:0') tensor(2.0296e-05, device='cuda:0') tensor(1.1800e-09, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.013020
Average total loss: 2.315605
tensor(-11.3741, device='cuda:0') tensor(2.0276e-05, device='cuda:0') tensor(1.1490e-09, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.012679
Average total loss: 2.315264
tensor(-11.4006, device='cuda:0') tensor(2.0257e-05, device='cuda:0') tensor(1.1189e-09, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.012347
Average total loss: 2.314932
tensor(-11.4270, device='cuda:0') tensor(2.0237e-05, device='cuda:0') tensor(1.0897e-09, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.012025
Average total loss: 2.314610
tensor(-11.4534, device='cuda:0') tensor(2.0217e-05, device='cuda:0') tensor(1.0613e-09, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.011713
Average total loss: 2.314298
tensor(-11.4797, device='cuda:0') tensor(2.0197e-05, device='cuda:0') tensor(1.0338e-09, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 0.011409
Average total loss: 2.313994
tensor(-11.5059, device='cuda:0') tensor(2.0177e-05, device='cuda:0') tensor(1.0071e-09, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 0.011115
Average total loss: 2.313700
tensor(-11.5320, device='cuda:0') tensor(2.0158e-05, device='cuda:0') tensor(9.8107e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.010828
Average total loss: 2.313413
tensor(-11.5581, device='cuda:0') tensor(2.0137e-05, device='cuda:0') tensor(9.5583e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.010550
Average total loss: 2.313135
tensor(-11.5841, device='cuda:0') tensor(2.0116e-05, device='cuda:0') tensor(9.3131e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.010280
Average total loss: 2.312865
tensor(-11.6100, device='cuda:0') tensor(2.0094e-05, device='cuda:0') tensor(9.0749e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.010017
Average total loss: 2.312602
tensor(-11.6358, device='cuda:0') tensor(2.0072e-05, device='cuda:0') tensor(8.8433e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.009762
Average total loss: 2.312347
tensor(-11.6616, device='cuda:0') tensor(2.0050e-05, device='cuda:0') tensor(8.6183e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.009514
Average total loss: 2.312099
tensor(-11.6873, device='cuda:0') tensor(2.0028e-05, device='cuda:0') tensor(8.3996e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302586
Average KL loss: 0.009273
Average total loss: 2.311858
tensor(-11.7130, device='cuda:0') tensor(1.9383e-05, device='cuda:0') tensor(8.1870e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 0.009038
Average total loss: 2.311623
tensor(-11.7385, device='cuda:0') tensor(1.9359e-05, device='cuda:0') tensor(7.9802e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.008810
Average total loss: 2.311395
tensor(-11.7640, device='cuda:0') tensor(1.9337e-05, device='cuda:0') tensor(7.7793e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.008589
Average total loss: 2.311174
tensor(-11.7895, device='cuda:0') tensor(1.9314e-05, device='cuda:0') tensor(7.5839e-10, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.008373
Average total loss: 2.310958
tensor(-11.8149, device='cuda:0') tensor(1.9291e-05, device='cuda:0') tensor(7.3938e-10, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.008164
Average total loss: 2.310748
tensor(-11.8402, device='cuda:0') tensor(1.9268e-05, device='cuda:0') tensor(7.2090e-10, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.007960
Average total loss: 2.310545
tensor(-11.8654, device='cuda:0') tensor(1.9244e-05, device='cuda:0') tensor(7.0293e-10, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.007762
Average total loss: 2.310347
tensor(-11.8906, device='cuda:0') tensor(1.9220e-05, device='cuda:0') tensor(6.8544e-10, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.007569
Average total loss: 2.310154
tensor(-11.9157, device='cuda:0') tensor(1.9196e-05, device='cuda:0') tensor(6.6844e-10, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 0.007381
Average total loss: 2.309966
tensor(-11.9408, device='cuda:0') tensor(1.9171e-05, device='cuda:0') tensor(6.5189e-10, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.007199
Average total loss: 2.309784
tensor(-11.9658, device='cuda:0') tensor(1.9146e-05, device='cuda:0') tensor(6.3579e-10, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.007021
Average total loss: 2.309606
tensor(-11.9908, device='cuda:0') tensor(1.9120e-05, device='cuda:0') tensor(6.2013e-10, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.006848
Average total loss: 2.309433
tensor(-12.0156, device='cuda:0') tensor(1.9094e-05, device='cuda:0') tensor(6.0488e-10, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.006680
Average total loss: 2.309265
tensor(-12.0405, device='cuda:0') tensor(1.9067e-05, device='cuda:0') tensor(5.9005e-10, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.006517
Average total loss: 2.309102
tensor(-12.0652, device='cuda:0') tensor(1.9040e-05, device='cuda:0') tensor(5.7562e-10, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.006357
Average total loss: 2.308942
tensor(-12.0899, device='cuda:0') tensor(1.9013e-05, device='cuda:0') tensor(5.6157e-10, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.006202
Average total loss: 2.308787
tensor(-12.1146, device='cuda:0') tensor(1.8984e-05, device='cuda:0') tensor(5.4789e-10, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.006052
Average total loss: 2.308636
tensor(-12.1392, device='cuda:0') tensor(1.8956e-05, device='cuda:0') tensor(5.3458e-10, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.005905
Average total loss: 2.308490
tensor(-12.1637, device='cuda:0') tensor(1.8927e-05, device='cuda:0') tensor(5.2162e-10, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.005762
Average total loss: 2.308347
tensor(-12.1882, device='cuda:0') tensor(1.8897e-05, device='cuda:0') tensor(5.0901e-10, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.005623
Average total loss: 2.308207
tensor(-12.2126, device='cuda:0') tensor(1.8867e-05, device='cuda:0') tensor(4.9673e-10, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.005487
Average total loss: 2.308072
tensor(-12.2370, device='cuda:0') tensor(1.8836e-05, device='cuda:0') tensor(4.8477e-10, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.005355
Average total loss: 2.307940
tensor(-12.2613, device='cuda:0') tensor(1.8805e-05, device='cuda:0') tensor(4.7313e-10, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.005227
Average total loss: 2.307812
tensor(-12.2856, device='cuda:0') tensor(1.8773e-05, device='cuda:0') tensor(4.6179e-10, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 0.005102
Average total loss: 2.307686
tensor(-12.3098, device='cuda:0') tensor(1.8740e-05, device='cuda:0') tensor(4.5075e-10, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.004980
Average total loss: 2.307565
tensor(-12.3339, device='cuda:0') tensor(1.8707e-05, device='cuda:0') tensor(4.4000e-10, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.004861
Average total loss: 2.307446
tensor(-12.3580, device='cuda:0') tensor(1.8673e-05, device='cuda:0') tensor(4.2953e-10, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.004746
Average total loss: 2.307330
tensor(-12.3820, device='cuda:0') tensor(1.8639e-05, device='cuda:0') tensor(4.1933e-10, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.004633
Average total loss: 2.307218
tensor(-12.4060, device='cuda:0') tensor(1.8604e-05, device='cuda:0') tensor(4.0940e-10, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.004523
Average total loss: 2.307108
tensor(-12.4299, device='cuda:0') tensor(1.8568e-05, device='cuda:0') tensor(3.9972e-10, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.004417
Average total loss: 2.307001
tensor(-12.4538, device='cuda:0') tensor(1.8532e-05, device='cuda:0') tensor(3.9030e-10, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.004313
Average total loss: 2.306897
tensor(-12.4776, device='cuda:0') tensor(1.8495e-05, device='cuda:0') tensor(3.8112e-10, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.004211
Average total loss: 2.306796
tensor(-12.5013, device='cuda:0') tensor(1.8457e-05, device='cuda:0') tensor(3.7217e-10, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.004113
Average total loss: 2.306697
tensor(-12.5250, device='cuda:0') tensor(1.8419e-05, device='cuda:0') tensor(3.6346e-10, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.004016
Average total loss: 2.306601
tensor(-12.5487, device='cuda:0') tensor(1.8379e-05, device='cuda:0') tensor(3.5497e-10, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.003923
Average total loss: 2.306507
tensor(-12.5722, device='cuda:0') tensor(1.8339e-05, device='cuda:0') tensor(3.4670e-10, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.003831
Average total loss: 2.306416
tensor(-12.5958, device='cuda:0') tensor(1.8299e-05, device='cuda:0') tensor(3.3864e-10, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.003742
Average total loss: 2.306327
tensor(-12.6192, device='cuda:0') tensor(1.8257e-05, device='cuda:0') tensor(3.3078e-10, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.003656
Average total loss: 2.306241
tensor(-12.6426, device='cuda:0') tensor(1.8215e-05, device='cuda:0') tensor(3.2313e-10, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.003571
Average total loss: 2.306156
tensor(-12.6660, device='cuda:0') tensor(1.8172e-05, device='cuda:0') tensor(3.1567e-10, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.003489
Average total loss: 2.306074
tensor(-12.6893, device='cuda:0') tensor(1.8128e-05, device='cuda:0') tensor(3.0841e-10, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.003409
Average total loss: 2.305994
tensor(-12.7125, device='cuda:0') tensor(1.8083e-05, device='cuda:0') tensor(3.0132e-10, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.003330
Average total loss: 2.305915
tensor(-12.7357, device='cuda:0') tensor(1.8038e-05, device='cuda:0') tensor(2.9442e-10, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.003254
Average total loss: 2.305839
tensor(-12.7588, device='cuda:0') tensor(1.7991e-05, device='cuda:0') tensor(2.8769e-10, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302585
Average KL loss: 0.003180
Average total loss: 2.305765
tensor(-12.7818, device='cuda:0') tensor(1.7944e-05, device='cuda:0') tensor(2.8114e-10, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.003108
Average total loss: 2.305693
tensor(-12.8048, device='cuda:0') tensor(1.7896e-05, device='cuda:0') tensor(2.7475e-10, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.003037
Average total loss: 2.305622
tensor(-12.8278, device='cuda:0') tensor(1.7847e-05, device='cuda:0') tensor(2.6852e-10, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.002968
Average total loss: 2.305553
tensor(-12.8506, device='cuda:0') tensor(1.7797e-05, device='cuda:0') tensor(2.6244e-10, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.002901
Average total loss: 2.305486
tensor(-12.8735, device='cuda:0') tensor(1.7747e-05, device='cuda:0') tensor(2.5652e-10, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.002836
Average total loss: 2.305421
tensor(-12.8962, device='cuda:0') tensor(1.7695e-05, device='cuda:0') tensor(2.5075e-10, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.002772
Average total loss: 2.305357
tensor(-12.9189, device='cuda:0') tensor(1.7643e-05, device='cuda:0') tensor(2.4513e-10, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302585
Average KL loss: 0.002710
Average total loss: 2.305295
tensor(-12.9415, device='cuda:0') tensor(1.7590e-05, device='cuda:0') tensor(2.3964e-10, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.002650
Average total loss: 2.305234
tensor(-12.9641, device='cuda:0') tensor(1.7535e-05, device='cuda:0') tensor(2.3430e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.002590
Average total loss: 2.305175
tensor(-12.9866, device='cuda:0') tensor(1.7480e-05, device='cuda:0') tensor(2.2908e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.002533
Average total loss: 2.305118
tensor(-13.0090, device='cuda:0') tensor(1.7424e-05, device='cuda:0') tensor(2.2400e-10, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.002477
Average total loss: 2.305062
tensor(-13.0314, device='cuda:0') tensor(1.7367e-05, device='cuda:0') tensor(2.1905e-10, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.002422
Average total loss: 2.305007
tensor(-13.0537, device='cuda:0') tensor(1.7309e-05, device='cuda:0') tensor(2.1421e-10, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.002369
Average total loss: 2.304954
tensor(-13.0760, device='cuda:0') tensor(1.7249e-05, device='cuda:0') tensor(2.0950e-10, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302585
Average KL loss: 0.002317
Average total loss: 2.304902
tensor(-13.0981, device='cuda:0') tensor(1.7189e-05, device='cuda:0') tensor(2.0491e-10, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302585
Average KL loss: 0.002266
Average total loss: 2.304851
tensor(-13.1202, device='cuda:0') tensor(1.7129e-05, device='cuda:0') tensor(2.0043e-10, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302585
Average KL loss: 0.002217
Average total loss: 2.304801
tensor(-13.1423, device='cuda:0') tensor(1.7066e-05, device='cuda:0') tensor(1.9606e-10, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302585
Average KL loss: 0.002168
Average total loss: 2.304753
tensor(-13.1643, device='cuda:0') tensor(1.7004e-05, device='cuda:0') tensor(1.9180e-10, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302585
Average KL loss: 0.002121
Average total loss: 2.304706
tensor(-13.1862, device='cuda:0') tensor(1.6940e-05, device='cuda:0') tensor(1.8764e-10, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302585
Average KL loss: 0.002075
Average total loss: 2.304660
tensor(-13.2080, device='cuda:0') tensor(1.6875e-05, device='cuda:0') tensor(1.8359e-10, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-13.2298, device='cuda:0') tensor(1.6809e-05, device='cuda:0') tensor(1.7964e-10, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302585
Average KL loss: 0.001987
Average total loss: 2.304572
tensor(-13.2514, device='cuda:0') tensor(1.6742e-05, device='cuda:0') tensor(1.7578e-10, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302585
Average KL loss: 0.001944
Average total loss: 2.304529
tensor(-13.2731, device='cuda:0') tensor(1.6674e-05, device='cuda:0') tensor(1.7202e-10, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302585
Average KL loss: 0.001903
Average total loss: 2.304488
tensor(-13.2946, device='cuda:0') tensor(1.6605e-05, device='cuda:0') tensor(1.6835e-10, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302585
Average KL loss: 0.001862
Average total loss: 2.304447
tensor(-13.3161, device='cuda:0') tensor(1.6535e-05, device='cuda:0') tensor(1.6478e-10, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302585
Average KL loss: 0.001823
Average total loss: 2.304408
tensor(-13.3375, device='cuda:0') tensor(1.6464e-05, device='cuda:0') tensor(1.6129e-10, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302585
Average KL loss: 0.001784
Average total loss: 2.304369
tensor(-13.3588, device='cuda:0') tensor(1.6392e-05, device='cuda:0') tensor(1.5788e-10, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302585
Average KL loss: 0.001747
Average total loss: 2.304332
tensor(-13.3801, device='cuda:0') tensor(1.6319e-05, device='cuda:0') tensor(1.5456e-10, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302585
Average KL loss: 0.001710
Average total loss: 2.304295
tensor(-13.4013, device='cuda:0') tensor(1.6245e-05, device='cuda:0') tensor(1.5132e-10, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302585
Average KL loss: 0.001674
Average total loss: 2.304259
tensor(-13.4224, device='cuda:0') tensor(1.6169e-05, device='cuda:0') tensor(1.4816e-10, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302585
Average KL loss: 0.001639
Average total loss: 2.304224
tensor(-13.4434, device='cuda:0') tensor(1.6093e-05, device='cuda:0') tensor(1.4508e-10, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302585
Average KL loss: 0.001605
Average total loss: 2.304190
tensor(-13.4643, device='cuda:0') tensor(1.6016e-05, device='cuda:0') tensor(1.4207e-10, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302585
Average KL loss: 0.001572
Average total loss: 2.304157
 Percentile value: -13.485240936279297
Non-zero model percentage: 30.000001907348633%, Non-zero mask percentage: 30.000001907348633%

--- Pruning Level [1/7]: ---
conv1.weight         | nonzeros =     309 /    1728             ( 17.88%) | total_pruned =    1419 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
bn1.bias             | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3484 /   36864             (  9.45%) | total_pruned =   33380 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6688 /   36864             ( 18.14%) | total_pruned =   30176 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6818 /   36864             ( 18.50%) | total_pruned =   30046 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7402 /   36864             ( 20.08%) | total_pruned =   29462 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20151 /   73728             ( 27.33%) | total_pruned =   53577 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   33885 /  147456             ( 22.98%) | total_pruned =  113571 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2232 /    8192             ( 27.25%) | total_pruned =    5960 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   15646 /  147456             ( 10.61%) | total_pruned =  131810 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   11822 /  147456             (  8.02%) | total_pruned =  135634 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   67209 /  294912             ( 22.79%) | total_pruned =  227703 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   67108 /  589824             ( 11.38%) | total_pruned =  522716 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4258 /   32768             ( 12.99%) | total_pruned =   28510 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   25845 /  589824             (  4.38%) | total_pruned =  563979 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   18804 /  589824             (  3.19%) | total_pruned =  571020 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   88441 / 1179648             (  7.50%) | total_pruned = 1091207 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  180105 / 2359296             (  7.63%) | total_pruned = 2179191 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     235 /     512             ( 45.90%) | total_pruned =     277 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     284 /     512             ( 55.47%) | total_pruned =     228 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   10608 /  131072             (  8.09%) | total_pruned =  120464 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     292 /     512             ( 57.03%) | total_pruned =     220 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  738150 / 2359296             ( 31.29%) | total_pruned = 1621146 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2035652 / 2359296             ( 86.28%) | total_pruned =  323644 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3201 /    5120             ( 62.52%) | total_pruned =    1919 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3353629, pruned : 7825133, total: 11178762, Compression rate :       3.33x  ( 70.00% pruned)
Train Epoch: 71/200 Loss: 0.012325 Accuracy: 84.44 100.00 % Best test Accuracy: 84.66%
tensor(-13.4852, device='cuda:0') tensor(1.7025e-05, device='cuda:0') tensor(1.3914e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-13.5509, device='cuda:0') tensor(1.3624e-05, device='cuda:0') tensor(1.3030e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001413
Average total loss: 2.303998
tensor(-13.6127, device='cuda:0') tensor(1.1317e-05, device='cuda:0') tensor(1.2249e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001331
Average total loss: 2.303915
tensor(-13.6709, device='cuda:0') tensor(9.6060e-06, device='cuda:0') tensor(1.1556e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001257
Average total loss: 2.303842
tensor(-13.7259, device='cuda:0') tensor(8.2803e-06, device='cuda:0') tensor(1.0937e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001192
Average total loss: 2.303777
tensor(-13.7780, device='cuda:0') tensor(7.2244e-06, device='cuda:0') tensor(1.0382e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001133
Average total loss: 2.303718
tensor(-13.8275, device='cuda:0') tensor(6.3661e-06, device='cuda:0') tensor(9.8804e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001079
Average total loss: 2.303664
tensor(-13.8747, device='cuda:0') tensor(5.6569e-06, device='cuda:0') tensor(9.4251e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001031
Average total loss: 2.303615
tensor(-13.9198, device='cuda:0') tensor(5.0637e-06, device='cuda:0') tensor(9.0099e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000986
Average total loss: 2.303571
tensor(-13.9629, device='cuda:0') tensor(4.5614e-06, device='cuda:0') tensor(8.6298e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000945
Average total loss: 2.303530
tensor(-14.0042, device='cuda:0') tensor(4.1321e-06, device='cuda:0') tensor(8.2805e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000908
Average total loss: 2.303493
tensor(-14.0439, device='cuda:0') tensor(3.7619e-06, device='cuda:0') tensor(7.9585e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000873
Average total loss: 2.303458
tensor(-14.0820, device='cuda:0') tensor(3.4402e-06, device='cuda:0') tensor(7.6605e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000841
Average total loss: 2.303426
tensor(-14.1188, device='cuda:0') tensor(3.1588e-06, device='cuda:0') tensor(7.3841e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-14.1542, device='cuda:0') tensor(2.9112e-06, device='cuda:0') tensor(7.1269e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000783
Average total loss: 2.303368
tensor(-14.1885, device='cuda:0') tensor(2.6919e-06, device='cuda:0') tensor(6.8871e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000757
Average total loss: 2.303342
tensor(-14.2215, device='cuda:0') tensor(2.4969e-06, device='cuda:0') tensor(6.6628e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000733
Average total loss: 2.303318
tensor(-14.2536, device='cuda:0') tensor(2.3226e-06, device='cuda:0') tensor(6.4528e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2846, device='cuda:0') tensor(2.1662e-06, device='cuda:0') tensor(6.2555e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000689
Average total loss: 2.303274
tensor(-14.3147, device='cuda:0') tensor(2.0252e-06, device='cuda:0') tensor(6.0700e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000669
Average total loss: 2.303254
tensor(-14.3440, device='cuda:0') tensor(1.8978e-06, device='cuda:0') tensor(5.8951e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000650
Average total loss: 2.303235
tensor(-14.3724, device='cuda:0') tensor(1.7821e-06, device='cuda:0') tensor(5.7301e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.4000, device='cuda:0') tensor(1.6769e-06, device='cuda:0') tensor(5.5740e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000615
Average total loss: 2.303200
tensor(-14.4269, device='cuda:0') tensor(1.5807e-06, device='cuda:0') tensor(5.4262e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000599
Average total loss: 2.303184
tensor(-14.4530, device='cuda:0') tensor(1.4926e-06, device='cuda:0') tensor(5.2860e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000584
Average total loss: 2.303168
tensor(-14.4785, device='cuda:0') tensor(1.4118e-06, device='cuda:0') tensor(5.1529e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000569
Average total loss: 2.303154
tensor(-14.5034, device='cuda:0') tensor(1.3374e-06, device='cuda:0') tensor(5.0264e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000555
Average total loss: 2.303140
tensor(-14.5277, device='cuda:0') tensor(1.2688e-06, device='cuda:0') tensor(4.9059e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000542
Average total loss: 2.303127
tensor(-14.5514, device='cuda:0') tensor(1.2054e-06, device='cuda:0') tensor(4.7910e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000535
Average total loss: 2.303120
tensor(-14.5537, device='cuda:0') tensor(1.1995e-06, device='cuda:0') tensor(4.7798e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000534
Average total loss: 2.303119
tensor(-14.5560, device='cuda:0') tensor(1.1937e-06, device='cuda:0') tensor(4.7687e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000532
Average total loss: 2.303117
tensor(-14.5584, device='cuda:0') tensor(1.1881e-06, device='cuda:0') tensor(4.7575e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000531
Average total loss: 2.303116
tensor(-14.5607, device='cuda:0') tensor(1.1822e-06, device='cuda:0') tensor(4.7465e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000530
Average total loss: 2.303115
tensor(-14.5630, device='cuda:0') tensor(1.1760e-06, device='cuda:0') tensor(4.7357e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000529
Average total loss: 2.303114
tensor(-14.5653, device='cuda:0') tensor(1.1699e-06, device='cuda:0') tensor(4.7248e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000528
Average total loss: 2.303112
tensor(-14.5676, device='cuda:0') tensor(1.1639e-06, device='cuda:0') tensor(4.7140e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000526
Average total loss: 2.303111
tensor(-14.5698, device='cuda:0') tensor(1.1582e-06, device='cuda:0') tensor(4.7032e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000525
Average total loss: 2.303110
tensor(-14.5721, device='cuda:0') tensor(1.1525e-06, device='cuda:0') tensor(4.6925e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000524
Average total loss: 2.303109
tensor(-14.5744, device='cuda:0') tensor(1.1470e-06, device='cuda:0') tensor(4.6818e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000523
Average total loss: 2.303108
tensor(-14.5767, device='cuda:0') tensor(1.1416e-06, device='cuda:0') tensor(4.6710e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5769, device='cuda:0') tensor(1.1413e-06, device='cuda:0') tensor(4.6700e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5772, device='cuda:0') tensor(1.1408e-06, device='cuda:0') tensor(4.6689e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5774, device='cuda:0') tensor(1.1405e-06, device='cuda:0') tensor(4.6678e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5776, device='cuda:0') tensor(1.1400e-06, device='cuda:0') tensor(4.6667e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5779, device='cuda:0') tensor(1.1397e-06, device='cuda:0') tensor(4.6656e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5781, device='cuda:0') tensor(1.1392e-06, device='cuda:0') tensor(4.6645e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5783, device='cuda:0') tensor(1.1389e-06, device='cuda:0') tensor(4.6634e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5786, device='cuda:0') tensor(1.1384e-06, device='cuda:0') tensor(4.6623e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5788, device='cuda:0') tensor(1.1381e-06, device='cuda:0') tensor(4.6612e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5791, device='cuda:0') tensor(1.1376e-06, device='cuda:0') tensor(4.6601e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1373e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1365e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1358e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1350e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1343e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1335e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1328e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1320e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1313e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1305e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1298e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
 Percentile value: -14.579283714294434
Non-zero model percentage: 9.000003814697266%, Non-zero mask percentage: 9.000003814697266%

--- Pruning Level [2/7]: ---
conv1.weight         | nonzeros =     288 /    1728             ( 16.67%) | total_pruned =    1440 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2452 /   36864             (  6.65%) | total_pruned =   34412 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4335 /   36864             ( 11.76%) | total_pruned =   32529 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4291 /   36864             ( 11.64%) | total_pruned =   32573 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4506 /   36864             ( 12.22%) | total_pruned =   32358 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12916 /   73728             ( 17.52%) | total_pruned =   60812 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   20480 /  147456             ( 13.89%) | total_pruned =  126976 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1570 /    8192             ( 19.17%) | total_pruned =    6622 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    6843 /  147456             (  4.64%) | total_pruned =  140613 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    5160 /  147456             (  3.50%) | total_pruned =  142296 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   40669 /  294912             ( 13.79%) | total_pruned =  254243 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     118 /     256             ( 46.09%) | total_pruned =     138 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44331 /  589824             (  7.52%) | total_pruned =  545493 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4258 /   32768             ( 12.99%) | total_pruned =   28510 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   25845 /  589824             (  4.38%) | total_pruned =  563979 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   18804 /  589824             (  3.19%) | total_pruned =  571020 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   88441 / 1179648             (  7.50%) | total_pruned = 1091207 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  180105 / 2359296             (  7.63%) | total_pruned = 2179191 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     235 /     512             ( 45.90%) | total_pruned =     277 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     284 /     512             ( 55.47%) | total_pruned =     228 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   10608 /  131072             (  8.09%) | total_pruned =  120464 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     292 /     512             ( 57.03%) | total_pruned =     220 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  280639 / 2359296             ( 11.90%) | total_pruned = 2078657 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     244 /     512             ( 47.66%) | total_pruned =     268 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  241650 / 2359296             ( 10.24%) | total_pruned = 2117646 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     237 /     512             ( 46.29%) | total_pruned =     275 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3197 /    5120             ( 62.44%) | total_pruned =    1923 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1006089, pruned : 10172673, total: 11178762, Compression rate :      11.11x  ( 91.00% pruned)
Train Epoch: 82/200 Loss: 0.018689 Accuracy: 83.60 100.00 % Best test Accuracy: 84.31%
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000515
Average total loss: 2.303100
tensor(-14.6018, device='cuda:0') tensor(1.0748e-06, device='cuda:0') tensor(4.5552e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000504
Average total loss: 2.303089
tensor(-14.6239, device='cuda:0') tensor(1.0243e-06, device='cuda:0') tensor(4.4558e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000493
Average total loss: 2.303078
tensor(-14.6455, device='cuda:0') tensor(9.7727e-07, device='cuda:0') tensor(4.3606e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000482
Average total loss: 2.303067
tensor(-14.6666, device='cuda:0') tensor(9.3345e-07, device='cuda:0') tensor(4.2694e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000472
Average total loss: 2.303057
tensor(-14.6873, device='cuda:0') tensor(8.9252e-07, device='cuda:0') tensor(4.1820e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000463
Average total loss: 2.303048
tensor(-14.7076, device='cuda:0') tensor(8.5424e-07, device='cuda:0') tensor(4.0980e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000454
Average total loss: 2.303039
tensor(-14.7275, device='cuda:0') tensor(8.1836e-07, device='cuda:0') tensor(4.0174e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000445
Average total loss: 2.303030
tensor(-14.7469, device='cuda:0') tensor(7.8469e-07, device='cuda:0') tensor(3.9399e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000436
Average total loss: 2.303021
tensor(-14.7661, device='cuda:0') tensor(7.5309e-07, device='cuda:0') tensor(3.8653e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000428
Average total loss: 2.303013
tensor(-14.7848, device='cuda:0') tensor(7.2339e-07, device='cuda:0') tensor(3.7935e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000420
Average total loss: 2.303005
tensor(-14.8032, device='cuda:0') tensor(6.9539e-07, device='cuda:0') tensor(3.7243e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000413
Average total loss: 2.302998
tensor(-14.8213, device='cuda:0') tensor(6.6898e-07, device='cuda:0') tensor(3.6576e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000409
Average total loss: 2.302993
tensor(-14.8231, device='cuda:0') tensor(6.6631e-07, device='cuda:0') tensor(3.6511e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000408
Average total loss: 2.302993
tensor(-14.8248, device='cuda:0') tensor(6.6373e-07, device='cuda:0') tensor(3.6446e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000407
Average total loss: 2.302992
tensor(-14.8266, device='cuda:0') tensor(6.6121e-07, device='cuda:0') tensor(3.6382e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000406
Average total loss: 2.302991
tensor(-14.8284, device='cuda:0') tensor(6.5876e-07, device='cuda:0') tensor(3.6317e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000406
Average total loss: 2.302991
tensor(-14.8302, device='cuda:0') tensor(6.5637e-07, device='cuda:0') tensor(3.6253e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000405
Average total loss: 2.302990
tensor(-14.8319, device='cuda:0') tensor(6.5404e-07, device='cuda:0') tensor(3.6188e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000404
Average total loss: 2.302989
tensor(-14.8337, device='cuda:0') tensor(6.5177e-07, device='cuda:0') tensor(3.6124e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000403
Average total loss: 2.302988
tensor(-14.8355, device='cuda:0') tensor(6.4954e-07, device='cuda:0') tensor(3.6060e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000403
Average total loss: 2.302988
tensor(-14.8373, device='cuda:0') tensor(6.4736e-07, device='cuda:0') tensor(3.5996e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000402
Average total loss: 2.302987
tensor(-14.8390, device='cuda:0') tensor(6.4520e-07, device='cuda:0') tensor(3.5932e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8408, device='cuda:0') tensor(6.4284e-07, device='cuda:0') tensor(3.5869e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8410, device='cuda:0') tensor(6.4278e-07, device='cuda:0') tensor(3.5862e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8412, device='cuda:0') tensor(6.4273e-07, device='cuda:0') tensor(3.5855e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8414, device='cuda:0') tensor(6.4267e-07, device='cuda:0') tensor(3.5849e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8416, device='cuda:0') tensor(6.4261e-07, device='cuda:0') tensor(3.5842e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302985
tensor(-14.8417, device='cuda:0') tensor(6.4256e-07, device='cuda:0') tensor(3.5835e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302985
tensor(-14.8419, device='cuda:0') tensor(6.4250e-07, device='cuda:0') tensor(3.5829e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8421, device='cuda:0') tensor(6.4245e-07, device='cuda:0') tensor(3.5822e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8423, device='cuda:0') tensor(6.4239e-07, device='cuda:0') tensor(3.5815e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8425, device='cuda:0') tensor(6.4233e-07, device='cuda:0') tensor(3.5809e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8427, device='cuda:0') tensor(6.4228e-07, device='cuda:0') tensor(3.5802e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
 Percentile value: -14.842838287353516
Non-zero model percentage: 2.7000038623809814%, Non-zero mask percentage: 2.7000038623809814%

--- Pruning Level [3/7]: ---
conv1.weight         | nonzeros =     272 /    1728             ( 15.74%) | total_pruned =    1456 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1593 /   36864             (  4.32%) | total_pruned =   35271 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2756 /   36864             (  7.48%) | total_pruned =   34108 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2468 /   36864             (  6.69%) | total_pruned =   34396 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2604 /   36864             (  7.06%) | total_pruned =   34260 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    7462 /   73728             ( 10.12%) | total_pruned =   66266 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   11338 /  147456             (  7.69%) | total_pruned =  136118 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1085 /    8192             ( 13.24%) | total_pruned =    7107 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2777 /  147456             (  1.88%) | total_pruned =  144679 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2098 /  147456             (  1.42%) | total_pruned =  145358 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   22784 /  294912             (  7.73%) | total_pruned =  272128 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   20977 /  589824             (  3.56%) | total_pruned =  568847 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     126 /     256             ( 49.22%) | total_pruned =     130 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1394 /   32768             (  4.25%) | total_pruned =   31374 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      86 /     256             ( 33.59%) | total_pruned =     170 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     120 /     256             ( 46.88%) | total_pruned =     136 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4347 /  589824             (  0.74%) | total_pruned =  585477 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      70 /     256             ( 27.34%) | total_pruned =     186 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      77 /     256             ( 30.08%) | total_pruned =     179 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3215 /  589824             (  0.55%) | total_pruned =  586609 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      71 /     256             ( 27.73%) | total_pruned =     185 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   24824 / 1179648             (  2.10%) | total_pruned = 1154824 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     201 /     512             ( 39.26%) | total_pruned =     311 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     219 /     512             ( 42.77%) | total_pruned =     293 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   48159 / 2359296             (  2.04%) | total_pruned = 2311137 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     206 /     512             ( 40.23%) | total_pruned =     306 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     260 /     512             ( 50.78%) | total_pruned =     252 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2977 /  131072             (  2.27%) | total_pruned =  128095 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     154 /     512             ( 30.08%) | total_pruned =     358 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   75316 / 2359296             (  3.19%) | total_pruned = 2283980 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   56015 / 2359296             (  2.37%) | total_pruned = 2303281 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     232 /     512             ( 45.31%) | total_pruned =     280 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3185 /    5120             ( 62.21%) | total_pruned =    1935 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 301827, pruned : 10876935, total: 11178762, Compression rate :      37.04x  ( 97.30% pruned)
Train Epoch: 188/200 Loss: 0.164288 Accuracy: 78.13 99.84 % Best test Accuracy: 80.97%
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000397
Average total loss: 2.302982
tensor(-14.8602, device='cuda:0') tensor(6.1885e-07, device='cuda:0') tensor(3.5179e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000390
Average total loss: 2.302975
tensor(-14.8773, device='cuda:0') tensor(5.9671e-07, device='cuda:0') tensor(3.4583e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000383
Average total loss: 2.302968
tensor(-14.8941, device='cuda:0') tensor(5.7571e-07, device='cuda:0') tensor(3.4007e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000377
Average total loss: 2.302962
tensor(-14.9106, device='cuda:0') tensor(5.5582e-07, device='cuda:0') tensor(3.3450e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000371
Average total loss: 2.302956
tensor(-14.9269, device='cuda:0') tensor(5.3694e-07, device='cuda:0') tensor(3.2910e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000365
Average total loss: 2.302950
tensor(-14.9429, device='cuda:0') tensor(5.1902e-07, device='cuda:0') tensor(3.2388e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000359
Average total loss: 2.302944
tensor(-14.9586, device='cuda:0') tensor(5.0196e-07, device='cuda:0') tensor(3.1882e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000354
Average total loss: 2.302939
tensor(-14.9741, device='cuda:0') tensor(4.8577e-07, device='cuda:0') tensor(3.1392e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000348
Average total loss: 2.302933
tensor(-14.9894, device='cuda:0') tensor(4.7033e-07, device='cuda:0') tensor(3.0917e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000343
Average total loss: 2.302928
tensor(-15.0044, device='cuda:0') tensor(4.5563e-07, device='cuda:0') tensor(3.0455e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000338
Average total loss: 2.302923
tensor(-15.0192, device='cuda:0') tensor(4.4162e-07, device='cuda:0') tensor(3.0008e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000333
Average total loss: 2.302918
tensor(-15.0338, device='cuda:0') tensor(4.2821e-07, device='cuda:0') tensor(2.9573e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000330
Average total loss: 2.302915
tensor(-15.0353, device='cuda:0') tensor(4.2699e-07, device='cuda:0') tensor(2.9530e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000330
Average total loss: 2.302915
tensor(-15.0367, device='cuda:0') tensor(4.2580e-07, device='cuda:0') tensor(2.9488e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000329
Average total loss: 2.302914
tensor(-15.0382, device='cuda:0') tensor(4.2465e-07, device='cuda:0') tensor(2.9445e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000329
Average total loss: 2.302914
tensor(-15.0396, device='cuda:0') tensor(4.2351e-07, device='cuda:0') tensor(2.9402e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-15.0411, device='cuda:0') tensor(4.2240e-07, device='cuda:0') tensor(2.9360e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-15.0425, device='cuda:0') tensor(4.2131e-07, device='cuda:0') tensor(2.9317e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302912
tensor(-15.0440, device='cuda:0') tensor(4.2024e-07, device='cuda:0') tensor(2.9275e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302912
tensor(-15.0454, device='cuda:0') tensor(4.1919e-07, device='cuda:0') tensor(2.9232e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302911
tensor(-15.0468, device='cuda:0') tensor(4.1814e-07, device='cuda:0') tensor(2.9190e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000326
Average total loss: 2.302911
tensor(-15.0483, device='cuda:0') tensor(4.1681e-07, device='cuda:0') tensor(2.9148e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000326
Average total loss: 2.302911
tensor(-15.0497, device='cuda:0') tensor(4.1528e-07, device='cuda:0') tensor(2.9108e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0498, device='cuda:0') tensor(4.1522e-07, device='cuda:0') tensor(2.9104e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0500, device='cuda:0') tensor(4.1516e-07, device='cuda:0') tensor(2.9099e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0501, device='cuda:0') tensor(4.1510e-07, device='cuda:0') tensor(2.9095e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0502, device='cuda:0') tensor(4.1504e-07, device='cuda:0') tensor(2.9091e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0504, device='cuda:0') tensor(4.1498e-07, device='cuda:0') tensor(2.9087e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0505, device='cuda:0') tensor(4.1492e-07, device='cuda:0') tensor(2.9083e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0507, device='cuda:0') tensor(4.1486e-07, device='cuda:0') tensor(2.9079e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0508, device='cuda:0') tensor(4.1480e-07, device='cuda:0') tensor(2.9075e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0509, device='cuda:0') tensor(4.1474e-07, device='cuda:0') tensor(2.9071e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0511, device='cuda:0') tensor(4.1468e-07, device='cuda:0') tensor(2.9067e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
 Percentile value: -15.051131248474121
Non-zero model percentage: 0.8100091218948364%, Non-zero mask percentage: 0.8100091218948364%

--- Pruning Level [4/7]: ---
conv1.weight         | nonzeros =     236 /    1728             ( 13.66%) | total_pruned =    1492 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     780 /   36864             (  2.12%) | total_pruned =   36084 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1210 /   36864             (  3.28%) | total_pruned =   35654 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     961 /   36864             (  2.61%) | total_pruned =   35903 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1029 /   36864             (  2.79%) | total_pruned =   35835 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2656 /   73728             (  3.60%) | total_pruned =   71072 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    3850 /  147456             (  2.61%) | total_pruned =  143606 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     574 /    8192             (  7.01%) | total_pruned =    7618 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      47 /     128             ( 36.72%) | total_pruned =      81 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     553 /  147456             (  0.38%) | total_pruned =  146903 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      32 /     128             ( 25.00%) | total_pruned =      96 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     484 /  147456             (  0.33%) | total_pruned =  146972 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8266 /  294912             (  2.80%) | total_pruned =  286646 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     109 /     256             ( 42.58%) | total_pruned =     147 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    8124 /  589824             (  1.38%) | total_pruned =  581700 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      82 /     256             ( 32.03%) | total_pruned =     174 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     554 /   32768             (  1.69%) | total_pruned =   32214 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     909 /  589824             (  0.15%) | total_pruned =  588915 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      50 /     256             ( 19.53%) | total_pruned =     206 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      53 /     256             ( 20.70%) | total_pruned =     203 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     703 /  589824             (  0.12%) | total_pruned =  589121 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      47 /     256             ( 18.36%) | total_pruned =     209 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8606 / 1179648             (  0.73%) | total_pruned = 1171042 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     182 /     512             ( 35.55%) | total_pruned =     330 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   12031 / 2359296             (  0.51%) | total_pruned = 2347265 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     173 /     512             ( 33.79%) | total_pruned =     339 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     886 /  131072             (  0.68%) | total_pruned =  130186 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     100 /     512             ( 19.53%) | total_pruned =     412 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   17786 / 2359296             (  0.75%) | total_pruned = 2341510 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     185 /     512             ( 36.13%) | total_pruned =     327 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     165 /     512             ( 32.23%) | total_pruned =     347 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   13664 / 2359296             (  0.58%) | total_pruned = 2345632 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     216 /     512             ( 42.19%) | total_pruned =     296 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3144 /    5120             ( 61.41%) | total_pruned =    1976 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 90549, pruned : 11088213, total: 11178762, Compression rate :     123.46x  ( 99.19% pruned)
Train Epoch: 154/200 Loss: 0.672334 Accuracy: 71.15 79.05 % Best test Accuracy: 71.45%
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000323
Average total loss: 2.302908
tensor(-15.0654, device='cuda:0') tensor(4.0249e-07, device='cuda:0') tensor(2.8655e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000318
Average total loss: 2.302903
tensor(-15.0793, device='cuda:0') tensor(3.9085e-07, device='cuda:0') tensor(2.8258e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000314
Average total loss: 2.302899
tensor(-15.0930, device='cuda:0') tensor(3.7969e-07, device='cuda:0') tensor(2.7873e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000310
Average total loss: 2.302894
tensor(-15.1066, device='cuda:0') tensor(3.6903e-07, device='cuda:0') tensor(2.7497e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-15.1200, device='cuda:0') tensor(3.5881e-07, device='cuda:0') tensor(2.7132e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000301
Average total loss: 2.302886
tensor(-15.1332, device='cuda:0') tensor(3.4899e-07, device='cuda:0') tensor(2.6776e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000297
Average total loss: 2.302882
tensor(-15.1462, device='cuda:0') tensor(3.3957e-07, device='cuda:0') tensor(2.6429e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000294
Average total loss: 2.302878
tensor(-15.1591, device='cuda:0') tensor(3.3055e-07, device='cuda:0') tensor(2.6091e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000290
Average total loss: 2.302875
tensor(-15.1718, device='cuda:0') tensor(3.2185e-07, device='cuda:0') tensor(2.5762e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000286
Average total loss: 2.302871
tensor(-15.1843, device='cuda:0') tensor(3.1352e-07, device='cuda:0') tensor(2.5441e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000283
Average total loss: 2.302868
tensor(-15.1967, device='cuda:0') tensor(3.0552e-07, device='cuda:0') tensor(2.5128e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000279
Average total loss: 2.302864
tensor(-15.2089, device='cuda:0') tensor(2.9780e-07, device='cuda:0') tensor(2.4822e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2102, device='cuda:0') tensor(2.9709e-07, device='cuda:0') tensor(2.4792e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2114, device='cuda:0') tensor(2.9641e-07, device='cuda:0') tensor(2.4762e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2126, device='cuda:0') tensor(2.9575e-07, device='cuda:0') tensor(2.4732e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2138, device='cuda:0') tensor(2.9510e-07, device='cuda:0') tensor(2.4702e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2150, device='cuda:0') tensor(2.9446e-07, device='cuda:0') tensor(2.4672e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2162, device='cuda:0') tensor(2.9384e-07, device='cuda:0') tensor(2.4642e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2174, device='cuda:0') tensor(2.9323e-07, device='cuda:0') tensor(2.4612e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2187, device='cuda:0') tensor(2.9264e-07, device='cuda:0') tensor(2.4582e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2199, device='cuda:0') tensor(2.9205e-07, device='cuda:0') tensor(2.4552e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2211, device='cuda:0') tensor(2.9148e-07, device='cuda:0') tensor(2.4522e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2223, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4493e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2224, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4489e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2226, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4486e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2227, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4482e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2229, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4479e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2230, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4475e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2231, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4472e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2233, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4469e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2234, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4465e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2236, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4462e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2237, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4458e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
 Percentile value: -15.223478317260742
Non-zero model percentage: 0.2430054396390915%, Non-zero mask percentage: 0.2430054396390915%

--- Pruning Level [5/7]: ---
conv1.weight         | nonzeros =     176 /    1728             ( 10.19%) | total_pruned =    1552 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
bn1.bias             | nonzeros =       6 /      64             (  9.38%) | total_pruned =      58 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     312 /   36864             (  0.85%) | total_pruned =   36552 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     450 /   36864             (  1.22%) | total_pruned =   36414 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     304 /   36864             (  0.82%) | total_pruned =   36560 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       6 /      64             (  9.38%) | total_pruned =      58 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     316 /   36864             (  0.86%) | total_pruned =   36548 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     707 /   73728             (  0.96%) | total_pruned =   73021 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    1019 /  147456             (  0.69%) | total_pruned =  146437 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     284 /    8192             (  3.47%) | total_pruned =    7908 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      34 /     128             ( 26.56%) | total_pruned =      94 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      85 /  147456             (  0.06%) | total_pruned =  147371 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      14 /     128             ( 10.94%) | total_pruned =     114 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      19 /     128             ( 14.84%) | total_pruned =     109 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      87 /  147456             (  0.06%) | total_pruned =  147369 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      27 /     128             ( 21.09%) | total_pruned =     101 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    2279 /  294912             (  0.77%) | total_pruned =  292633 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      93 /     256             ( 36.33%) | total_pruned =     163 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    2592 /  589824             (  0.44%) | total_pruned =  587232 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      67 /     256             ( 26.17%) | total_pruned =     189 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      83 /     256             ( 32.42%) | total_pruned =     173 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     225 /   32768             (  0.69%) | total_pruned =   32543 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      45 /     256             ( 17.58%) | total_pruned =     211 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     134 /  589824             (  0.02%) | total_pruned =  589690 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      27 /     256             ( 10.55%) | total_pruned =     229 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      30 /     256             ( 11.72%) | total_pruned =     226 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     108 /  589824             (  0.02%) | total_pruned =  589716 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      16 /     256             (  6.25%) | total_pruned =     240 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      68 /     256             ( 26.56%) | total_pruned =     188 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    2548 / 1179648             (  0.22%) | total_pruned = 1177100 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     121 /     512             ( 23.63%) | total_pruned =     391 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     135 /     512             ( 26.37%) | total_pruned =     377 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    2552 / 2359296             (  0.11%) | total_pruned = 2356744 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     127 /     512             ( 24.80%) | total_pruned =     385 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     214 /     512             ( 41.80%) | total_pruned =     298 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     240 /  131072             (  0.18%) | total_pruned =  130832 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      53 /     512             ( 10.35%) | total_pruned =     459 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     222 /     512             ( 43.36%) | total_pruned =     290 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    3660 / 2359296             (  0.16%) | total_pruned = 2355636 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     150 /     512             ( 29.30%) | total_pruned =     362 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     108 /     512             ( 21.09%) | total_pruned =     404 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    3305 / 2359296             (  0.14%) | total_pruned = 2355991 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     178 /     512             ( 34.77%) | total_pruned =     334 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     446 /     512             ( 87.11%) | total_pruned =      66 | shape = torch.Size([512])
linear.weight        | nonzeros =    3010 /    5120             ( 58.79%) | total_pruned =    2110 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 27165, pruned : 11151597, total: 11178762, Compression rate :     411.51x  ( 99.76% pruned)
Train Epoch: 122/200 Loss: 1.075198 Accuracy: 56.68 57.76 % Best test Accuracy: 56.83%
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000272
Average total loss: 2.302857
tensor(-15.2358, device='cuda:0') tensor(2.8380e-07, device='cuda:0') tensor(2.4166e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000269
Average total loss: 2.302853
tensor(-15.2475, device='cuda:0') tensor(2.7692e-07, device='cuda:0') tensor(2.3883e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000265
Average total loss: 2.302850
tensor(-15.2592, device='cuda:0') tensor(2.7029e-07, device='cuda:0') tensor(2.3607e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000262
Average total loss: 2.302847
tensor(-15.2707, device='cuda:0') tensor(2.6388e-07, device='cuda:0') tensor(2.3337e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-15.2820, device='cuda:0') tensor(2.5769e-07, device='cuda:0') tensor(2.3073e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000257
Average total loss: 2.302841
tensor(-15.2933, device='cuda:0') tensor(2.5171e-07, device='cuda:0') tensor(2.2815e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000254
Average total loss: 2.302839
tensor(-15.3044, device='cuda:0') tensor(2.4598e-07, device='cuda:0') tensor(2.2563e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000251
Average total loss: 2.302836
tensor(-15.3154, device='cuda:0') tensor(2.4041e-07, device='cuda:0') tensor(2.2316e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000248
Average total loss: 2.302833
tensor(-15.3263, device='cuda:0') tensor(2.3504e-07, device='cuda:0') tensor(2.2075e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000245
Average total loss: 2.302830
tensor(-15.3370, device='cuda:0') tensor(2.2983e-07, device='cuda:0') tensor(2.1839e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000243
Average total loss: 2.302828
tensor(-15.3476, device='cuda:0') tensor(2.2483e-07, device='cuda:0') tensor(2.1607e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000240
Average total loss: 2.302825
tensor(-15.3582, device='cuda:0') tensor(2.1995e-07, device='cuda:0') tensor(2.1381e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000239
Average total loss: 2.302824
tensor(-15.3592, device='cuda:0') tensor(2.1931e-07, device='cuda:0') tensor(2.1359e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000239
Average total loss: 2.302824
tensor(-15.3602, device='cuda:0') tensor(2.1869e-07, device='cuda:0') tensor(2.1337e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3613, device='cuda:0') tensor(2.1809e-07, device='cuda:0') tensor(2.1315e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3623, device='cuda:0') tensor(2.1751e-07, device='cuda:0') tensor(2.1293e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3633, device='cuda:0') tensor(2.1694e-07, device='cuda:0') tensor(2.1272e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3643, device='cuda:0') tensor(2.1638e-07, device='cuda:0') tensor(2.1250e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3654, device='cuda:0') tensor(2.1584e-07, device='cuda:0') tensor(2.1228e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3664, device='cuda:0') tensor(2.1530e-07, device='cuda:0') tensor(2.1206e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3674, device='cuda:0') tensor(2.1478e-07, device='cuda:0') tensor(2.1184e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3685, device='cuda:0') tensor(2.1428e-07, device='cuda:0') tensor(2.1162e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3695, device='cuda:0') tensor(2.1378e-07, device='cuda:0') tensor(2.1141e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3696, device='cuda:0') tensor(2.1377e-07, device='cuda:0') tensor(2.1139e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3697, device='cuda:0') tensor(2.1376e-07, device='cuda:0') tensor(2.1137e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3698, device='cuda:0') tensor(2.1376e-07, device='cuda:0') tensor(2.1135e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3699, device='cuda:0') tensor(2.1375e-07, device='cuda:0') tensor(2.1133e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3700, device='cuda:0') tensor(2.1374e-07, device='cuda:0') tensor(2.1131e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3700, device='cuda:0') tensor(2.1373e-07, device='cuda:0') tensor(2.1129e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3701, device='cuda:0') tensor(2.1372e-07, device='cuda:0') tensor(2.1127e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3702, device='cuda:0') tensor(2.1372e-07, device='cuda:0') tensor(2.1125e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3703, device='cuda:0') tensor(2.1371e-07, device='cuda:0') tensor(2.1123e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3704, device='cuda:0') tensor(2.1370e-07, device='cuda:0') tensor(2.1121e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
 Percentile value: -15.367754650115966
Non-zero model percentage: 0.07290609925985336%, Non-zero mask percentage: 0.07290609925985336%

--- Pruning Level [6/7]: ---
conv1.weight         | nonzeros =     105 /    1728             (  6.08%) | total_pruned =    1623 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
bn1.bias             | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =      90 /   36864             (  0.24%) | total_pruned =   36774 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       5 /      64             (  7.81%) | total_pruned =      59 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =      96 /   36864             (  0.26%) | total_pruned =   36768 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =       5 /      64             (  7.81%) | total_pruned =      59 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =      69 /   36864             (  0.19%) | total_pruned =   36795 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =      59 /   36864             (  0.16%) | total_pruned =   36805 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =      84 /   73728             (  0.11%) | total_pruned =   73644 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      10 /     128             (  7.81%) | total_pruned =     118 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     124 /  147456             (  0.08%) | total_pruned =  147332 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      23 /     128             ( 17.97%) | total_pruned =     105 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =      90 /    8192             (  1.10%) | total_pruned =    8102 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      26 /     128             ( 20.31%) | total_pruned =     102 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      19 /     128             ( 14.84%) | total_pruned =     109 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       6 /  147456             (  0.00%) | total_pruned =  147450 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       6 /     128             (  4.69%) | total_pruned =     122 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      11 /     128             (  8.59%) | total_pruned =     117 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       5 /  147456             (  0.00%) | total_pruned =  147451 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      13 /     128             ( 10.16%) | total_pruned =     115 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     389 /  294912             (  0.13%) | total_pruned =  294523 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      70 /     256             ( 27.34%) | total_pruned =     186 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      60 /     256             ( 23.44%) | total_pruned =     196 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     535 /  589824             (  0.09%) | total_pruned =  589289 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      44 /     256             ( 17.19%) | total_pruned =     212 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      48 /     256             ( 18.75%) | total_pruned =     208 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =      59 /   32768             (  0.18%) | total_pruned =   32709 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      26 /     256             ( 10.16%) | total_pruned =     230 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      50 /     256             ( 19.53%) | total_pruned =     206 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       4 /  589824             (  0.00%) | total_pruned =  589820 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       6 /     256             (  2.34%) | total_pruned =     250 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      11 /     256             (  4.30%) | total_pruned =     245 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       4 /  589824             (  0.00%) | total_pruned =  589820 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       5 /     256             (  1.95%) | total_pruned =     251 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      51 /     256             ( 19.92%) | total_pruned =     205 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     458 / 1179648             (  0.04%) | total_pruned = 1179190 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =      73 /     512             ( 14.26%) | total_pruned =     439 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =      83 /     512             ( 16.21%) | total_pruned =     429 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     339 / 2359296             (  0.01%) | total_pruned = 2358957 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =      83 /     512             ( 16.21%) | total_pruned =     429 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     190 /     512             ( 37.11%) | total_pruned =     322 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =      39 /  131072             (  0.03%) | total_pruned =  131033 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      23 /     512             (  4.49%) | total_pruned =     489 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     187 /     512             ( 36.52%) | total_pruned =     325 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     440 / 2359296             (  0.02%) | total_pruned = 2358856 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     102 /     512             ( 19.92%) | total_pruned =     410 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      54 /     512             ( 10.55%) | total_pruned =     458 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     519 / 2359296             (  0.02%) | total_pruned = 2358777 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     110 /     512             ( 21.48%) | total_pruned =     402 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     442 /     512             ( 86.33%) | total_pruned =      70 | shape = torch.Size([512])
linear.weight        | nonzeros =    2662 /    5120             ( 51.99%) | total_pruned =    2458 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8150, pruned : 11170612, total: 11178762, Compression rate :    1371.63x  ( 99.93% pruned)
Train Epoch: 59/200 Loss: 2.302626 Accuracy: 10.00 10.00 % Best test Accuracy: 16.57%
