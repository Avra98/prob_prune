Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/12]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 61/100 Loss: 0.000080 Accuracy: 86.83 100.00 % Best test Accuracy: 86.83%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(-2.1415e-07, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.074176
Average KL loss: 0.004748
Average total loss: 2.078924
tensor(6.9651e-05, device='cuda:0') tensor(3.7316e-05, device='cuda:0') tensor(-9.8019e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.102961
Average KL loss: 0.010245
Average total loss: 2.113206
tensor(9.3001e-05, device='cuda:0') tensor(7.7533e-05, device='cuda:0') tensor(-1.5474e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 1.987031
Average KL loss: 0.015599
Average total loss: 2.002630
tensor(0.0001, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(-2.0787e-07, device='cuda:0')
Epoch 4
Average batch original loss after noise: 1.955249
Average KL loss: 0.020453
Average total loss: 1.975701
tensor(0.0001, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(-1.0528e-07, device='cuda:0')
Epoch 5
Average batch original loss after noise: 1.861712
Average KL loss: 0.024785
Average total loss: 1.886498
tensor(0.0001, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(-1.4307e-07, device='cuda:0')
Epoch 6
Average batch original loss after noise: 1.869334
Average KL loss: 0.028616
Average total loss: 1.897951
tensor(0.0001, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(-8.0418e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 1.804702
Average KL loss: 0.032340
Average total loss: 1.837042
tensor(0.0002, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(-2.1522e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 1.788311
Average KL loss: 0.035793
Average total loss: 1.824104
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-1.9103e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 1.726325
Average KL loss: 0.039108
Average total loss: 1.765434
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-1.2155e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 1.661985
Average KL loss: 0.042026
Average total loss: 1.704011
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-9.3965e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 1.679557
Average KL loss: 0.044456
Average total loss: 1.724013
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-1.7786e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 1.641663
Average KL loss: 0.047031
Average total loss: 1.688693
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-1.1976e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 1.635614
Average KL loss: 0.049971
Average total loss: 1.685585
tensor(0.0002, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(-1.2726e-07, device='cuda:0')
Epoch 14
Average batch original loss after noise: 1.582427
Average KL loss: 0.052026
Average total loss: 1.634454
tensor(0.0002, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.2476e-07, device='cuda:0')
Epoch 15
Average batch original loss after noise: 1.555066
Average KL loss: 0.053839
Average total loss: 1.608905
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.2000e-07, device='cuda:0')
Epoch 16
Average batch original loss after noise: 1.523658
Average KL loss: 0.055618
Average total loss: 1.579276
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.1099e-07, device='cuda:0')
Epoch 17
Average batch original loss after noise: 1.504136
Average KL loss: 0.057113
Average total loss: 1.561249
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.7333e-07, device='cuda:0')
Epoch 18
Average batch original loss after noise: 1.510499
Average KL loss: 0.058491
Average total loss: 1.568990
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.8788e-07, device='cuda:0')
Epoch 19
Average batch original loss after noise: 1.442276
Average KL loss: 0.059585
Average total loss: 1.501861
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.5198e-07, device='cuda:0')
Epoch 20
Average batch original loss after noise: 1.488188
Average KL loss: 0.060781
Average total loss: 1.548969
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.8561e-07, device='cuda:0')
Epoch 21
Average batch original loss after noise: 1.451075
Average KL loss: 0.062040
Average total loss: 1.513115
tensor(0.0003, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-9.7656e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 1.418361
Average KL loss: 0.063344
Average total loss: 1.481705
tensor(0.0004, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.6513e-07, device='cuda:0')
Epoch 23
Average batch original loss after noise: 1.378449
Average KL loss: 0.064479
Average total loss: 1.442928
tensor(0.0004, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(-1.3242e-07, device='cuda:0')
Epoch 24
Average batch original loss after noise: 1.388789
Average KL loss: 0.065505
Average total loss: 1.454294
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.4145e-07, device='cuda:0')
Epoch 25
Average batch original loss after noise: 1.378435
Average KL loss: 0.066675
Average total loss: 1.445109
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.6053e-07, device='cuda:0')
Epoch 26
Average batch original loss after noise: 1.325468
Average KL loss: 0.067738
Average total loss: 1.393206
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.0688e-07, device='cuda:0')
Epoch 27
Average batch original loss after noise: 1.338149
Average KL loss: 0.068454
Average total loss: 1.406603
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.2404e-07, device='cuda:0')
Epoch 28
Average batch original loss after noise: 1.307483
Average KL loss: 0.069253
Average total loss: 1.376735
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.1894e-07, device='cuda:0')
Epoch 29
Average batch original loss after noise: 1.292954
Average KL loss: 0.069802
Average total loss: 1.362756
tensor(0.0004, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-8.6380e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 1.282298
Average KL loss: 0.070335
Average total loss: 1.352633
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.1929e-07, device='cuda:0')
Epoch 31
Average batch original loss after noise: 1.286311
Average KL loss: 0.070988
Average total loss: 1.357298
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.0066e-07, device='cuda:0')
Epoch 32
Average batch original loss after noise: 1.259562
Average KL loss: 0.071824
Average total loss: 1.331386
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.1494e-07, device='cuda:0')
Epoch 33
Average batch original loss after noise: 1.247812
Average KL loss: 0.072588
Average total loss: 1.320400
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.1809e-07, device='cuda:0')
Epoch 34
Average batch original loss after noise: 1.227524
Average KL loss: 0.073317
Average total loss: 1.300842
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.0285e-07, device='cuda:0')
Epoch 35
Average batch original loss after noise: 1.240271
Average KL loss: 0.073604
Average total loss: 1.313874
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.0738e-07, device='cuda:0')
Epoch 36
Average batch original loss after noise: 1.216286
Average KL loss: 0.074014
Average total loss: 1.290301
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.2310e-07, device='cuda:0')
Epoch 37
Average batch original loss after noise: 1.225367
Average KL loss: 0.074719
Average total loss: 1.300086
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.4008e-07, device='cuda:0')
Epoch 38
Average batch original loss after noise: 1.219082
Average KL loss: 0.075323
Average total loss: 1.294406
tensor(0.0005, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-7.8494e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 1.209060
Average KL loss: 0.075826
Average total loss: 1.284886
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-8.5008e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 1.192340
Average KL loss: 0.076285
Average total loss: 1.268625
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.2099e-07, device='cuda:0')
Epoch 41
Average batch original loss after noise: 1.186847
Average KL loss: 0.076877
Average total loss: 1.263725
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.1101e-07, device='cuda:0')
Epoch 42
Average batch original loss after noise: 1.169008
Average KL loss: 0.077389
Average total loss: 1.246398
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.2298e-07, device='cuda:0')
Epoch 43
Average batch original loss after noise: 1.150879
Average KL loss: 0.078029
Average total loss: 1.228908
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.0272e-07, device='cuda:0')
Epoch 44
Average batch original loss after noise: 1.148516
Average KL loss: 0.078320
Average total loss: 1.226836
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-1.2798e-07, device='cuda:0')
Epoch 45
Average batch original loss after noise: 1.136702
Average KL loss: 0.078541
Average total loss: 1.215243
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-9.3113e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 1.120858
Average KL loss: 0.078684
Average total loss: 1.199541
tensor(0.0006, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(-9.1959e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 1.119843
Average KL loss: 0.079131
Average total loss: 1.198974
tensor(0.0006, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.3277e-07, device='cuda:0')
Epoch 48
Average batch original loss after noise: 1.124769
Average KL loss: 0.079315
Average total loss: 1.204084
tensor(0.0006, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.0300e-07, device='cuda:0')
Epoch 49
Average batch original loss after noise: 1.108342
Average KL loss: 0.079610
Average total loss: 1.187951
tensor(0.0006, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.0477e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 1.075955
Average KL loss: 0.079904
Average total loss: 1.155859
tensor(0.0006, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.8102e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 1.093079
Average KL loss: 0.079976
Average total loss: 1.173055
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.5193e-07, device='cuda:0')
Epoch 52
Average batch original loss after noise: 1.066757
Average KL loss: 0.080163
Average total loss: 1.146921
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1255e-07, device='cuda:0')
Epoch 53
Average batch original loss after noise: 1.080132
Average KL loss: 0.080488
Average total loss: 1.160620
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1328e-07, device='cuda:0')
Epoch 54
Average batch original loss after noise: 1.055975
Average KL loss: 0.080718
Average total loss: 1.136693
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.0483e-07, device='cuda:0')
Epoch 55
Average batch original loss after noise: 1.033296
Average KL loss: 0.080675
Average total loss: 1.113971
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.3413e-07, device='cuda:0')
Epoch 56
Average batch original loss after noise: 1.049290
Average KL loss: 0.080702
Average total loss: 1.129992
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.3877e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 1.041347
Average KL loss: 0.081099
Average total loss: 1.122447
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1880e-07, device='cuda:0')
Epoch 58
Average batch original loss after noise: 1.051043
Average KL loss: 0.081550
Average total loss: 1.132594
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1071e-07, device='cuda:0')
Epoch 59
Average batch original loss after noise: 1.027529
Average KL loss: 0.081993
Average total loss: 1.109523
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1739e-07, device='cuda:0')
Epoch 60
Average batch original loss after noise: 1.024807
Average KL loss: 0.082334
Average total loss: 1.107141
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.6664e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.991446
Average KL loss: 0.082582
Average total loss: 1.074027
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.4266e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.993713
Average KL loss: 0.082525
Average total loss: 1.076238
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.1303e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 1.002473
Average KL loss: 0.082859
Average total loss: 1.085331
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.9621e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.985352
Average KL loss: 0.083038
Average total loss: 1.068390
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.6176e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.966525
Average KL loss: 0.083238
Average total loss: 1.049763
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.2404e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.976454
Average KL loss: 0.083207
Average total loss: 1.059661
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.0843e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.933209
Average KL loss: 0.083114
Average total loss: 1.016324
tensor(0.0007, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.1142e-07, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.969408
Average KL loss: 0.083227
Average total loss: 1.052635
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.8165e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.965772
Average KL loss: 0.083277
Average total loss: 1.049049
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.3817e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.929436
Average KL loss: 0.083410
Average total loss: 1.012846
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.3710e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.954460
Average KL loss: 0.083674
Average total loss: 1.038133
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.9367e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.929084
Average KL loss: 0.083862
Average total loss: 1.012945
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.1324e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 0.925449
Average KL loss: 0.084088
Average total loss: 1.009537
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.0887e-07, device='cuda:0')
Epoch 74
Average batch original loss after noise: 0.924854
Average KL loss: 0.084125
Average total loss: 1.008980
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.0054e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 0.902264
Average KL loss: 0.084245
Average total loss: 0.986510
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.4371e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 0.895151
Average KL loss: 0.084352
Average total loss: 0.979503
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.0252e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 0.895066
Average KL loss: 0.084348
Average total loss: 0.979414
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-1.0077e-07, device='cuda:0')
Epoch 78
Average batch original loss after noise: 0.909405
Average KL loss: 0.084371
Average total loss: 0.993777
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.2191e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 0.905257
Average KL loss: 0.084619
Average total loss: 0.989876
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.9009e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 0.887643
Average KL loss: 0.084925
Average total loss: 0.972569
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.6616e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 0.887522
Average KL loss: 0.085112
Average total loss: 0.972634
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.5607e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 0.873028
Average KL loss: 0.085166
Average total loss: 0.958195
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.1965e-08, device='cuda:0')
Epoch 83
Average batch original loss after noise: 0.873631
Average KL loss: 0.085322
Average total loss: 0.958953
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.2813e-08, device='cuda:0')
Epoch 84
Average batch original loss after noise: 0.852309
Average KL loss: 0.085526
Average total loss: 0.937835
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.1474e-08, device='cuda:0')
Epoch 85
Average batch original loss after noise: 0.852350
Average KL loss: 0.085524
Average total loss: 0.937873
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.8450e-08, device='cuda:0')
Epoch 86
Average batch original loss after noise: 0.851048
Average KL loss: 0.085704
Average total loss: 0.936752
tensor(0.0008, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.9210e-08, device='cuda:0')
Epoch 87
Average batch original loss after noise: 0.853802
Average KL loss: 0.085883
Average total loss: 0.939685
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.0220e-08, device='cuda:0')
Epoch 88
Average batch original loss after noise: 0.818028
Average KL loss: 0.085923
Average total loss: 0.903952
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.6316e-08, device='cuda:0')
Epoch 89
Average batch original loss after noise: 0.833061
Average KL loss: 0.086015
Average total loss: 0.919076
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.6111e-08, device='cuda:0')
Epoch 90
Average batch original loss after noise: 0.830673
Average KL loss: 0.086312
Average total loss: 0.916985
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.6446e-08, device='cuda:0')
Epoch 91
Average batch original loss after noise: 0.830427
Average KL loss: 0.086404
Average total loss: 0.916831
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.8438e-08, device='cuda:0')
Epoch 92
Average batch original loss after noise: 0.818117
Average KL loss: 0.086404
Average total loss: 0.904521
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.9615e-08, device='cuda:0')
Epoch 93
Average batch original loss after noise: 0.819381
Average KL loss: 0.086504
Average total loss: 0.905884
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.8844e-08, device='cuda:0')
Epoch 94
Average batch original loss after noise: 0.800482
Average KL loss: 0.086687
Average total loss: 0.887169
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.8147e-08, device='cuda:0')
Epoch 95
Average batch original loss after noise: 0.807277
Average KL loss: 0.086690
Average total loss: 0.893967
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.6210e-08, device='cuda:0')
Epoch 96
Average batch original loss after noise: 0.797313
Average KL loss: 0.086756
Average total loss: 0.884069
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.9964e-08, device='cuda:0')
Epoch 97
Average batch original loss after noise: 0.790984
Average KL loss: 0.086898
Average total loss: 0.877882
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7635e-08, device='cuda:0')
Epoch 98
Average batch original loss after noise: 0.808921
Average KL loss: 0.087012
Average total loss: 0.895933
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.1056e-08, device='cuda:0')
Epoch 99
Average batch original loss after noise: 0.780096
Average KL loss: 0.087133
Average total loss: 0.867229
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.8402e-08, device='cuda:0')
Epoch 100
Average batch original loss after noise: 0.787364
Average KL loss: 0.087230
Average total loss: 0.874594
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-4.8909e-08, device='cuda:0')
Epoch 101
Average batch original loss after noise: 0.779518
Average KL loss: 0.087339
Average total loss: 0.866857
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.2790e-08, device='cuda:0')
Epoch 102
Average batch original loss after noise: 0.778112
Average KL loss: 0.087567
Average total loss: 0.865680
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7710e-08, device='cuda:0')
Epoch 103
Average batch original loss after noise: 0.769629
Average KL loss: 0.087860
Average total loss: 0.857489
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.1163e-08, device='cuda:0')
Epoch 104
Average batch original loss after noise: 0.761722
Average KL loss: 0.088058
Average total loss: 0.849779
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.2926e-08, device='cuda:0')
Epoch 105
Average batch original loss after noise: 0.760713
Average KL loss: 0.087970
Average total loss: 0.848683
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.6085e-08, device='cuda:0')
Epoch 106
Average batch original loss after noise: 0.747402
Average KL loss: 0.087929
Average total loss: 0.835331
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-4.8034e-08, device='cuda:0')
Epoch 107
Average batch original loss after noise: 0.735052
Average KL loss: 0.087981
Average total loss: 0.823034
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.2669e-08, device='cuda:0')
Epoch 108
Average batch original loss after noise: 0.753648
Average KL loss: 0.088098
Average total loss: 0.841746
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.0230e-08, device='cuda:0')
Epoch 109
Average batch original loss after noise: 0.732857
Average KL loss: 0.088317
Average total loss: 0.821174
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.8370e-08, device='cuda:0')
Epoch 110
Average batch original loss after noise: 0.743985
Average KL loss: 0.088545
Average total loss: 0.832530
tensor(0.0009, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.4111e-08, device='cuda:0')
Epoch 111
Average batch original loss after noise: 0.753829
Average KL loss: 0.088755
Average total loss: 0.842584
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.2451e-08, device='cuda:0')
Epoch 112
Average batch original loss after noise: 0.733056
Average KL loss: 0.089154
Average total loss: 0.822210
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.1996e-08, device='cuda:0')
Epoch 113
Average batch original loss after noise: 0.713778
Average KL loss: 0.089181
Average total loss: 0.802960
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7339e-08, device='cuda:0')
Epoch 114
Average batch original loss after noise: 0.711743
Average KL loss: 0.089029
Average total loss: 0.800772
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.5043e-08, device='cuda:0')
Epoch 115
Average batch original loss after noise: 0.709946
Average KL loss: 0.089082
Average total loss: 0.799028
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-8.0927e-08, device='cuda:0')
Epoch 116
Average batch original loss after noise: 0.713824
Average KL loss: 0.089148
Average total loss: 0.802971
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7592e-08, device='cuda:0')
Epoch 117
Average batch original loss after noise: 0.724500
Average KL loss: 0.089260
Average total loss: 0.813760
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.7638e-08, device='cuda:0')
Epoch 118
Average batch original loss after noise: 0.712442
Average KL loss: 0.089526
Average total loss: 0.801968
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-9.8307e-08, device='cuda:0')
Epoch 119
Average batch original loss after noise: 0.702710
Average KL loss: 0.089569
Average total loss: 0.792279
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7236e-08, device='cuda:0')
Epoch 120
Average batch original loss after noise: 0.687670
Average KL loss: 0.089549
Average total loss: 0.777219
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.9546e-08, device='cuda:0')
Epoch 121
Average batch original loss after noise: 0.685291
Average KL loss: 0.089452
Average total loss: 0.774743
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.8698e-08, device='cuda:0')
Epoch 122
Average batch original loss after noise: 0.678917
Average KL loss: 0.089526
Average total loss: 0.768442
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-7.6525e-08, device='cuda:0')
Epoch 123
Average batch original loss after noise: 0.688491
Average KL loss: 0.089592
Average total loss: 0.778083
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-4.4177e-08, device='cuda:0')
Epoch 124
Average batch original loss after noise: 0.677563
Average KL loss: 0.089865
Average total loss: 0.767427
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-5.7435e-08, device='cuda:0')
Epoch 125
Average batch original loss after noise: 0.667730
Average KL loss: 0.089959
Average total loss: 0.757689
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-4.6690e-08, device='cuda:0')
Epoch 126
Average batch original loss after noise: 0.669452
Average KL loss: 0.089916
Average total loss: 0.759368
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-4.3028e-08, device='cuda:0')
Epoch 127
Average batch original loss after noise: 0.672211
Average KL loss: 0.090084
Average total loss: 0.762295
tensor(0.0010, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(-6.6428e-08, device='cuda:0')
Epoch 128
Average batch original loss after noise: 0.668171
Average KL loss: 0.090352
Average total loss: 0.758523
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.7510e-08, device='cuda:0')
Epoch 129
Average batch original loss after noise: 0.662122
Average KL loss: 0.090479
Average total loss: 0.752601
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.8903e-08, device='cuda:0')
Epoch 130
Average batch original loss after noise: 0.651132
Average KL loss: 0.090426
Average total loss: 0.741559
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.7058e-08, device='cuda:0')
Epoch 131
Average batch original loss after noise: 0.642456
Average KL loss: 0.090548
Average total loss: 0.733004
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-9.9735e-08, device='cuda:0')
Epoch 132
Average batch original loss after noise: 0.657073
Average KL loss: 0.090520
Average total loss: 0.747593
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.7895e-08, device='cuda:0')
Epoch 133
Average batch original loss after noise: 0.640942
Average KL loss: 0.090671
Average total loss: 0.731612
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.2703e-08, device='cuda:0')
Epoch 134
Average batch original loss after noise: 0.645398
Average KL loss: 0.090725
Average total loss: 0.736122
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.6985e-08, device='cuda:0')
Epoch 135
Average batch original loss after noise: 0.632913
Average KL loss: 0.090779
Average total loss: 0.723692
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.7941e-08, device='cuda:0')
Epoch 136
Average batch original loss after noise: 0.656240
Average KL loss: 0.091156
Average total loss: 0.747396
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.0470e-08, device='cuda:0')
Epoch 137
Average batch original loss after noise: 0.616888
Average KL loss: 0.091124
Average total loss: 0.708013
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.0782e-08, device='cuda:0')
Epoch 138
Average batch original loss after noise: 0.628793
Average KL loss: 0.091129
Average total loss: 0.719921
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.3843e-08, device='cuda:0')
Epoch 139
Average batch original loss after noise: 0.632156
Average KL loss: 0.091451
Average total loss: 0.723607
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.3994e-08, device='cuda:0')
Epoch 140
Average batch original loss after noise: 0.621936
Average KL loss: 0.091435
Average total loss: 0.713371
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.9920e-08, device='cuda:0')
Epoch 141
Average batch original loss after noise: 0.630427
Average KL loss: 0.091807
Average total loss: 0.722233
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.2051e-08, device='cuda:0')
Epoch 142
Average batch original loss after noise: 0.624244
Average KL loss: 0.091983
Average total loss: 0.716227
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-7.3787e-08, device='cuda:0')
Epoch 143
Average batch original loss after noise: 0.610695
Average KL loss: 0.092225
Average total loss: 0.702920
tensor(0.0010, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.4179e-08, device='cuda:0')
Epoch 144
Average batch original loss after noise: 0.599715
Average KL loss: 0.092385
Average total loss: 0.692101
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.8440e-08, device='cuda:0')
Epoch 145
Average batch original loss after noise: 0.620082
Average KL loss: 0.092602
Average total loss: 0.712685
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.0542e-08, device='cuda:0')
Epoch 146
Average batch original loss after noise: 0.602764
Average KL loss: 0.092773
Average total loss: 0.695538
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.9929e-08, device='cuda:0')
Epoch 147
Average batch original loss after noise: 0.612227
Average KL loss: 0.092875
Average total loss: 0.705102
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.4815e-08, device='cuda:0')
Epoch 148
Average batch original loss after noise: 0.589484
Average KL loss: 0.092977
Average total loss: 0.682461
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.7278e-08, device='cuda:0')
Epoch 149
Average batch original loss after noise: 0.590717
Average KL loss: 0.093149
Average total loss: 0.683866
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.4749e-08, device='cuda:0')
Epoch 150
Average batch original loss after noise: 0.580279
Average KL loss: 0.093237
Average total loss: 0.673516
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.3740e-08, device='cuda:0')
Epoch 151
Average batch original loss after noise: 0.595349
Average KL loss: 0.093304
Average total loss: 0.688653
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.5866e-08, device='cuda:0')
Epoch 152
Average batch original loss after noise: 0.585533
Average KL loss: 0.093505
Average total loss: 0.679038
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.6382e-08, device='cuda:0')
Epoch 153
Average batch original loss after noise: 0.581729
Average KL loss: 0.093554
Average total loss: 0.675284
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.7531e-08, device='cuda:0')
Epoch 154
Average batch original loss after noise: 0.579572
Average KL loss: 0.093748
Average total loss: 0.673320
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.4699e-08, device='cuda:0')
Epoch 155
Average batch original loss after noise: 0.574793
Average KL loss: 0.093935
Average total loss: 0.668728
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.5406e-08, device='cuda:0')
Epoch 156
Average batch original loss after noise: 0.572816
Average KL loss: 0.094020
Average total loss: 0.666837
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.0204e-08, device='cuda:0')
Epoch 157
Average batch original loss after noise: 0.574788
Average KL loss: 0.094109
Average total loss: 0.668897
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.6392e-08, device='cuda:0')
Epoch 158
Average batch original loss after noise: 0.564388
Average KL loss: 0.094098
Average total loss: 0.658486
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.9714e-08, device='cuda:0')
Epoch 159
Average batch original loss after noise: 0.578654
Average KL loss: 0.094178
Average total loss: 0.672832
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.2864e-08, device='cuda:0')
Epoch 160
Average batch original loss after noise: 0.571032
Average KL loss: 0.094313
Average total loss: 0.665345
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.6380e-08, device='cuda:0')
Epoch 161
Average batch original loss after noise: 0.558874
Average KL loss: 0.094591
Average total loss: 0.653465
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.4135e-08, device='cuda:0')
Epoch 162
Average batch original loss after noise: 0.550861
Average KL loss: 0.094686
Average total loss: 0.645548
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.7921e-08, device='cuda:0')
Epoch 163
Average batch original loss after noise: 0.561652
Average KL loss: 0.094786
Average total loss: 0.656438
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.9734e-08, device='cuda:0')
Epoch 164
Average batch original loss after noise: 0.552050
Average KL loss: 0.094763
Average total loss: 0.646813
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.5601e-08, device='cuda:0')
Epoch 165
Average batch original loss after noise: 0.550082
Average KL loss: 0.094977
Average total loss: 0.645059
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.0088e-08, device='cuda:0')
Epoch 166
Average batch original loss after noise: 0.544986
Average KL loss: 0.095141
Average total loss: 0.640126
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.5621e-08, device='cuda:0')
Epoch 167
Average batch original loss after noise: 0.539284
Average KL loss: 0.095215
Average total loss: 0.634499
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-6.6707e-08, device='cuda:0')
Epoch 168
Average batch original loss after noise: 0.550893
Average KL loss: 0.095527
Average total loss: 0.646420
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.1299e-08, device='cuda:0')
Epoch 169
Average batch original loss after noise: 0.546933
Average KL loss: 0.095811
Average total loss: 0.642744
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.6798e-08, device='cuda:0')
Epoch 170
Average batch original loss after noise: 0.526291
Average KL loss: 0.095704
Average total loss: 0.621995
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.8406e-08, device='cuda:0')
Epoch 171
Average batch original loss after noise: 0.543155
Average KL loss: 0.095806
Average total loss: 0.638962
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.9619e-08, device='cuda:0')
Epoch 172
Average batch original loss after noise: 0.522900
Average KL loss: 0.096088
Average total loss: 0.618988
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.5428e-08, device='cuda:0')
Epoch 173
Average batch original loss after noise: 0.534715
Average KL loss: 0.096201
Average total loss: 0.630916
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.5502e-08, device='cuda:0')
Epoch 174
Average batch original loss after noise: 0.525377
Average KL loss: 0.096338
Average total loss: 0.621716
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.2367e-08, device='cuda:0')
Epoch 175
Average batch original loss after noise: 0.527241
Average KL loss: 0.096668
Average total loss: 0.623910
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.4730e-08, device='cuda:0')
Epoch 176
Average batch original loss after noise: 0.519202
Average KL loss: 0.096700
Average total loss: 0.615902
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.4409e-08, device='cuda:0')
Epoch 177
Average batch original loss after noise: 0.524443
Average KL loss: 0.096842
Average total loss: 0.621285
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.2699e-08, device='cuda:0')
Epoch 178
Average batch original loss after noise: 0.534235
Average KL loss: 0.097324
Average total loss: 0.631559
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.6951e-08, device='cuda:0')
Epoch 179
Average batch original loss after noise: 0.512891
Average KL loss: 0.097652
Average total loss: 0.610543
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-5.3563e-08, device='cuda:0')
Epoch 180
Average batch original loss after noise: 0.505403
Average KL loss: 0.097419
Average total loss: 0.602822
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.8635e-08, device='cuda:0')
Epoch 181
Average batch original loss after noise: 0.509435
Average KL loss: 0.097482
Average total loss: 0.606917
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.9617e-08, device='cuda:0')
Epoch 182
Average batch original loss after noise: 0.503992
Average KL loss: 0.097684
Average total loss: 0.601677
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.7885e-08, device='cuda:0')
Epoch 183
Average batch original loss after noise: 0.502134
Average KL loss: 0.097788
Average total loss: 0.599921
tensor(0.0011, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.1975e-08, device='cuda:0')
Epoch 184
Average batch original loss after noise: 0.495619
Average KL loss: 0.097833
Average total loss: 0.593452
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.4971e-08, device='cuda:0')
Epoch 185
Average batch original loss after noise: 0.501158
Average KL loss: 0.098088
Average total loss: 0.599246
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.1243e-08, device='cuda:0')
Epoch 186
Average batch original loss after noise: 0.496869
Average KL loss: 0.098276
Average total loss: 0.595145
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.8230e-08, device='cuda:0')
Epoch 187
Average batch original loss after noise: 0.490818
Average KL loss: 0.098415
Average total loss: 0.589233
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.3302e-08, device='cuda:0')
Epoch 188
Average batch original loss after noise: 0.483121
Average KL loss: 0.098508
Average total loss: 0.581629
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.8362e-08, device='cuda:0')
Epoch 189
Average batch original loss after noise: 0.487240
Average KL loss: 0.098562
Average total loss: 0.585802
tensor(0.0012, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.8795e-08, device='cuda:0')
Epoch 190
Average batch original loss after noise: 0.496295
Average KL loss: 0.098857
Average total loss: 0.595152
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.8699e-08, device='cuda:0')
Epoch 191
Average batch original loss after noise: 0.489450
Average KL loss: 0.099195
Average total loss: 0.588645
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.8017e-08, device='cuda:0')
Epoch 192
Average batch original loss after noise: 0.491894
Average KL loss: 0.099469
Average total loss: 0.591363
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.8236e-08, device='cuda:0')
Epoch 193
Average batch original loss after noise: 0.487645
Average KL loss: 0.099629
Average total loss: 0.587274
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-4.8239e-08, device='cuda:0')
Epoch 194
Average batch original loss after noise: 0.480090
Average KL loss: 0.099922
Average total loss: 0.580012
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.2221e-08, device='cuda:0')
Epoch 195
Average batch original loss after noise: 0.488212
Average KL loss: 0.100003
Average total loss: 0.588214
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-4.0210e-08, device='cuda:0')
Epoch 196
Average batch original loss after noise: 0.473152
Average KL loss: 0.099942
Average total loss: 0.573094
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-5.6987e-08, device='cuda:0')
Epoch 197
Average batch original loss after noise: 0.466163
Average KL loss: 0.100108
Average total loss: 0.566271
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.1613e-08, device='cuda:0')
Epoch 198
Average batch original loss after noise: 0.472047
Average KL loss: 0.100265
Average total loss: 0.572311
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.8713e-08, device='cuda:0')
Epoch 199
Average batch original loss after noise: 0.482685
Average KL loss: 0.100518
Average total loss: 0.583203
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.5234e-08, device='cuda:0')
Epoch 200
Average batch original loss after noise: 0.471632
Average KL loss: 0.100722
Average total loss: 0.572355
 Percentile value: 0.0031386837363243103
Non-zero model percentage: 49.999996185302734%, Non-zero mask percentage: 49.999996185302734%

--- Pruning Level [1/12]: ---
conv1.weight         | nonzeros =    1230 /    1728             ( 71.18%) | total_pruned =     498 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   19630 /   36864             ( 53.25%) | total_pruned =   17234 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   19748 /   36864             ( 53.57%) | total_pruned =   17116 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   19802 /   36864             ( 53.72%) | total_pruned =   17062 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   19399 /   36864             ( 52.62%) | total_pruned =   17465 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   39210 /   73728             ( 53.18%) | total_pruned =   34518 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   77310 /  147456             ( 52.43%) | total_pruned =   70146 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    5036 /    8192             ( 61.47%) | total_pruned =    3156 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     121 /     128             ( 94.53%) | total_pruned =       7 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   76430 /  147456             ( 51.83%) | total_pruned =   71026 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   75498 /  147456             ( 51.20%) | total_pruned =   71958 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  155031 /  294912             ( 52.57%) | total_pruned =  139881 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  306105 /  589824             ( 51.90%) | total_pruned =  283719 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     120 /     256             ( 46.88%) | total_pruned =     136 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   19093 /   32768             ( 58.27%) | total_pruned =   13675 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  298532 /  589824             ( 50.61%) | total_pruned =  291292 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     198 /     256             ( 77.34%) | total_pruned =      58 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  293857 /  589824             ( 49.82%) | total_pruned =  295967 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      85 /     256             ( 33.20%) | total_pruned =     171 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  602966 / 1179648             ( 51.11%) | total_pruned =  576682 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     488 /     512             ( 95.31%) | total_pruned =      24 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     371 /     512             ( 72.46%) | total_pruned =     141 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1179347 / 2359296             ( 49.99%) | total_pruned = 1179949 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   69709 /  131072             ( 53.18%) | total_pruned =   61363 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     421 /     512             ( 82.23%) | total_pruned =      91 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     454 /     512             ( 88.67%) | total_pruned =      58 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1160256 / 2359296             ( 49.18%) | total_pruned = 1199040 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     380 /     512             ( 74.22%) | total_pruned =     132 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     435 /     512             ( 84.96%) | total_pruned =      77 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1139777 / 2359296             ( 48.31%) | total_pruned = 1219519 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     500 /     512             ( 97.66%) | total_pruned =      12 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     333 /     512             ( 65.04%) | total_pruned =     179 | shape = torch.Size([512])
linear.weight        | nonzeros =    3778 /    5120             ( 73.79%) | total_pruned =    1342 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       3 /      10             ( 30.00%) | total_pruned =       7 | shape = torch.Size([10])
alive: 5589381, pruned : 5589381, total: 11178762, Compression rate :       2.00x  ( 50.00% pruned)
Train Epoch: 57/100 Loss: 0.000017 Accuracy: 86.73 100.00 % Best test Accuracy: 86.88%
tensor(0.0012, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-8.6534e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 0.660322
Average KL loss: 0.090023
Average total loss: 0.750345
tensor(0.0028, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.1076e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 0.656823
Average KL loss: 0.085728
Average total loss: 0.742551
tensor(0.0029, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-4.5990e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 0.624257
Average KL loss: 0.085820
Average total loss: 0.710076
tensor(0.0028, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.9761e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 0.620907
Average KL loss: 0.086748
Average total loss: 0.707655
tensor(0.0028, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.6084e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 0.604285
Average KL loss: 0.088089
Average total loss: 0.692374
tensor(0.0027, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.4370e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 0.585152
Average KL loss: 0.089426
Average total loss: 0.674578
tensor(0.0027, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-3.3172e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 0.591236
Average KL loss: 0.090957
Average total loss: 0.682193
tensor(0.0026, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-2.3773e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 0.583574
Average KL loss: 0.092341
Average total loss: 0.675915
tensor(0.0026, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.6333e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 0.568607
Average KL loss: 0.093725
Average total loss: 0.662331
tensor(0.0026, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.9600e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 0.569575
Average KL loss: 0.094934
Average total loss: 0.664509
tensor(0.0025, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(-1.4666e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 0.557750
Average KL loss: 0.096186
Average total loss: 0.653936
tensor(0.0025, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.5943e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 0.563072
Average KL loss: 0.097275
Average total loss: 0.660346
tensor(0.0025, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.4484e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 0.550198
Average KL loss: 0.098297
Average total loss: 0.648495
tensor(0.0024, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.2511e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 0.541134
Average KL loss: 0.099156
Average total loss: 0.640290
tensor(0.0024, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.9993e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 0.547498
Average KL loss: 0.100103
Average total loss: 0.647600
tensor(0.0024, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.4163e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 0.519470
Average KL loss: 0.101094
Average total loss: 0.620565
tensor(0.0023, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-4.7411e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 0.528536
Average KL loss: 0.101675
Average total loss: 0.630212
tensor(0.0023, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.5245e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 0.518917
Average KL loss: 0.102309
Average total loss: 0.621225
tensor(0.0023, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.9650e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 0.526374
Average KL loss: 0.102982
Average total loss: 0.629356
tensor(0.0023, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-7.8257e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 0.519308
Average KL loss: 0.103629
Average total loss: 0.622937
tensor(0.0023, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-3.1537e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 0.522936
Average KL loss: 0.104278
Average total loss: 0.627214
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.6035e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 0.520649
Average KL loss: 0.105074
Average total loss: 0.625724
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.0173e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 0.518258
Average KL loss: 0.105864
Average total loss: 0.624121
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-5.9437e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 0.499632
Average KL loss: 0.106515
Average total loss: 0.606147
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-2.2839e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 0.504729
Average KL loss: 0.106852
Average total loss: 0.611581
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.6064e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 0.490396
Average KL loss: 0.107358
Average total loss: 0.597754
tensor(0.0022, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(-1.5445e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 0.494804
Average KL loss: 0.107758
Average total loss: 0.602562
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-4.0655e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 0.487032
Average KL loss: 0.108257
Average total loss: 0.595289
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-3.6680e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 0.480969
Average KL loss: 0.108677
Average total loss: 0.589646
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-3.5975e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 0.477324
Average KL loss: 0.108909
Average total loss: 0.586233
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-5.0740e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 0.484450
Average KL loss: 0.109108
Average total loss: 0.593558
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.0883e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 0.480685
Average KL loss: 0.109534
Average total loss: 0.590219
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.4707e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 0.494989
Average KL loss: 0.109877
Average total loss: 0.604865
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.6832e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 0.484135
Average KL loss: 0.110324
Average total loss: 0.594460
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.6346e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 0.475039
Average KL loss: 0.110661
Average total loss: 0.585700
tensor(0.0021, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-4.8727e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 0.477128
Average KL loss: 0.110987
Average total loss: 0.588115
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.5410e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 0.462521
Average KL loss: 0.111286
Average total loss: 0.573807
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.2350e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 0.470154
Average KL loss: 0.111686
Average total loss: 0.581839
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.4148e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 0.465072
Average KL loss: 0.112054
Average total loss: 0.577126
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.8862e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 0.465686
Average KL loss: 0.112283
Average total loss: 0.577969
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.7231e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 0.461338
Average KL loss: 0.112678
Average total loss: 0.574015
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.7674e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 0.458138
Average KL loss: 0.112878
Average total loss: 0.571016
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.9495e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 0.457054
Average KL loss: 0.113036
Average total loss: 0.570090
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-9.2098e-09, device='cuda:0')
Epoch 44
Average batch original loss after noise: 0.455403
Average KL loss: 0.113267
Average total loss: 0.568670
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.2326e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 0.459675
Average KL loss: 0.113591
Average total loss: 0.573266
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.2945e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 0.457520
Average KL loss: 0.113881
Average total loss: 0.571400
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-3.0090e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 0.459074
Average KL loss: 0.114328
Average total loss: 0.573402
tensor(0.0020, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.0978e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 0.440048
Average KL loss: 0.114646
Average total loss: 0.554694
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.6717e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 0.448622
Average KL loss: 0.114769
Average total loss: 0.563391
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.4437e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 0.443545
Average KL loss: 0.114990
Average total loss: 0.558535
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.9131e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 0.443864
Average KL loss: 0.115217
Average total loss: 0.559081
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.3165e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 0.435304
Average KL loss: 0.115287
Average total loss: 0.550591
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.8304e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 0.449395
Average KL loss: 0.115512
Average total loss: 0.564907
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-2.3228e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 0.436475
Average KL loss: 0.116003
Average total loss: 0.552478
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.9666e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 0.436408
Average KL loss: 0.116140
Average total loss: 0.552548
tensor(0.0019, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(-1.6239e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 0.444883
Average KL loss: 0.116527
Average total loss: 0.561409
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.7471e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 0.429112
Average KL loss: 0.116904
Average total loss: 0.546016
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.6868e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 0.434485
Average KL loss: 0.117150
Average total loss: 0.551635
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-4.3119e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 0.438063
Average KL loss: 0.117380
Average total loss: 0.555444
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.3301e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 0.427906
Average KL loss: 0.117756
Average total loss: 0.545662
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.9155e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.434933
Average KL loss: 0.118049
Average total loss: 0.552982
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.6574e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.420582
Average KL loss: 0.118218
Average total loss: 0.538799
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.4734e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 0.436297
Average KL loss: 0.118458
Average total loss: 0.554756
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-5.9719e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.415647
Average KL loss: 0.118697
Average total loss: 0.534344
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.6274e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.416070
Average KL loss: 0.118823
Average total loss: 0.534893
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.9751e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.421660
Average KL loss: 0.118976
Average total loss: 0.540636
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.4686e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.414241
Average KL loss: 0.119190
Average total loss: 0.533432
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.9331e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.413672
Average KL loss: 0.119435
Average total loss: 0.533108
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.7445e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.418788
Average KL loss: 0.119644
Average total loss: 0.538432
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.9310e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.408260
Average KL loss: 0.119904
Average total loss: 0.528164
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-3.3996e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.412883
Average KL loss: 0.120010
Average total loss: 0.532892
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.8432e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.409713
Average KL loss: 0.120184
Average total loss: 0.529896
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.4648e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 0.407172
Average KL loss: 0.120449
Average total loss: 0.527621
tensor(0.0019, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.4661e-08, device='cuda:0')
Epoch 74
Average batch original loss after noise: 0.402016
Average KL loss: 0.120600
Average total loss: 0.522616
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-8.5616e-09, device='cuda:0')
Epoch 75
Average batch original loss after noise: 0.398984
Average KL loss: 0.120800
Average total loss: 0.519784
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-3.2956e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 0.409606
Average KL loss: 0.121107
Average total loss: 0.530712
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.5986e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 0.403003
Average KL loss: 0.121307
Average total loss: 0.524309
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.3495e-08, device='cuda:0')
Epoch 78
Average batch original loss after noise: 0.409580
Average KL loss: 0.121573
Average total loss: 0.531153
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-8.8593e-09, device='cuda:0')
Epoch 79
Average batch original loss after noise: 0.406122
Average KL loss: 0.121851
Average total loss: 0.527973
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.8614e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 0.402149
Average KL loss: 0.122146
Average total loss: 0.524295
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.9463e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 0.394585
Average KL loss: 0.122260
Average total loss: 0.516845
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.4813e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 0.409190
Average KL loss: 0.122384
Average total loss: 0.531574
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.6779e-08, device='cuda:0')
Epoch 83
Average batch original loss after noise: 0.382266
Average KL loss: 0.122707
Average total loss: 0.504972
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-4.0464e-08, device='cuda:0')
Epoch 84
Average batch original loss after noise: 0.386158
Average KL loss: 0.122752
Average total loss: 0.508911
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-3.7209e-08, device='cuda:0')
Epoch 85
Average batch original loss after noise: 0.398395
Average KL loss: 0.122815
Average total loss: 0.521210
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-2.3132e-09, device='cuda:0')
Epoch 86
Average batch original loss after noise: 0.388466
Average KL loss: 0.123093
Average total loss: 0.511559
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.9125e-08, device='cuda:0')
Epoch 87
Average batch original loss after noise: 0.389701
Average KL loss: 0.123303
Average total loss: 0.513004
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-3.5504e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 0.384725
Average KL loss: 0.123467
Average total loss: 0.508192
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.3889e-08, device='cuda:0')
Epoch 89
Average batch original loss after noise: 0.379224
Average KL loss: 0.123532
Average total loss: 0.502755
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.7640e-08, device='cuda:0')
Epoch 90
Average batch original loss after noise: 0.384972
Average KL loss: 0.123690
Average total loss: 0.508662
tensor(0.0018, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(-1.9704e-08, device='cuda:0')
Epoch 91
Average batch original loss after noise: 0.387268
Average KL loss: 0.124027
Average total loss: 0.511294
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.7581e-08, device='cuda:0')
Epoch 92
Average batch original loss after noise: 0.385079
Average KL loss: 0.124240
Average total loss: 0.509319
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.3316e-08, device='cuda:0')
Epoch 93
Average batch original loss after noise: 0.375148
Average KL loss: 0.124374
Average total loss: 0.499521
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.6900e-08, device='cuda:0')
Epoch 94
Average batch original loss after noise: 0.385461
Average KL loss: 0.124523
Average total loss: 0.509984
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-3.1774e-08, device='cuda:0')
Epoch 95
Average batch original loss after noise: 0.372891
Average KL loss: 0.124903
Average total loss: 0.497794
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.7054e-08, device='cuda:0')
Epoch 96
Average batch original loss after noise: 0.378554
Average KL loss: 0.125000
Average total loss: 0.503553
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.1140e-08, device='cuda:0')
Epoch 97
Average batch original loss after noise: 0.371097
Average KL loss: 0.125159
Average total loss: 0.496256
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.7873e-08, device='cuda:0')
Epoch 98
Average batch original loss after noise: 0.382882
Average KL loss: 0.125311
Average total loss: 0.508192
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.8552e-08, device='cuda:0')
Epoch 99
Average batch original loss after noise: 0.369036
Average KL loss: 0.125468
Average total loss: 0.494504
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.0793e-08, device='cuda:0')
Epoch 100
Average batch original loss after noise: 0.380818
Average KL loss: 0.125715
Average total loss: 0.506533
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.2700e-08, device='cuda:0')
Epoch 101
Average batch original loss after noise: 0.380105
Average KL loss: 0.126002
Average total loss: 0.506107
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.9995e-08, device='cuda:0')
Epoch 102
Average batch original loss after noise: 0.370202
Average KL loss: 0.126147
Average total loss: 0.496349
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.4937e-08, device='cuda:0')
Epoch 103
Average batch original loss after noise: 0.370473
Average KL loss: 0.126343
Average total loss: 0.496816
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.9578e-08, device='cuda:0')
Epoch 104
Average batch original loss after noise: 0.369851
Average KL loss: 0.126612
Average total loss: 0.496463
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.7236e-08, device='cuda:0')
Epoch 105
Average batch original loss after noise: 0.367743
Average KL loss: 0.126921
Average total loss: 0.494665
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.1002e-08, device='cuda:0')
Epoch 106
Average batch original loss after noise: 0.362887
Average KL loss: 0.127062
Average total loss: 0.489948
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.7805e-08, device='cuda:0')
Epoch 107
Average batch original loss after noise: 0.364542
Average KL loss: 0.127223
Average total loss: 0.491765
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.3343e-08, device='cuda:0')
Epoch 108
Average batch original loss after noise: 0.362894
Average KL loss: 0.127386
Average total loss: 0.490280
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.4067e-08, device='cuda:0')
Epoch 109
Average batch original loss after noise: 0.363686
Average KL loss: 0.127529
Average total loss: 0.491215
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.5011e-08, device='cuda:0')
Epoch 110
Average batch original loss after noise: 0.364527
Average KL loss: 0.127700
Average total loss: 0.492227
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.6734e-08, device='cuda:0')
Epoch 111
Average batch original loss after noise: 0.357083
Average KL loss: 0.127837
Average total loss: 0.484920
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.5027e-08, device='cuda:0')
Epoch 112
Average batch original loss after noise: 0.351391
Average KL loss: 0.127928
Average total loss: 0.479319
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-7.8729e-09, device='cuda:0')
Epoch 113
Average batch original loss after noise: 0.358044
Average KL loss: 0.128087
Average total loss: 0.486131
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.3774e-08, device='cuda:0')
Epoch 114
Average batch original loss after noise: 0.356847
Average KL loss: 0.128267
Average total loss: 0.485114
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.9647e-08, device='cuda:0')
Epoch 115
Average batch original loss after noise: 0.352990
Average KL loss: 0.128518
Average total loss: 0.481508
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.7459e-08, device='cuda:0')
Epoch 116
Average batch original loss after noise: 0.362750
Average KL loss: 0.128770
Average total loss: 0.491520
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.5399e-08, device='cuda:0')
Epoch 117
Average batch original loss after noise: 0.358829
Average KL loss: 0.129112
Average total loss: 0.487940
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.1773e-08, device='cuda:0')
Epoch 118
Average batch original loss after noise: 0.358524
Average KL loss: 0.129357
Average total loss: 0.487881
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.1463e-08, device='cuda:0')
Epoch 119
Average batch original loss after noise: 0.361821
Average KL loss: 0.129618
Average total loss: 0.491439
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.3164e-08, device='cuda:0')
Epoch 120
Average batch original loss after noise: 0.357212
Average KL loss: 0.129871
Average total loss: 0.487084
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-2.8881e-08, device='cuda:0')
Epoch 121
Average batch original loss after noise: 0.360019
Average KL loss: 0.129993
Average total loss: 0.490012
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.4164e-08, device='cuda:0')
Epoch 122
Average batch original loss after noise: 0.348131
Average KL loss: 0.130228
Average total loss: 0.478359
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.9275e-08, device='cuda:0')
Epoch 123
Average batch original loss after noise: 0.352492
Average KL loss: 0.130396
Average total loss: 0.482888
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.4652e-08, device='cuda:0')
Epoch 124
Average batch original loss after noise: 0.346871
Average KL loss: 0.130582
Average total loss: 0.477453
tensor(0.0018, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(-1.7069e-08, device='cuda:0')
Epoch 125
Average batch original loss after noise: 0.347026
Average KL loss: 0.130785
Average total loss: 0.477811
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7939e-08, device='cuda:0')
Epoch 126
Average batch original loss after noise: 0.355447
Average KL loss: 0.130881
Average total loss: 0.486328
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-9.0203e-09, device='cuda:0')
Epoch 127
Average batch original loss after noise: 0.348921
Average KL loss: 0.131104
Average total loss: 0.480026
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-3.7404e-08, device='cuda:0')
Epoch 128
Average batch original loss after noise: 0.344682
Average KL loss: 0.131308
Average total loss: 0.475990
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-5.2242e-09, device='cuda:0')
Epoch 129
Average batch original loss after noise: 0.341873
Average KL loss: 0.131370
Average total loss: 0.473244
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.9041e-08, device='cuda:0')
Epoch 130
Average batch original loss after noise: 0.352620
Average KL loss: 0.131494
Average total loss: 0.484114
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-6.9930e-09, device='cuda:0')
Epoch 131
Average batch original loss after noise: 0.350253
Average KL loss: 0.131834
Average total loss: 0.482086
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7742e-08, device='cuda:0')
Epoch 132
Average batch original loss after noise: 0.339465
Average KL loss: 0.132122
Average total loss: 0.471587
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-7.6790e-09, device='cuda:0')
Epoch 133
Average batch original loss after noise: 0.347887
Average KL loss: 0.132306
Average total loss: 0.480193
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.5687e-08, device='cuda:0')
Epoch 134
Average batch original loss after noise: 0.345682
Average KL loss: 0.132486
Average total loss: 0.478168
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.4062e-08, device='cuda:0')
Epoch 135
Average batch original loss after noise: 0.339587
Average KL loss: 0.132576
Average total loss: 0.472163
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-3.0609e-09, device='cuda:0')
Epoch 136
Average batch original loss after noise: 0.336094
Average KL loss: 0.132620
Average total loss: 0.468715
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.0311e-08, device='cuda:0')
Epoch 137
Average batch original loss after noise: 0.340417
Average KL loss: 0.132856
Average total loss: 0.473273
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-7.2038e-09, device='cuda:0')
Epoch 138
Average batch original loss after noise: 0.340193
Average KL loss: 0.133110
Average total loss: 0.473302
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.5410e-08, device='cuda:0')
Epoch 139
Average batch original loss after noise: 0.338989
Average KL loss: 0.133274
Average total loss: 0.472263
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-3.7318e-09, device='cuda:0')
Epoch 140
Average batch original loss after noise: 0.328277
Average KL loss: 0.133473
Average total loss: 0.461750
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-2.0365e-08, device='cuda:0')
Epoch 141
Average batch original loss after noise: 0.329421
Average KL loss: 0.133559
Average total loss: 0.462979
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-3.2116e-08, device='cuda:0')
Epoch 142
Average batch original loss after noise: 0.326408
Average KL loss: 0.133713
Average total loss: 0.460120
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7211e-08, device='cuda:0')
Epoch 143
Average batch original loss after noise: 0.329475
Average KL loss: 0.133806
Average total loss: 0.463281
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-2.1190e-08, device='cuda:0')
Epoch 144
Average batch original loss after noise: 0.340830
Average KL loss: 0.134026
Average total loss: 0.474856
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-2.1950e-08, device='cuda:0')
Epoch 145
Average batch original loss after noise: 0.331525
Average KL loss: 0.134274
Average total loss: 0.465800
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-3.5822e-08, device='cuda:0')
Epoch 146
Average batch original loss after noise: 0.324617
Average KL loss: 0.134439
Average total loss: 0.459056
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-6.0127e-09, device='cuda:0')
Epoch 147
Average batch original loss after noise: 0.328022
Average KL loss: 0.134494
Average total loss: 0.462515
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-2.5090e-08, device='cuda:0')
Epoch 148
Average batch original loss after noise: 0.328370
Average KL loss: 0.134655
Average total loss: 0.463026
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7366e-08, device='cuda:0')
Epoch 149
Average batch original loss after noise: 0.331733
Average KL loss: 0.134790
Average total loss: 0.466524
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.0223e-08, device='cuda:0')
Epoch 150
Average batch original loss after noise: 0.329856
Average KL loss: 0.135117
Average total loss: 0.464973
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.3367e-08, device='cuda:0')
Epoch 151
Average batch original loss after noise: 0.331739
Average KL loss: 0.135341
Average total loss: 0.467080
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.5552e-08, device='cuda:0')
Epoch 152
Average batch original loss after noise: 0.326096
Average KL loss: 0.135588
Average total loss: 0.461684
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.5557e-08, device='cuda:0')
Epoch 153
Average batch original loss after noise: 0.321829
Average KL loss: 0.135726
Average total loss: 0.457555
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.0936e-08, device='cuda:0')
Epoch 154
Average batch original loss after noise: 0.326397
Average KL loss: 0.135882
Average total loss: 0.462279
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.0292e-08, device='cuda:0')
Epoch 155
Average batch original loss after noise: 0.323224
Average KL loss: 0.136101
Average total loss: 0.459325
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.8930e-08, device='cuda:0')
Epoch 156
Average batch original loss after noise: 0.325368
Average KL loss: 0.136274
Average total loss: 0.461642
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7760e-08, device='cuda:0')
Epoch 157
Average batch original loss after noise: 0.321777
Average KL loss: 0.136561
Average total loss: 0.458338
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-2.2607e-08, device='cuda:0')
Epoch 158
Average batch original loss after noise: 0.322733
Average KL loss: 0.136723
Average total loss: 0.459456
tensor(0.0018, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(-1.7033e-08, device='cuda:0')
Epoch 159
Average batch original loss after noise: 0.322392
Average KL loss: 0.136993
Average total loss: 0.459385
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6034e-08, device='cuda:0')
Epoch 160
Average batch original loss after noise: 0.314298
Average KL loss: 0.137245
Average total loss: 0.451543
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.1384e-08, device='cuda:0')
Epoch 161
Average batch original loss after noise: 0.321534
Average KL loss: 0.137414
Average total loss: 0.458948
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.6709e-08, device='cuda:0')
Epoch 162
Average batch original loss after noise: 0.320772
Average KL loss: 0.137658
Average total loss: 0.458430
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.5825e-08, device='cuda:0')
Epoch 163
Average batch original loss after noise: 0.316371
Average KL loss: 0.137784
Average total loss: 0.454155
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.4841e-08, device='cuda:0')
Epoch 164
Average batch original loss after noise: 0.318302
Average KL loss: 0.137949
Average total loss: 0.456251
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.8955e-08, device='cuda:0')
Epoch 165
Average batch original loss after noise: 0.322990
Average KL loss: 0.138167
Average total loss: 0.461156
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.0611e-08, device='cuda:0')
Epoch 166
Average batch original loss after noise: 0.313440
Average KL loss: 0.138293
Average total loss: 0.451733
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-4.1865e-09, device='cuda:0')
Epoch 167
Average batch original loss after noise: 0.307606
Average KL loss: 0.138484
Average total loss: 0.446090
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.0050e-08, device='cuda:0')
Epoch 168
Average batch original loss after noise: 0.309681
Average KL loss: 0.138589
Average total loss: 0.448270
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-8.5842e-09, device='cuda:0')
Epoch 169
Average batch original loss after noise: 0.313849
Average KL loss: 0.138779
Average total loss: 0.452627
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.3543e-08, device='cuda:0')
Epoch 170
Average batch original loss after noise: 0.317066
Average KL loss: 0.139011
Average total loss: 0.456077
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6383e-08, device='cuda:0')
Epoch 171
Average batch original loss after noise: 0.306676
Average KL loss: 0.139229
Average total loss: 0.445906
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6332e-08, device='cuda:0')
Epoch 172
Average batch original loss after noise: 0.312481
Average KL loss: 0.139262
Average total loss: 0.451743
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.2608e-08, device='cuda:0')
Epoch 173
Average batch original loss after noise: 0.300725
Average KL loss: 0.139398
Average total loss: 0.440123
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.0818e-08, device='cuda:0')
Epoch 174
Average batch original loss after noise: 0.318180
Average KL loss: 0.139525
Average total loss: 0.457705
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.5971e-08, device='cuda:0')
Epoch 175
Average batch original loss after noise: 0.306841
Average KL loss: 0.139698
Average total loss: 0.446539
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6181e-08, device='cuda:0')
Epoch 176
Average batch original loss after noise: 0.309563
Average KL loss: 0.139868
Average total loss: 0.449431
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.7829e-08, device='cuda:0')
Epoch 177
Average batch original loss after noise: 0.308719
Average KL loss: 0.140039
Average total loss: 0.448758
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-7.3609e-09, device='cuda:0')
Epoch 178
Average batch original loss after noise: 0.304165
Average KL loss: 0.140217
Average total loss: 0.444383
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.5270e-08, device='cuda:0')
Epoch 179
Average batch original loss after noise: 0.315271
Average KL loss: 0.140448
Average total loss: 0.455719
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.1442e-08, device='cuda:0')
Epoch 180
Average batch original loss after noise: 0.305632
Average KL loss: 0.140730
Average total loss: 0.446362
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.9985e-08, device='cuda:0')
Epoch 181
Average batch original loss after noise: 0.303575
Average KL loss: 0.140816
Average total loss: 0.444391
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.0019e-08, device='cuda:0')
Epoch 182
Average batch original loss after noise: 0.297948
Average KL loss: 0.140986
Average total loss: 0.438933
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.9410e-08, device='cuda:0')
Epoch 183
Average batch original loss after noise: 0.304419
Average KL loss: 0.141129
Average total loss: 0.445548
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.2786e-08, device='cuda:0')
Epoch 184
Average batch original loss after noise: 0.304652
Average KL loss: 0.141364
Average total loss: 0.446016
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(4.3720e-10, device='cuda:0')
Epoch 185
Average batch original loss after noise: 0.295784
Average KL loss: 0.141375
Average total loss: 0.437159
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6443e-08, device='cuda:0')
Epoch 186
Average batch original loss after noise: 0.301355
Average KL loss: 0.141449
Average total loss: 0.442804
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.3633e-08, device='cuda:0')
Epoch 187
Average batch original loss after noise: 0.306887
Average KL loss: 0.141675
Average total loss: 0.448562
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.0634e-08, device='cuda:0')
Epoch 188
Average batch original loss after noise: 0.304158
Average KL loss: 0.141946
Average total loss: 0.446104
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-3.7783e-09, device='cuda:0')
Epoch 189
Average batch original loss after noise: 0.308623
Average KL loss: 0.142136
Average total loss: 0.450759
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-2.2997e-08, device='cuda:0')
Epoch 190
Average batch original loss after noise: 0.296039
Average KL loss: 0.142353
Average total loss: 0.438392
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.2540e-08, device='cuda:0')
Epoch 191
Average batch original loss after noise: 0.294833
Average KL loss: 0.142457
Average total loss: 0.437290
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.6627e-08, device='cuda:0')
Epoch 192
Average batch original loss after noise: 0.294285
Average KL loss: 0.142543
Average total loss: 0.436828
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.4767e-08, device='cuda:0')
Epoch 193
Average batch original loss after noise: 0.302483
Average KL loss: 0.142848
Average total loss: 0.445331
tensor(0.0018, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-8.6249e-09, device='cuda:0')
Epoch 194
Average batch original loss after noise: 0.305216
Average KL loss: 0.143087
Average total loss: 0.448302
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.9415e-08, device='cuda:0')
Epoch 195
Average batch original loss after noise: 0.301012
Average KL loss: 0.143317
Average total loss: 0.444329
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.6861e-08, device='cuda:0')
Epoch 196
Average batch original loss after noise: 0.289237
Average KL loss: 0.143377
Average total loss: 0.432613
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-5.0452e-09, device='cuda:0')
Epoch 197
Average batch original loss after noise: 0.296198
Average KL loss: 0.143459
Average total loss: 0.439657
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.2300e-08, device='cuda:0')
Epoch 198
Average batch original loss after noise: 0.284872
Average KL loss: 0.143620
Average total loss: 0.428492
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-7.3034e-09, device='cuda:0')
Epoch 199
Average batch original loss after noise: 0.300060
Average KL loss: 0.143727
Average total loss: 0.443787
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-7.1296e-09, device='cuda:0')
Epoch 200
Average batch original loss after noise: 0.282778
Average KL loss: 0.143887
Average total loss: 0.426665
 Percentile value: 0.004937547259032726
Non-zero model percentage: 25.000003814697266%, Non-zero mask percentage: 25.000003814697266%

--- Pruning Level [2/12]: ---
conv1.weight         | nonzeros =    1100 /    1728             ( 63.66%) | total_pruned =     628 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       5 /      64             (  7.81%) | total_pruned =      59 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   11555 /   36864             ( 31.34%) | total_pruned =   25309 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   11743 /   36864             ( 31.85%) | total_pruned =   25121 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   11660 /   36864             ( 31.63%) | total_pruned =   25204 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   11177 /   36864             ( 30.32%) | total_pruned =   25687 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   22088 /   73728             ( 29.96%) | total_pruned =   51640 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   43022 /  147456             ( 29.18%) | total_pruned =  104434 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3536 /    8192             ( 43.16%) | total_pruned =    4656 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     120 /     128             ( 93.75%) | total_pruned =       8 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   40313 /  147456             ( 27.34%) | total_pruned =  107143 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   39512 /  147456             ( 26.80%) | total_pruned =  107944 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      36 /     128             ( 28.12%) | total_pruned =      92 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   85473 /  294912             ( 28.98%) | total_pruned =  209439 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  166186 /  589824             ( 28.18%) | total_pruned =  423638 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   12253 /   32768             ( 37.39%) | total_pruned =   20515 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  146426 /  589824             ( 24.83%) | total_pruned =  443398 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  142969 /  589824             ( 24.24%) | total_pruned =  446855 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      69 /     256             ( 26.95%) | total_pruned =     187 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  317914 / 1179648             ( 26.95%) | total_pruned =  861734 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     488 /     512             ( 95.31%) | total_pruned =      24 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     329 /     512             ( 64.26%) | total_pruned =     183 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  592877 / 2359296             ( 25.13%) | total_pruned = 1766419 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     444 /     512             ( 86.72%) | total_pruned =      68 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   37519 /  131072             ( 28.62%) | total_pruned =   93553 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     414 /     512             ( 80.86%) | total_pruned =      98 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     428 /     512             ( 83.59%) | total_pruned =      84 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  549415 / 2359296             ( 23.29%) | total_pruned = 1809881 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     379 /     512             ( 74.02%) | total_pruned =     133 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     315 /     512             ( 61.52%) | total_pruned =     197 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  537397 / 2359296             ( 22.78%) | total_pruned = 1821899 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     500 /     512             ( 97.66%) | total_pruned =      12 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     261 /     512             ( 50.98%) | total_pruned =     251 | shape = torch.Size([512])
linear.weight        | nonzeros =    3353 /    5120             ( 65.49%) | total_pruned =    1767 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       2 /      10             ( 20.00%) | total_pruned =       8 | shape = torch.Size([10])
alive: 2794691, pruned : 8384071, total: 11178762, Compression rate :       4.00x  ( 75.00% pruned)
Train Epoch: 32/100 Loss: 0.000233 Accuracy: 86.65 100.00 % Best test Accuracy: 86.77%
tensor(0.0018, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-2.8395e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 0.505315
Average KL loss: 0.136389
Average total loss: 0.641704
tensor(0.0031, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.5368e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 0.486149
Average KL loss: 0.133772
Average total loss: 0.619921
tensor(0.0031, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(3.6308e-09, device='cuda:0')
Epoch 3
Average batch original loss after noise: 0.507219
Average KL loss: 0.134243
Average total loss: 0.641462
tensor(0.0030, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-6.9082e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 0.474955
Average KL loss: 0.135260
Average total loss: 0.610214
tensor(0.0030, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-1.3202e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 0.489725
Average KL loss: 0.136331
Average total loss: 0.626055
tensor(0.0029, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-7.7039e-09, device='cuda:0')
Epoch 6
Average batch original loss after noise: 0.471795
Average KL loss: 0.137411
Average total loss: 0.609206
tensor(0.0029, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(1.1394e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 0.468183
Average KL loss: 0.138365
Average total loss: 0.606548
tensor(0.0029, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(2.1951e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 0.458785
Average KL loss: 0.139329
Average total loss: 0.598114
tensor(0.0028, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(-4.5035e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 0.457581
Average KL loss: 0.140331
Average total loss: 0.597912
tensor(0.0028, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(1.1728e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 0.458023
Average KL loss: 0.141300
Average total loss: 0.599322
tensor(0.0028, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.4298e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 0.449827
Average KL loss: 0.142260
Average total loss: 0.592087
tensor(0.0027, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-2.9531e-09, device='cuda:0')
Epoch 12
Average batch original loss after noise: 0.458939
Average KL loss: 0.143036
Average total loss: 0.601976
tensor(0.0027, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(5.4698e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 0.458795
Average KL loss: 0.143759
Average total loss: 0.602555
tensor(0.0027, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-8.7277e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 0.445559
Average KL loss: 0.144573
Average total loss: 0.590132
tensor(0.0027, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.0267e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 0.434456
Average KL loss: 0.145278
Average total loss: 0.579734
tensor(0.0026, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-1.8357e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 0.440952
Average KL loss: 0.145963
Average total loss: 0.586914
tensor(0.0026, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-6.8225e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 0.438542
Average KL loss: 0.146597
Average total loss: 0.585139
tensor(0.0026, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-2.2628e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 0.439525
Average KL loss: 0.147241
Average total loss: 0.586766
tensor(0.0026, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-4.2427e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 0.439951
Average KL loss: 0.147845
Average total loss: 0.587796
tensor(0.0026, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.4868e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 0.424040
Average KL loss: 0.148471
Average total loss: 0.572511
tensor(0.0025, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-9.9029e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 0.421268
Average KL loss: 0.148917
Average total loss: 0.570185
tensor(0.0025, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-2.9591e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 0.430257
Average KL loss: 0.149438
Average total loss: 0.579695
tensor(0.0025, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-4.7045e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 0.426936
Average KL loss: 0.150020
Average total loss: 0.576956
tensor(0.0025, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(-4.0048e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 0.425253
Average KL loss: 0.150563
Average total loss: 0.575815
tensor(0.0025, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(6.0773e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 0.423300
Average KL loss: 0.151085
Average total loss: 0.574385
tensor(0.0025, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-7.1598e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 0.426622
Average KL loss: 0.151479
Average total loss: 0.578101
tensor(0.0025, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-3.8203e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 0.426421
Average KL loss: 0.152088
Average total loss: 0.578508
tensor(0.0025, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-3.7712e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 0.423125
Average KL loss: 0.152627
Average total loss: 0.575751
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-5.4147e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 0.414434
Average KL loss: 0.153087
Average total loss: 0.567522
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-5.8679e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 0.414834
Average KL loss: 0.153498
Average total loss: 0.568332
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-2.2524e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 0.426643
Average KL loss: 0.153902
Average total loss: 0.580545
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.1863e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 0.413869
Average KL loss: 0.154388
Average total loss: 0.568257
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-3.9592e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 0.409563
Average KL loss: 0.154846
Average total loss: 0.564409
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.5395e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 0.407847
Average KL loss: 0.155147
Average total loss: 0.562993
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.6923e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 0.410628
Average KL loss: 0.155480
Average total loss: 0.566109
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-6.1758e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 0.411872
Average KL loss: 0.155838
Average total loss: 0.567710
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-2.7149e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 0.414730
Average KL loss: 0.156153
Average total loss: 0.570883
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-8.4531e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 0.400150
Average KL loss: 0.156502
Average total loss: 0.556652
tensor(0.0024, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(4.1113e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 0.397707
Average KL loss: 0.156721
Average total loss: 0.554428
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.4168e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 0.406966
Average KL loss: 0.157084
Average total loss: 0.564050
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-6.1162e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 0.402161
Average KL loss: 0.157427
Average total loss: 0.559587
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(3.5757e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 0.410451
Average KL loss: 0.157726
Average total loss: 0.568178
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(5.5288e-09, device='cuda:0')
Epoch 43
Average batch original loss after noise: 0.404231
Average KL loss: 0.158066
Average total loss: 0.562296
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.6293e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 0.397466
Average KL loss: 0.158388
Average total loss: 0.555854
tensor(0.0023, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-1.4279e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 0.399268
Average KL loss: 0.158673
Average total loss: 0.557940
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.5032e-09, device='cuda:0')
Epoch 46
Average batch original loss after noise: 0.401952
Average KL loss: 0.158977
Average total loss: 0.560929
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.4358e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 0.397613
Average KL loss: 0.159244
Average total loss: 0.556858
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.5818e-09, device='cuda:0')
Epoch 48
Average batch original loss after noise: 0.404397
Average KL loss: 0.159554
Average total loss: 0.563951
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.8153e-09, device='cuda:0')
Epoch 49
Average batch original loss after noise: 0.400131
Average KL loss: 0.159852
Average total loss: 0.559983
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.4353e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 0.396768
Average KL loss: 0.160106
Average total loss: 0.556874
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.1417e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 0.390259
Average KL loss: 0.160232
Average total loss: 0.550491
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-9.5250e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 0.392280
Average KL loss: 0.160150
Average total loss: 0.552430
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.6456e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 0.399411
Average KL loss: 0.160069
Average total loss: 0.559479
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.8687e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 0.394619
Average KL loss: 0.159994
Average total loss: 0.554613
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-3.9045e-09, device='cuda:0')
Epoch 55
Average batch original loss after noise: 0.388081
Average KL loss: 0.159923
Average total loss: 0.548005
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.2475e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 0.399812
Average KL loss: 0.159856
Average total loss: 0.559669
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.8736e-09, device='cuda:0')
Epoch 57
Average batch original loss after noise: 0.388864
Average KL loss: 0.159792
Average total loss: 0.548656
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-9.7391e-09, device='cuda:0')
Epoch 58
Average batch original loss after noise: 0.394712
Average KL loss: 0.159727
Average total loss: 0.554439
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.5088e-09, device='cuda:0')
Epoch 59
Average batch original loss after noise: 0.380989
Average KL loss: 0.159666
Average total loss: 0.540655
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.4819e-09, device='cuda:0')
Epoch 60
Average batch original loss after noise: 0.383576
Average KL loss: 0.159604
Average total loss: 0.543180
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.3706e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.387974
Average KL loss: 0.159545
Average total loss: 0.547519
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.9971e-09, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.384466
Average KL loss: 0.159486
Average total loss: 0.543951
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.2189e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 0.394054
Average KL loss: 0.159431
Average total loss: 0.553485
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-9.9101e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.401115
Average KL loss: 0.159388
Average total loss: 0.560503
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.1550e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.392759
Average KL loss: 0.159350
Average total loss: 0.552109
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.3824e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.395944
Average KL loss: 0.159303
Average total loss: 0.555247
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.6496e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.390392
Average KL loss: 0.159253
Average total loss: 0.549645
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.0362e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.384846
Average KL loss: 0.159209
Average total loss: 0.544055
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.0295e-09, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.395997
Average KL loss: 0.159169
Average total loss: 0.555166
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-4.8959e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.376380
Average KL loss: 0.159140
Average total loss: 0.535520
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-6.5522e-09, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.383660
Average KL loss: 0.159096
Average total loss: 0.542756
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-6.6393e-09, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.392072
Average KL loss: 0.159061
Average total loss: 0.551133
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.0412e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 0.385909
Average KL loss: 0.159034
Average total loss: 0.544944
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(8.8787e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 0.390759
Average KL loss: 0.159000
Average total loss: 0.549759
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.6854e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 0.389240
Average KL loss: 0.158965
Average total loss: 0.548205
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.0673e-09, device='cuda:0')
Epoch 76
Average batch original loss after noise: 0.390956
Average KL loss: 0.158934
Average total loss: 0.549890
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.0082e-09, device='cuda:0')
Epoch 77
Average batch original loss after noise: 0.391425
Average KL loss: 0.158910
Average total loss: 0.550335
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(6.4117e-09, device='cuda:0')
Epoch 78
Average batch original loss after noise: 0.394835
Average KL loss: 0.158890
Average total loss: 0.553726
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.0773e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 0.399510
Average KL loss: 0.158868
Average total loss: 0.558378
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.7765e-09, device='cuda:0')
Epoch 80
Average batch original loss after noise: 0.397324
Average KL loss: 0.158847
Average total loss: 0.556172
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-8.2613e-09, device='cuda:0')
Epoch 81
Average batch original loss after noise: 0.395223
Average KL loss: 0.158830
Average total loss: 0.554053
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.0796e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 0.379049
Average KL loss: 0.158817
Average total loss: 0.537866
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.5504e-09, device='cuda:0')
Epoch 83
Average batch original loss after noise: 0.389073
Average KL loss: 0.158813
Average total loss: 0.547886
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-9.1064e-09, device='cuda:0')
Epoch 84
Average batch original loss after noise: 0.383335
Average KL loss: 0.158809
Average total loss: 0.542144
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-3.6812e-09, device='cuda:0')
Epoch 85
Average batch original loss after noise: 0.396017
Average KL loss: 0.158805
Average total loss: 0.554822
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.0950e-08, device='cuda:0')
Epoch 86
Average batch original loss after noise: 0.397264
Average KL loss: 0.158802
Average total loss: 0.556066
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.5017e-08, device='cuda:0')
Epoch 87
Average batch original loss after noise: 0.389656
Average KL loss: 0.158799
Average total loss: 0.548455
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-6.2221e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 0.383514
Average KL loss: 0.158795
Average total loss: 0.542309
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.3404e-09, device='cuda:0')
Epoch 89
Average batch original loss after noise: 0.386456
Average KL loss: 0.158791
Average total loss: 0.545247
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.8508e-09, device='cuda:0')
Epoch 90
Average batch original loss after noise: 0.384594
Average KL loss: 0.158788
Average total loss: 0.543382
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.4800e-09, device='cuda:0')
Epoch 91
Average batch original loss after noise: 0.388237
Average KL loss: 0.158785
Average total loss: 0.547022
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.5645e-09, device='cuda:0')
 Percentile value: 0.008897920604795218
Non-zero model percentage: 12.500005722045898%, Non-zero mask percentage: 12.500005722045898%

--- Pruning Level [3/12]: ---
conv1.weight         | nonzeros =    1034 /    1728             ( 59.84%) | total_pruned =     694 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    7811 /   36864             ( 21.19%) | total_pruned =   29053 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    8106 /   36864             ( 21.99%) | total_pruned =   28758 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7808 /   36864             ( 21.18%) | total_pruned =   29056 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7204 /   36864             ( 19.54%) | total_pruned =   29660 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   14071 /   73728             ( 19.09%) | total_pruned =   59657 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   26519 /  147456             ( 17.98%) | total_pruned =  120937 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2838 /    8192             ( 34.64%) | total_pruned =    5354 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     120 /     128             ( 93.75%) | total_pruned =       8 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   21905 /  147456             ( 14.86%) | total_pruned =  125551 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   21643 /  147456             ( 14.68%) | total_pruned =  125813 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   51605 /  294912             ( 17.50%) | total_pruned =  243307 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   97092 /  589824             ( 16.46%) | total_pruned =  492732 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     112 /     256             ( 43.75%) | total_pruned =     144 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    9026 /   32768             ( 27.55%) | total_pruned =   23742 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   69792 /  589824             ( 11.83%) | total_pruned =  520032 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   66538 /  589824             ( 11.28%) | total_pruned =  523286 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      69 /     256             ( 26.95%) | total_pruned =     187 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  176302 / 1179648             ( 14.95%) | total_pruned = 1003346 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     487 /     512             ( 95.12%) | total_pruned =      25 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     296 /     512             ( 57.81%) | total_pruned =     216 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  298440 / 2359296             ( 12.65%) | total_pruned = 2060856 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     424 /     512             ( 82.81%) | total_pruned =      88 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   21241 /  131072             ( 16.21%) | total_pruned =  109831 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     410 /     512             ( 80.08%) | total_pruned =     102 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     409 /     512             ( 79.88%) | total_pruned =     103 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  241596 / 2359296             ( 10.24%) | total_pruned = 2117700 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     257 /     512             ( 50.20%) | total_pruned =     255 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  236674 / 2359296             ( 10.03%) | total_pruned = 2122622 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     498 /     512             ( 97.27%) | total_pruned =      14 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     240 /     512             ( 46.88%) | total_pruned =     272 | shape = torch.Size([512])
linear.weight        | nonzeros =    3149 /    5120             ( 61.50%) | total_pruned =    1971 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 1397346, pruned : 9781416, total: 11178762, Compression rate :       8.00x  ( 87.50% pruned)
Train Epoch: 30/100 Loss: 0.000028 Accuracy: 86.64 100.00 % Best test Accuracy: 86.94%
tensor(0.0023, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-5.7825e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 0.631804
Average KL loss: 0.152642
Average total loss: 0.784446
tensor(0.0031, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-4.5257e-09, device='cuda:0')
Epoch 2
Average batch original loss after noise: 0.595009
Average KL loss: 0.150208
Average total loss: 0.745217
tensor(0.0031, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-2.8782e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 0.603811
Average KL loss: 0.150553
Average total loss: 0.754364
tensor(0.0031, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-4.2319e-09, device='cuda:0')
Epoch 4
Average batch original loss after noise: 0.597044
Average KL loss: 0.151311
Average total loss: 0.748355
tensor(0.0031, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-9.7564e-09, device='cuda:0')
Epoch 5
Average batch original loss after noise: 0.574503
Average KL loss: 0.152177
Average total loss: 0.726680
tensor(0.0031, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-2.9573e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 0.576016
Average KL loss: 0.153030
Average total loss: 0.729046
tensor(0.0030, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(9.4933e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 0.580072
Average KL loss: 0.153870
Average total loss: 0.733942
tensor(0.0030, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-9.7378e-09, device='cuda:0')
Epoch 8
Average batch original loss after noise: 0.569923
Average KL loss: 0.154710
Average total loss: 0.724632
tensor(0.0030, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(-8.6889e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 0.560116
Average KL loss: 0.155560
Average total loss: 0.715676
tensor(0.0030, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.5268e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 0.578751
Average KL loss: 0.156447
Average total loss: 0.735198
tensor(0.0030, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.7647e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 0.548010
Average KL loss: 0.157300
Average total loss: 0.705310
tensor(0.0029, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.4623e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 0.540729
Average KL loss: 0.158019
Average total loss: 0.698749
tensor(0.0029, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-1.7182e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 0.531080
Average KL loss: 0.158716
Average total loss: 0.689796
tensor(0.0029, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(6.5170e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 0.543180
Average KL loss: 0.159438
Average total loss: 0.702618
tensor(0.0029, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.3042e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 0.545054
Average KL loss: 0.160123
Average total loss: 0.705177
tensor(0.0029, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.2960e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 0.560266
Average KL loss: 0.160882
Average total loss: 0.721148
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(7.2619e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 0.552275
Average KL loss: 0.161614
Average total loss: 0.713889
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(8.7543e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 0.526151
Average KL loss: 0.162274
Average total loss: 0.688426
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.8930e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 0.534108
Average KL loss: 0.162761
Average total loss: 0.696868
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-7.4802e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 0.530432
Average KL loss: 0.163362
Average total loss: 0.693794
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-2.1896e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 0.544046
Average KL loss: 0.164091
Average total loss: 0.708137
tensor(0.0028, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(-3.7741e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 0.526194
Average KL loss: 0.164795
Average total loss: 0.690990
tensor(0.0028, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.4951e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 0.517406
Average KL loss: 0.165298
Average total loss: 0.682704
tensor(0.0028, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.4437e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 0.517942
Average KL loss: 0.165774
Average total loss: 0.683716
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.9270e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 0.525272
Average KL loss: 0.166285
Average total loss: 0.691557
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-2.8196e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 0.515133
Average KL loss: 0.166763
Average total loss: 0.681896
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.5283e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 0.519053
Average KL loss: 0.167222
Average total loss: 0.686275
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(8.0325e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 0.515742
Average KL loss: 0.167735
Average total loss: 0.683476
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.7046e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 0.501289
Average KL loss: 0.168122
Average total loss: 0.669411
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-7.8938e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 0.522623
Average KL loss: 0.168534
Average total loss: 0.691157
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(3.0991e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 0.525349
Average KL loss: 0.169009
Average total loss: 0.694358
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.2263e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 0.514813
Average KL loss: 0.169606
Average total loss: 0.684419
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-3.6317e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 0.513560
Average KL loss: 0.170056
Average total loss: 0.683616
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-7.7798e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 0.514616
Average KL loss: 0.170464
Average total loss: 0.685080
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.5843e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 0.502066
Average KL loss: 0.170884
Average total loss: 0.672950
tensor(0.0027, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-2.9406e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 0.506674
Average KL loss: 0.171247
Average total loss: 0.677921
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(3.9189e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 0.505445
Average KL loss: 0.171679
Average total loss: 0.677123
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-5.5479e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 0.497568
Average KL loss: 0.172075
Average total loss: 0.669644
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.4558e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 0.490318
Average KL loss: 0.172475
Average total loss: 0.662794
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.2206e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 0.507746
Average KL loss: 0.172749
Average total loss: 0.680495
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.1852e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 0.497264
Average KL loss: 0.173205
Average total loss: 0.670469
tensor(0.0026, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.2460e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 0.494958
Average KL loss: 0.173525
Average total loss: 0.668484
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.7698e-09, device='cuda:0')
Epoch 43
Average batch original loss after noise: 0.491157
Average KL loss: 0.173864
Average total loss: 0.665021
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.2147e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 0.497927
Average KL loss: 0.174191
Average total loss: 0.672118
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.8624e-09, device='cuda:0')
Epoch 45
Average batch original loss after noise: 0.510208
Average KL loss: 0.174509
Average total loss: 0.684717
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.4690e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 0.500726
Average KL loss: 0.175000
Average total loss: 0.675726
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.0876e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 0.498764
Average KL loss: 0.175383
Average total loss: 0.674147
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(8.7116e-09, device='cuda:0')
Epoch 48
Average batch original loss after noise: 0.494857
Average KL loss: 0.175717
Average total loss: 0.670574
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.5235e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 0.492576
Average KL loss: 0.176076
Average total loss: 0.668652
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.2222e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 0.491903
Average KL loss: 0.176353
Average total loss: 0.668256
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-5.9474e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 0.485354
Average KL loss: 0.176484
Average total loss: 0.661838
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.0260e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 0.487948
Average KL loss: 0.176452
Average total loss: 0.664401
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.9081e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 0.495018
Average KL loss: 0.176418
Average total loss: 0.671435
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.2621e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 0.492528
Average KL loss: 0.176389
Average total loss: 0.668917
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(3.8288e-09, device='cuda:0')
Epoch 55
Average batch original loss after noise: 0.487763
Average KL loss: 0.176362
Average total loss: 0.664125
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(3.5823e-09, device='cuda:0')
Epoch 56
Average batch original loss after noise: 0.485805
Average KL loss: 0.176333
Average total loss: 0.662138
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-7.7805e-09, device='cuda:0')
Epoch 57
Average batch original loss after noise: 0.480485
Average KL loss: 0.176302
Average total loss: 0.656786
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.8218e-09, device='cuda:0')
Epoch 58
Average batch original loss after noise: 0.484189
Average KL loss: 0.176271
Average total loss: 0.660461
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.7539e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 0.483285
Average KL loss: 0.176245
Average total loss: 0.659529
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.0368e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 0.498487
Average KL loss: 0.176225
Average total loss: 0.674712
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.8455e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.490903
Average KL loss: 0.176211
Average total loss: 0.667114
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.4653e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.487533
Average KL loss: 0.176193
Average total loss: 0.663727
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.5855e-09, device='cuda:0')
Epoch 63
Average batch original loss after noise: 0.485521
Average KL loss: 0.176172
Average total loss: 0.661693
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.7712e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.504080
Average KL loss: 0.176153
Average total loss: 0.680233
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.0112e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.490613
Average KL loss: 0.176147
Average total loss: 0.666759
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.1495e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.493843
Average KL loss: 0.176138
Average total loss: 0.669981
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.7709e-09, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.495684
Average KL loss: 0.176122
Average total loss: 0.671806
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-8.8384e-09, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.501814
Average KL loss: 0.176117
Average total loss: 0.677930
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(8.4825e-09, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.506222
Average KL loss: 0.176113
Average total loss: 0.682335
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.3100e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.486703
Average KL loss: 0.176112
Average total loss: 0.662815
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.5763e-09, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.476308
Average KL loss: 0.176109
Average total loss: 0.652417
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.6735e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.489689
Average KL loss: 0.176107
Average total loss: 0.665795
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.9807e-09, device='cuda:0')
Epoch 73
Average batch original loss after noise: 0.488319
Average KL loss: 0.176105
Average total loss: 0.664424
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.5150e-08, device='cuda:0')
Epoch 74
Average batch original loss after noise: 0.486523
Average KL loss: 0.176103
Average total loss: 0.662626
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(3.0873e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 0.484549
Average KL loss: 0.176101
Average total loss: 0.660650
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.3248e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 0.486433
Average KL loss: 0.176099
Average total loss: 0.662532
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.1335e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 0.497536
Average KL loss: 0.176097
Average total loss: 0.673633
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.0311e-08, device='cuda:0')
Epoch 78
Average batch original loss after noise: 0.484188
Average KL loss: 0.176095
Average total loss: 0.660283
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.7509e-09, device='cuda:0')
Epoch 79
Average batch original loss after noise: 0.490659
Average KL loss: 0.176093
Average total loss: 0.666751
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.5627e-09, device='cuda:0')
Epoch 80
Average batch original loss after noise: 0.484079
Average KL loss: 0.176091
Average total loss: 0.660169
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-5.7499e-09, device='cuda:0')
Epoch 81
Average batch original loss after noise: 0.486762
Average KL loss: 0.176089
Average total loss: 0.662851
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.1243e-08, device='cuda:0')
 Percentile value: 0.02409302443265915
Non-zero model percentage: 6.250002861022949%, Non-zero mask percentage: 6.250002861022949%

--- Pruning Level [4/12]: ---
conv1.weight         | nonzeros =     992 /    1728             ( 57.41%) | total_pruned =     736 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5719 /   36864             ( 15.51%) | total_pruned =   31145 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    5977 /   36864             ( 16.21%) | total_pruned =   30887 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    5711 /   36864             ( 15.49%) | total_pruned =   31153 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    5011 /   36864             ( 13.59%) | total_pruned =   31853 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    9803 /   73728             ( 13.30%) | total_pruned =   63925 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   17388 /  147456             ( 11.79%) | total_pruned =  130068 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      47 /     128             ( 36.72%) | total_pruned =      81 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2387 /    8192             ( 29.14%) | total_pruned =    5805 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     120 /     128             ( 93.75%) | total_pruned =       8 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   12678 /  147456             (  8.60%) | total_pruned =  134778 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   12508 /  147456             (  8.48%) | total_pruned =  134948 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      34 /     128             ( 26.56%) | total_pruned =      94 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   32825 /  294912             ( 11.13%) | total_pruned =  262087 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   57913 /  589824             (  9.82%) | total_pruned =  531911 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     108 /     256             ( 42.19%) | total_pruned =     148 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    6981 /   32768             ( 21.30%) | total_pruned =   25787 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   34666 /  589824             (  5.88%) | total_pruned =  555158 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     109 /     256             ( 42.58%) | total_pruned =     147 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   33255 /  589824             (  5.64%) | total_pruned =  556569 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      64 /     256             ( 25.00%) | total_pruned =     192 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   98789 / 1179648             (  8.37%) | total_pruned = 1080859 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     487 /     512             ( 95.12%) | total_pruned =      25 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     279 /     512             ( 54.49%) | total_pruned =     233 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  145318 / 2359296             (  6.16%) | total_pruned = 2213978 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     419 /     512             ( 81.84%) | total_pruned =      93 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   13313 /  131072             ( 10.16%) | total_pruned =  117759 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     408 /     512             ( 79.69%) | total_pruned =     104 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     399 /     512             ( 77.93%) | total_pruned =     113 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  102368 / 2359296             (  4.34%) | total_pruned = 2256928 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     232 /     512             ( 45.31%) | total_pruned =     280 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   85350 / 2359296             (  3.62%) | total_pruned = 2273946 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     496 /     512             ( 96.88%) | total_pruned =      16 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
linear.weight        | nonzeros =    2927 /    5120             ( 57.17%) | total_pruned =    2193 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 698673, pruned : 10480089, total: 11178762, Compression rate :      16.00x  ( 93.75% pruned)
Train Epoch: 27/100 Loss: 0.000098 Accuracy: 86.19 100.00 % Best test Accuracy: 86.87%
tensor(0.0026, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.7055e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 0.790718
Average KL loss: 0.170812
Average total loss: 0.961529
tensor(0.0031, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-9.3775e-09, device='cuda:0')
Epoch 2
Average batch original loss after noise: 0.774860
Average KL loss: 0.168301
Average total loss: 0.943161
tensor(0.0032, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.1344e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 0.758606
Average KL loss: 0.168319
Average total loss: 0.926925
tensor(0.0032, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-1.8307e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 0.751658
Average KL loss: 0.168865
Average total loss: 0.920524
tensor(0.0032, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-8.8913e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 0.753115
Average KL loss: 0.169610
Average total loss: 0.922725
tensor(0.0032, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(-3.7016e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 0.733712
Average KL loss: 0.170376
Average total loss: 0.904088
tensor(0.0032, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-6.7202e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 0.724882
Average KL loss: 0.171169
Average total loss: 0.896051
tensor(0.0032, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.1128e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 0.745248
Average KL loss: 0.172037
Average total loss: 0.917286
tensor(0.0032, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-2.7226e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 0.731710
Average KL loss: 0.172906
Average total loss: 0.904615
tensor(0.0032, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.1962e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 0.717053
Average KL loss: 0.173674
Average total loss: 0.890727
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-8.1516e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 0.717831
Average KL loss: 0.174432
Average total loss: 0.892263
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.1521e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 0.710596
Average KL loss: 0.175277
Average total loss: 0.885872
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.0148e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 0.707515
Average KL loss: 0.176115
Average total loss: 0.883630
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-4.3238e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 0.690779
Average KL loss: 0.176908
Average total loss: 0.867687
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(1.7264e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 0.708291
Average KL loss: 0.177650
Average total loss: 0.885941
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-1.5558e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 0.710396
Average KL loss: 0.178524
Average total loss: 0.888921
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.9068e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 0.715237
Average KL loss: 0.179352
Average total loss: 0.894588
tensor(0.0031, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(-3.6293e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 0.693432
Average KL loss: 0.180131
Average total loss: 0.873564
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-1.7345e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 0.709175
Average KL loss: 0.180885
Average total loss: 0.890059
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-5.3621e-08, device='cuda:0')
Epoch 20
Average batch original loss after noise: 0.705132
Average KL loss: 0.181729
Average total loss: 0.886861
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-1.5031e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 0.676419
Average KL loss: 0.182532
Average total loss: 0.858951
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.9000e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 0.694886
Average KL loss: 0.183303
Average total loss: 0.878189
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(7.0893e-09, device='cuda:0')
Epoch 23
Average batch original loss after noise: 0.696785
Average KL loss: 0.184079
Average total loss: 0.880863
tensor(0.0031, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-3.5714e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 0.687272
Average KL loss: 0.184835
Average total loss: 0.872107
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-3.9879e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 0.663054
Average KL loss: 0.185406
Average total loss: 0.848460
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-1.5855e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 0.689936
Average KL loss: 0.186057
Average total loss: 0.875993
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-5.6722e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 0.673183
Average KL loss: 0.186670
Average total loss: 0.859854
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-9.2295e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 0.653437
Average KL loss: 0.187235
Average total loss: 0.840672
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-1.8278e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 0.667288
Average KL loss: 0.187767
Average total loss: 0.855055
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(-1.2521e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 0.690293
Average KL loss: 0.188499
Average total loss: 0.878791
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.6848e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 0.664025
Average KL loss: 0.189173
Average total loss: 0.853197
tensor(0.0030, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(7.8055e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 0.663401
Average KL loss: 0.189853
Average total loss: 0.853254
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-2.0468e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 0.655650
Average KL loss: 0.190428
Average total loss: 0.846078
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-2.9566e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 0.656106
Average KL loss: 0.190855
Average total loss: 0.846961
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-4.2325e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 0.647676
Average KL loss: 0.191341
Average total loss: 0.839017
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.2745e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 0.652102
Average KL loss: 0.191843
Average total loss: 0.843945
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-2.2738e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 0.676655
Average KL loss: 0.192446
Average total loss: 0.869101
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-1.0401e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 0.644569
Average KL loss: 0.193094
Average total loss: 0.837663
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7364e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 0.646481
Average KL loss: 0.193554
Average total loss: 0.840035
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-3.5034e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 0.645712
Average KL loss: 0.193973
Average total loss: 0.839685
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-7.0696e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 0.647883
Average KL loss: 0.194480
Average total loss: 0.842362
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-1.2141e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 0.618885
Average KL loss: 0.194922
Average total loss: 0.813807
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-9.8387e-09, device='cuda:0')
Epoch 43
Average batch original loss after noise: 0.650108
Average KL loss: 0.195337
Average total loss: 0.845445
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-6.4464e-09, device='cuda:0')
Epoch 44
Average batch original loss after noise: 0.645965
Average KL loss: 0.195816
Average total loss: 0.841781
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0931e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 0.647100
Average KL loss: 0.196288
Average total loss: 0.843388
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-1.6584e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 0.641004
Average KL loss: 0.196700
Average total loss: 0.837704
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.4131e-09, device='cuda:0')
Epoch 47
Average batch original loss after noise: 0.628132
Average KL loss: 0.197152
Average total loss: 0.825284
tensor(0.0030, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-1.4343e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 0.648474
Average KL loss: 0.197535
Average total loss: 0.846009
tensor(0.0029, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-4.8184e-09, device='cuda:0')
Epoch 49
Average batch original loss after noise: 0.636386
Average KL loss: 0.198054
Average total loss: 0.834440
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.3478e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 0.665006
Average KL loss: 0.198563
Average total loss: 0.863569
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(6.9270e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 0.632673
Average KL loss: 0.199120
Average total loss: 0.831794
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(3.5927e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 0.629983
Average KL loss: 0.199578
Average total loss: 0.829561
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.6448e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 0.637006
Average KL loss: 0.199957
Average total loss: 0.836963
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.9570e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 0.623614
Average KL loss: 0.200180
Average total loss: 0.823794
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(1.0017e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 0.614908
Average KL loss: 0.200176
Average total loss: 0.815083
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.5943e-09, device='cuda:0')
Epoch 56
Average batch original loss after noise: 0.623438
Average KL loss: 0.200178
Average total loss: 0.823615
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-6.2367e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 0.618425
Average KL loss: 0.200180
Average total loss: 0.818604
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.9610e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 0.631059
Average KL loss: 0.200179
Average total loss: 0.831238
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.7464e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 0.623757
Average KL loss: 0.200186
Average total loss: 0.823942
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.9068e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 0.622920
Average KL loss: 0.200193
Average total loss: 0.823113
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-7.3170e-09, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.630908
Average KL loss: 0.200197
Average total loss: 0.831105
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-3.1631e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.645392
Average KL loss: 0.200205
Average total loss: 0.845597
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.0931e-09, device='cuda:0')
Epoch 63
Average batch original loss after noise: 0.622862
Average KL loss: 0.200216
Average total loss: 0.823077
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(1.5581e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.624335
Average KL loss: 0.200211
Average total loss: 0.824546
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-9.0359e-09, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.615675
Average KL loss: 0.200213
Average total loss: 0.815887
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(3.9098e-09, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.627665
Average KL loss: 0.200212
Average total loss: 0.827877
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-3.2615e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.637930
Average KL loss: 0.200212
Average total loss: 0.838143
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.8271e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.625293
Average KL loss: 0.200213
Average total loss: 0.825506
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.8519e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.622486
Average KL loss: 0.200213
Average total loss: 0.822699
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(2.2458e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.632160
Average KL loss: 0.200214
Average total loss: 0.832373
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-5.9630e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.645340
Average KL loss: 0.200215
Average total loss: 0.845555
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.6068e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.629545
Average KL loss: 0.200215
Average total loss: 0.829760
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.3881e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 0.636434
Average KL loss: 0.200216
Average total loss: 0.836650
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-8.1820e-09, device='cuda:0')
Epoch 74
Average batch original loss after noise: 0.621411
Average KL loss: 0.200216
Average total loss: 0.821627
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(1.7853e-08, device='cuda:0')
 Percentile value: 0.06281331181526184
Non-zero model percentage: 3.1250059604644775%, Non-zero mask percentage: 3.1250059604644775%

--- Pruning Level [5/12]: ---
conv1.weight         | nonzeros =     956 /    1728             ( 55.32%) | total_pruned =     772 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4454 /   36864             ( 12.08%) | total_pruned =   32410 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4174 /   36864             ( 11.32%) | total_pruned =   32690 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3555 /   36864             (  9.64%) | total_pruned =   33309 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    6958 /   73728             (  9.44%) | total_pruned =   66770 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   11546 /  147456             (  7.83%) | total_pruned =  135910 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      36 /     128             ( 28.12%) | total_pruned =      92 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2054 /    8192             ( 25.07%) | total_pruned =    6138 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      40 /     128             ( 31.25%) | total_pruned =      88 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    7324 /  147456             (  4.97%) | total_pruned =  140132 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    7140 /  147456             (  4.84%) | total_pruned =  140316 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      27 /     128             ( 21.09%) | total_pruned =     101 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   20843 /  294912             (  7.07%) | total_pruned =  274069 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   33845 /  589824             (  5.74%) | total_pruned =  555979 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      94 /     256             ( 36.72%) | total_pruned =     162 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5478 /   32768             ( 16.72%) | total_pruned =   27290 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   16417 /  589824             (  2.78%) | total_pruned =  573407 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   15693 /  589824             (  2.66%) | total_pruned =  574131 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      56 /     256             ( 21.88%) | total_pruned =     200 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   53434 / 1179648             (  4.53%) | total_pruned = 1126214 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     487 /     512             ( 95.12%) | total_pruned =      25 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     243 /     512             ( 47.46%) | total_pruned =     269 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   64707 / 2359296             (  2.74%) | total_pruned = 2294589 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     412 /     512             ( 80.47%) | total_pruned =     100 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    8396 /  131072             (  6.41%) | total_pruned =  122676 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     392 /     512             ( 76.56%) | total_pruned =     120 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   40804 / 2359296             (  1.73%) | total_pruned = 2318492 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     206 /     512             ( 40.23%) | total_pruned =     306 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   28070 / 2359296             (  1.19%) | total_pruned = 2331226 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     487 /     512             ( 95.12%) | total_pruned =      25 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     164 /     512             ( 32.03%) | total_pruned =     348 | shape = torch.Size([512])
linear.weight        | nonzeros =    2666 /    5120             ( 52.07%) | total_pruned =    2454 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 349337, pruned : 10829425, total: 11178762, Compression rate :      32.00x  ( 96.87% pruned)
Train Epoch: 30/100 Loss: 0.000024 Accuracy: 86.04 100.00 % Best test Accuracy: 86.24%
tensor(0.0029, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.7225e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 1.053437
Average KL loss: 0.195597
Average total loss: 1.249034
tensor(0.0032, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-3.0529e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 1.067235
Average KL loss: 0.193100
Average total loss: 1.260335
tensor(0.0033, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-3.4006e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 1.038206
Average KL loss: 0.192920
Average total loss: 1.231127
tensor(0.0033, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-5.9181e-09, device='cuda:0')
Epoch 4
Average batch original loss after noise: 1.028313
Average KL loss: 0.193343
Average total loss: 1.221655
tensor(0.0033, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-1.4525e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 1.010044
Average KL loss: 0.194006
Average total loss: 1.204049
tensor(0.0033, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-2.8892e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 0.973670
Average KL loss: 0.194730
Average total loss: 1.168400
tensor(0.0033, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(-7.9229e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 0.982522
Average KL loss: 0.195504
Average total loss: 1.178026
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.4277e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 0.992403
Average KL loss: 0.196299
Average total loss: 1.188702
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-3.3113e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 0.954554
Average KL loss: 0.197203
Average total loss: 1.151757
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.7537e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 0.985421
Average KL loss: 0.198133
Average total loss: 1.183553
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.6795e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 0.952801
Average KL loss: 0.199018
Average total loss: 1.151820
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-4.3885e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 0.984476
Average KL loss: 0.199894
Average total loss: 1.184370
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.5888e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 0.968298
Average KL loss: 0.200784
Average total loss: 1.169082
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-8.6083e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 0.966855
Average KL loss: 0.201779
Average total loss: 1.168634
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-2.6362e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 0.924172
Average KL loss: 0.202614
Average total loss: 1.126786
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-5.9908e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 0.917638
Average KL loss: 0.203349
Average total loss: 1.120986
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-7.1428e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 0.953230
Average KL loss: 0.204168
Average total loss: 1.157398
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-3.7887e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 0.932248
Average KL loss: 0.205017
Average total loss: 1.137265
tensor(0.0033, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(-1.1467e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 0.937752
Average KL loss: 0.205869
Average total loss: 1.143621
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(2.2900e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 0.912426
Average KL loss: 0.206744
Average total loss: 1.119170
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-1.5117e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 0.906447
Average KL loss: 0.207598
Average total loss: 1.114045
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.8847e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 0.903906
Average KL loss: 0.208395
Average total loss: 1.112302
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.6335e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 0.924073
Average KL loss: 0.209222
Average total loss: 1.133294
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.8663e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 0.895602
Average KL loss: 0.210063
Average total loss: 1.105664
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-4.6211e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 0.889825
Average KL loss: 0.210850
Average total loss: 1.100675
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.1473e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 0.869276
Average KL loss: 0.211545
Average total loss: 1.080820
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.3208e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 0.901414
Average KL loss: 0.212249
Average total loss: 1.113663
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-3.2666e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 0.936211
Average KL loss: 0.213004
Average total loss: 1.149215
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-9.0302e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 0.900625
Average KL loss: 0.213829
Average total loss: 1.114454
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-2.6072e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 0.893494
Average KL loss: 0.214625
Average total loss: 1.108119
tensor(0.0033, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(-8.5437e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 0.867943
Average KL loss: 0.215299
Average total loss: 1.083242
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-5.4779e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 0.917815
Average KL loss: 0.216068
Average total loss: 1.133883
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.7457e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 0.868606
Average KL loss: 0.216905
Average total loss: 1.085511
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.4147e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 0.866664
Average KL loss: 0.217488
Average total loss: 1.084153
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(3.2707e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 0.869077
Average KL loss: 0.218119
Average total loss: 1.087196
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-5.8861e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 0.871098
Average KL loss: 0.218789
Average total loss: 1.089886
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.2903e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 0.904947
Average KL loss: 0.219429
Average total loss: 1.124376
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.2930e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 0.857125
Average KL loss: 0.219843
Average total loss: 1.076968
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.0612e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 0.894065
Average KL loss: 0.219893
Average total loss: 1.113958
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.9081e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 0.874628
Average KL loss: 0.219950
Average total loss: 1.094578
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.2788e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 0.880504
Average KL loss: 0.219997
Average total loss: 1.100502
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(2.2078e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 0.884088
Average KL loss: 0.220046
Average total loss: 1.104134
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.6925e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 0.846187
Average KL loss: 0.220092
Average total loss: 1.066279
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-9.3218e-09, device='cuda:0')
Epoch 44
Average batch original loss after noise: 0.836296
Average KL loss: 0.220138
Average total loss: 1.056435
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.7243e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 0.850355
Average KL loss: 0.220182
Average total loss: 1.070537
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.1881e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 0.900950
Average KL loss: 0.220231
Average total loss: 1.121182
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-8.9146e-09, device='cuda:0')
Epoch 47
Average batch original loss after noise: 0.855082
Average KL loss: 0.220289
Average total loss: 1.075371
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.0532e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 0.898923
Average KL loss: 0.220347
Average total loss: 1.119270
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.1514e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 0.905627
Average KL loss: 0.220412
Average total loss: 1.126039
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-4.7239e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 0.883333
Average KL loss: 0.220478
Average total loss: 1.103811
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-5.5145e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 0.834765
Average KL loss: 0.220533
Average total loss: 1.055298
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(2.1578e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 0.875604
Average KL loss: 0.220575
Average total loss: 1.096180
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.9450e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 0.890382
Average KL loss: 0.220638
Average total loss: 1.111020
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-4.2419e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 0.884470
Average KL loss: 0.220698
Average total loss: 1.105168
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.3048e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 0.860277
Average KL loss: 0.220757
Average total loss: 1.081034
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.2328e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 0.891263
Average KL loss: 0.220816
Average total loss: 1.112079
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.2479e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 0.852904
Average KL loss: 0.220877
Average total loss: 1.073782
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.6077e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 0.880999
Average KL loss: 0.220931
Average total loss: 1.101930
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-3.0361e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 0.859224
Average KL loss: 0.220978
Average total loss: 1.080202
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.4435e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 0.856321
Average KL loss: 0.221025
Average total loss: 1.077346
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(7.3854e-09, device='cuda:0')
Epoch 61
Average batch original loss after noise: 0.866851
Average KL loss: 0.221078
Average total loss: 1.087929
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.5300e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 0.866115
Average KL loss: 0.221128
Average total loss: 1.087243
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.5033e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 0.872101
Average KL loss: 0.221160
Average total loss: 1.093260
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(4.4620e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 0.868058
Average KL loss: 0.221165
Average total loss: 1.089222
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.1192e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 0.862120
Average KL loss: 0.221170
Average total loss: 1.083289
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-3.6517e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 0.909419
Average KL loss: 0.221176
Average total loss: 1.130595
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.2619e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 0.871049
Average KL loss: 0.221182
Average total loss: 1.092231
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.3432e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 0.871709
Average KL loss: 0.221186
Average total loss: 1.092895
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.5797e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 0.844395
Average KL loss: 0.221191
Average total loss: 1.065586
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-4.2170e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 0.884207
Average KL loss: 0.221196
Average total loss: 1.105402
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(1.4806e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 0.881617
Average KL loss: 0.221201
Average total loss: 1.102818
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-7.8576e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 0.882434
Average KL loss: 0.221207
Average total loss: 1.103641
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.0953e-08, device='cuda:0')
 Percentile value: 0.13474510610103607
Non-zero model percentage: 1.5625073909759521%, Non-zero mask percentage: 1.5625073909759521%

--- Pruning Level [6/12]: ---
conv1.weight         | nonzeros =     912 /    1728             ( 52.78%) | total_pruned =     816 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3086 /   36864             (  8.37%) | total_pruned =   33778 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3251 /   36864             (  8.82%) | total_pruned =   33613 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2961 /   36864             (  8.03%) | total_pruned =   33903 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2499 /   36864             (  6.78%) | total_pruned =   34365 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    4839 /   73728             (  6.56%) | total_pruned =   68889 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    7510 /  147456             (  5.09%) | total_pruned =  139946 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1669 /    8192             ( 20.37%) | total_pruned =    6523 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4125 /  147456             (  2.80%) | total_pruned =  143331 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3917 /  147456             (  2.66%) | total_pruned =  143539 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      19 /     128             ( 14.84%) | total_pruned =     109 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   12855 /  294912             (  4.36%) | total_pruned =  282057 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   19071 /  589824             (  3.23%) | total_pruned =  570753 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      77 /     256             ( 30.08%) | total_pruned =     179 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4022 /   32768             ( 12.27%) | total_pruned =   28746 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      82 /     256             ( 32.03%) | total_pruned =     174 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    7111 /  589824             (  1.21%) | total_pruned =  582713 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     197 /     256             ( 76.95%) | total_pruned =      59 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    6730 /  589824             (  1.14%) | total_pruned =  583094 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      36 /     256             ( 14.06%) | total_pruned =     220 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   26559 / 1179648             (  2.25%) | total_pruned = 1153089 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     487 /     512             ( 95.12%) | total_pruned =      25 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     203 /     512             ( 39.65%) | total_pruned =     309 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   26403 / 2359296             (  1.12%) | total_pruned = 2332893 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4863 /  131072             (  3.71%) | total_pruned =  126209 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     374 /     512             ( 73.05%) | total_pruned =     138 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   15127 / 2359296             (  0.64%) | total_pruned = 2344169 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     377 /     512             ( 73.63%) | total_pruned =     135 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     171 /     512             ( 33.40%) | total_pruned =     341 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    8635 / 2359296             (  0.37%) | total_pruned = 2350661 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     118 /     512             ( 23.05%) | total_pruned =     394 | shape = torch.Size([512])
linear.weight        | nonzeros =    2279 /    5120             ( 44.51%) | total_pruned =    2841 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 174669, pruned : 11004093, total: 11178762, Compression rate :      64.00x  ( 98.44% pruned)
Train Epoch: 27/100 Loss: 0.001982 Accuracy: 85.17 100.00 % Best test Accuracy: 85.27%
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.0802e-07, device='cuda:0')
Epoch 1
Average batch original loss after noise: 1.762216
Average KL loss: 0.217598
Average total loss: 1.979814
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.0036e-07, device='cuda:0')
Epoch 2
Average batch original loss after noise: 1.715964
Average KL loss: 0.216059
Average total loss: 1.932022
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-3.7998e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 1.718795
Average KL loss: 0.216228
Average total loss: 1.935023
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-5.0573e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 1.657328
Average KL loss: 0.216809
Average total loss: 1.874137
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-2.2732e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 1.669823
Average KL loss: 0.217573
Average total loss: 1.887396
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-8.5158e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 1.597019
Average KL loss: 0.218449
Average total loss: 1.815468
tensor(0.0032, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-3.9498e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 1.620837
Average KL loss: 0.219366
Average total loss: 1.840202
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-1.0330e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 1.573305
Average KL loss: 0.220302
Average total loss: 1.793607
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-9.5388e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 1.590403
Average KL loss: 0.221257
Average total loss: 1.811660
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-3.0594e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 1.571586
Average KL loss: 0.222248
Average total loss: 1.793834
tensor(0.0033, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(-4.8623e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 1.571788
Average KL loss: 0.223336
Average total loss: 1.795124
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-1.0709e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 1.598463
Average KL loss: 0.224415
Average total loss: 1.822879
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-5.9080e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 1.601757
Average KL loss: 0.225620
Average total loss: 1.827377
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-6.2816e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 1.631723
Average KL loss: 0.226893
Average total loss: 1.858616
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-5.4757e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 1.509830
Average KL loss: 0.228125
Average total loss: 1.737954
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-5.3932e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 1.514442
Average KL loss: 0.229201
Average total loss: 1.743642
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-8.2355e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 1.568275
Average KL loss: 0.230299
Average total loss: 1.798575
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-3.0491e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 1.480637
Average KL loss: 0.231405
Average total loss: 1.712042
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-3.5058e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 1.483821
Average KL loss: 0.232488
Average total loss: 1.716309
tensor(0.0033, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(-3.6839e-08, device='cuda:0')
Epoch 20
Average batch original loss after noise: 1.503410
Average KL loss: 0.233628
Average total loss: 1.737038
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-9.4768e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 1.428979
Average KL loss: 0.234725
Average total loss: 1.663704
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-3.4057e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 1.513422
Average KL loss: 0.235801
Average total loss: 1.749223
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-1.6960e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 1.452577
Average KL loss: 0.236985
Average total loss: 1.689561
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-2.4634e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 1.437083
Average KL loss: 0.238036
Average total loss: 1.675119
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-6.7665e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 1.456390
Average KL loss: 0.239236
Average total loss: 1.695626
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-1.1778e-07, device='cuda:0')
Epoch 26
Average batch original loss after noise: 1.410366
Average KL loss: 0.240272
Average total loss: 1.650637
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-1.5098e-07, device='cuda:0')
Epoch 27
Average batch original loss after noise: 1.457150
Average KL loss: 0.241303
Average total loss: 1.698453
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-1.0010e-07, device='cuda:0')
Epoch 28
Average batch original loss after noise: 1.438417
Average KL loss: 0.242489
Average total loss: 1.680905
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-4.0195e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 1.423294
Average KL loss: 0.243639
Average total loss: 1.666933
tensor(0.0033, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(-6.3395e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 1.426397
Average KL loss: 0.244665
Average total loss: 1.671062
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.3124e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 1.447218
Average KL loss: 0.245763
Average total loss: 1.692981
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-1.8893e-07, device='cuda:0')
Epoch 32
Average batch original loss after noise: 1.460969
Average KL loss: 0.246982
Average total loss: 1.707951
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.5173e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 1.357720
Average KL loss: 0.248117
Average total loss: 1.605837
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.9340e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 1.367928
Average KL loss: 0.249055
Average total loss: 1.616983
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-9.2190e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 1.363208
Average KL loss: 0.249983
Average total loss: 1.613191
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.1129e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 1.296871
Average KL loss: 0.250850
Average total loss: 1.547722
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-5.4303e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 1.369476
Average KL loss: 0.251693
Average total loss: 1.621169
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-1.9372e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 1.367065
Average KL loss: 0.252677
Average total loss: 1.619742
tensor(0.0033, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.5344e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 1.368762
Average KL loss: 0.253589
Average total loss: 1.622350
tensor(0.0034, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-9.8906e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 1.403418
Average KL loss: 0.254592
Average total loss: 1.658010
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3876e-07, device='cuda:0')
Epoch 41
Average batch original loss after noise: 1.347426
Average KL loss: 0.255630
Average total loss: 1.603056
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.1502e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 1.374690
Average KL loss: 0.256620
Average total loss: 1.631310
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.1023e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 1.381024
Average KL loss: 0.257627
Average total loss: 1.638652
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.0574e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 1.359268
Average KL loss: 0.258657
Average total loss: 1.617925
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(1.4035e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 1.358454
Average KL loss: 0.259593
Average total loss: 1.618046
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.5088e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 1.328409
Average KL loss: 0.260538
Average total loss: 1.588947
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.8183e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 1.336496
Average KL loss: 0.261541
Average total loss: 1.598036
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-7.9006e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 1.392457
Average KL loss: 0.262063
Average total loss: 1.654520
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.2892e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 1.324276
Average KL loss: 0.262163
Average total loss: 1.586439
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.0966e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 1.320495
Average KL loss: 0.262249
Average total loss: 1.582744
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.4884e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 1.315756
Average KL loss: 0.262330
Average total loss: 1.578086
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.5200e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 1.332089
Average KL loss: 0.262416
Average total loss: 1.594504
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.9891e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 1.311245
Average KL loss: 0.262500
Average total loss: 1.573744
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-7.4903e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 1.306690
Average KL loss: 0.262581
Average total loss: 1.569271
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.5142e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 1.281687
Average KL loss: 0.262657
Average total loss: 1.544345
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(9.0808e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 1.296219
Average KL loss: 0.262739
Average total loss: 1.558958
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-4.6774e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 1.326446
Average KL loss: 0.262825
Average total loss: 1.589271
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.2983e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 1.338858
Average KL loss: 0.262913
Average total loss: 1.601771
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3193e-07, device='cuda:0')
Epoch 59
Average batch original loss after noise: 1.346371
Average KL loss: 0.263000
Average total loss: 1.609371
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.8085e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 1.309590
Average KL loss: 0.263086
Average total loss: 1.572676
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.8751e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 1.312348
Average KL loss: 0.263170
Average total loss: 1.575518
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.5161e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 1.295693
Average KL loss: 0.263256
Average total loss: 1.558949
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(1.0217e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 1.362533
Average KL loss: 0.263339
Average total loss: 1.625871
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.2556e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 1.268461
Average KL loss: 0.263433
Average total loss: 1.531894
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.1984e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 1.246463
Average KL loss: 0.263508
Average total loss: 1.509971
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-7.1014e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 1.327290
Average KL loss: 0.263583
Average total loss: 1.590873
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3125e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 1.312903
Average KL loss: 0.263677
Average total loss: 1.576580
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.0264e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 1.350007
Average KL loss: 0.263767
Average total loss: 1.613774
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3783e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 1.344080
Average KL loss: 0.263870
Average total loss: 1.607950
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.5059e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 1.332924
Average KL loss: 0.263964
Average total loss: 1.596887
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.7054e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 1.362255
Average KL loss: 0.264056
Average total loss: 1.626311
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(1.9264e-09, device='cuda:0')
Epoch 72
Average batch original loss after noise: 1.329118
Average KL loss: 0.264152
Average total loss: 1.593270
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.0841e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 1.340189
Average KL loss: 0.264255
Average total loss: 1.604444
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.9449e-08, device='cuda:0')
Epoch 74
Average batch original loss after noise: 1.295121
Average KL loss: 0.264345
Average total loss: 1.559466
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.4032e-07, device='cuda:0')
Epoch 75
Average batch original loss after noise: 1.292702
Average KL loss: 0.264438
Average total loss: 1.557141
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.8654e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 1.324651
Average KL loss: 0.264526
Average total loss: 1.589177
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.4799e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 1.302004
Average KL loss: 0.264582
Average total loss: 1.566585
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(1.9765e-08, device='cuda:0')
Epoch 78
Average batch original loss after noise: 1.319070
Average KL loss: 0.264590
Average total loss: 1.583659
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3144e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 1.343541
Average KL loss: 0.264598
Average total loss: 1.608139
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.7648e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 1.287455
Average KL loss: 0.264607
Average total loss: 1.552062
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.8926e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 1.323141
Average KL loss: 0.264615
Average total loss: 1.587756
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.3167e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 1.347562
Average KL loss: 0.264624
Average total loss: 1.612187
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.2610e-08, device='cuda:0')
Epoch 83
Average batch original loss after noise: 1.336760
Average KL loss: 0.264634
Average total loss: 1.601394
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-5.2149e-08, device='cuda:0')
Epoch 84
Average batch original loss after noise: 1.352610
Average KL loss: 0.264642
Average total loss: 1.617252
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.5380e-08, device='cuda:0')
Epoch 85
Average batch original loss after noise: 1.330557
Average KL loss: 0.264651
Average total loss: 1.595208
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.0006e-07, device='cuda:0')
Epoch 86
Average batch original loss after noise: 1.335945
Average KL loss: 0.264659
Average total loss: 1.600604
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-5.3870e-09, device='cuda:0')
 Percentile value: 0.2732601761817932
Non-zero model percentage: 0.781258225440979%, Non-zero mask percentage: 0.781258225440979%

--- Pruning Level [7/12]: ---
conv1.weight         | nonzeros =     834 /    1728             ( 48.26%) | total_pruned =     894 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1982 /   36864             (  5.38%) | total_pruned =   34882 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2109 /   36864             (  5.72%) | total_pruned =   34755 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1930 /   36864             (  5.24%) | total_pruned =   34934 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1637 /   36864             (  4.44%) | total_pruned =   35227 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    3040 /   73728             (  4.12%) | total_pruned =   70688 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      27 /     128             ( 21.09%) | total_pruned =     101 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4441 /  147456             (  3.01%) | total_pruned =  143015 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1249 /    8192             ( 15.25%) | total_pruned =    6943 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      27 /     128             ( 21.09%) | total_pruned =     101 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2176 /  147456             (  1.48%) | total_pruned =  145280 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      34 /     128             ( 26.56%) | total_pruned =      94 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2081 /  147456             (  1.41%) | total_pruned =  145375 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     124 /     128             ( 96.88%) | total_pruned =       4 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      19 /     128             ( 14.84%) | total_pruned =     109 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    7134 /  294912             (  2.42%) | total_pruned =  287778 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      87 /     256             ( 33.98%) | total_pruned =     169 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    9639 /  589824             (  1.63%) | total_pruned =  580185 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      49 /     256             ( 19.14%) | total_pruned =     207 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2515 /   32768             (  7.68%) | total_pruned =   30253 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     243 /     256             ( 94.92%) | total_pruned =      13 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      55 /     256             ( 21.48%) | total_pruned =     201 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2793 /  589824             (  0.47%) | total_pruned =  587031 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     195 /     256             ( 76.17%) | total_pruned =      61 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      34 /     256             ( 13.28%) | total_pruned =     222 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    2750 /  589824             (  0.47%) | total_pruned =  587074 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     229 /     256             ( 89.45%) | total_pruned =      27 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      23 /     256             (  8.98%) | total_pruned =     233 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   11808 / 1179648             (  1.00%) | total_pruned = 1167840 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     486 /     512             ( 94.92%) | total_pruned =      26 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     150 /     512             ( 29.30%) | total_pruned =     362 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   10768 / 2359296             (  0.46%) | total_pruned = 2348528 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     334 /     512             ( 65.23%) | total_pruned =     178 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2366 /  131072             (  1.81%) | total_pruned =  128706 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     375 /     512             ( 73.24%) | total_pruned =     137 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    5912 / 2359296             (  0.25%) | total_pruned = 2353384 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     109 /     512             ( 21.29%) | total_pruned =     403 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    2845 / 2359296             (  0.12%) | total_pruned = 2356451 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     439 /     512             ( 85.74%) | total_pruned =      73 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =      59 /     512             ( 11.52%) | total_pruned =     453 | shape = torch.Size([512])
linear.weight        | nonzeros =    1618 /    5120             ( 31.60%) | total_pruned =    3502 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 87335, pruned : 11091427, total: 11178762, Compression rate :     128.00x  ( 99.22% pruned)
Train Epoch: 36/100 Loss: 0.014200 Accuracy: 82.70 99.96 % Best test Accuracy: 83.55%
tensor(0.0034, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(5.5278e-09, device='cuda:0')
Epoch 1
Average batch original loss after noise: 3.241974
Average KL loss: 0.259970
Average total loss: 3.501944
tensor(0.0032, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.6473e-07, device='cuda:0')
Epoch 2
Average batch original loss after noise: 3.049479
Average KL loss: 0.255730
Average total loss: 3.305209
tensor(0.0031, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.6624e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 3.244992
Average KL loss: 0.254323
Average total loss: 3.499315
tensor(0.0031, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.7156e-09, device='cuda:0')
Epoch 4
Average batch original loss after noise: 3.046433
Average KL loss: 0.254076
Average total loss: 3.300509
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.9211e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 3.147024
Average KL loss: 0.254242
Average total loss: 3.401266
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.9565e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.940783
Average KL loss: 0.254637
Average total loss: 3.195420
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.9298e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.938501
Average KL loss: 0.255147
Average total loss: 3.193648
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-7.5915e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.967168
Average KL loss: 0.255754
Average total loss: 3.222923
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-2.3696e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.969223
Average KL loss: 0.256406
Average total loss: 3.225629
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-7.6338e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.765299
Average KL loss: 0.257059
Average total loss: 3.022358
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.3139e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.937746
Average KL loss: 0.257773
Average total loss: 3.195519
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-5.2511e-09, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.848994
Average KL loss: 0.258563
Average total loss: 3.107557
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(8.5128e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.890427
Average KL loss: 0.259347
Average total loss: 3.149774
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(3.9428e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.829581
Average KL loss: 0.260216
Average total loss: 3.089797
tensor(0.0030, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-6.2078e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.807630
Average KL loss: 0.261066
Average total loss: 3.068696
tensor(0.0030, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-7.1678e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.811372
Average KL loss: 0.261901
Average total loss: 3.073273
tensor(0.0030, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-4.7734e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.932376
Average KL loss: 0.262791
Average total loss: 3.195167
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.3177e-07, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.725108
Average KL loss: 0.263648
Average total loss: 2.988757
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.6881e-07, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.848677
Average KL loss: 0.264614
Average total loss: 3.113291
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.4338e-07, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.882913
Average KL loss: 0.265578
Average total loss: 3.148491
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.5165e-07, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.684560
Average KL loss: 0.266549
Average total loss: 2.951109
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(2.7368e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.708467
Average KL loss: 0.267464
Average total loss: 2.975931
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-8.9986e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.652174
Average KL loss: 0.268296
Average total loss: 2.920470
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-2.1909e-07, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.752794
Average KL loss: 0.269231
Average total loss: 3.022025
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-3.2136e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.647054
Average KL loss: 0.270190
Average total loss: 2.917244
tensor(0.0031, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-4.6495e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.562366
Average KL loss: 0.271136
Average total loss: 2.833502
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.0035e-07, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.455405
Average KL loss: 0.272047
Average total loss: 2.727452
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-4.7305e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.502665
Average KL loss: 0.272912
Average total loss: 2.775577
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-5.2412e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.640893
Average KL loss: 0.273861
Average total loss: 2.914755
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.1862e-07, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.516035
Average KL loss: 0.274872
Average total loss: 2.790907
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-6.8789e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.467954
Average KL loss: 0.275801
Average total loss: 2.743755
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.1371e-07, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.404580
Average KL loss: 0.276750
Average total loss: 2.681330
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-3.9720e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.455550
Average KL loss: 0.277599
Average total loss: 2.733150
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.7630e-07, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.615240
Average KL loss: 0.278631
Average total loss: 2.893872
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.5921e-07, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.543103
Average KL loss: 0.279719
Average total loss: 2.822822
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(1.8327e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.431646
Average KL loss: 0.280705
Average total loss: 2.712351
tensor(0.0031, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(3.5970e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.397628
Average KL loss: 0.281593
Average total loss: 2.679221
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-7.9570e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.471541
Average KL loss: 0.282564
Average total loss: 2.754105
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.6928e-07, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.499888
Average KL loss: 0.283573
Average total loss: 2.783460
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.0767e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.398993
Average KL loss: 0.284596
Average total loss: 2.683590
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.2481e-07, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.421703
Average KL loss: 0.285563
Average total loss: 2.707266
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-8.5641e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.384838
Average KL loss: 0.286591
Average total loss: 2.671429
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.0717e-07, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.326772
Average KL loss: 0.287457
Average total loss: 2.614230
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-6.4105e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.298633
Average KL loss: 0.288348
Average total loss: 2.586981
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-3.8598e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.272236
Average KL loss: 0.289227
Average total loss: 2.561463
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-6.8022e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.464866
Average KL loss: 0.290188
Average total loss: 2.755055
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-5.6564e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.353641
Average KL loss: 0.291195
Average total loss: 2.644836
tensor(0.0031, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.0625e-07, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.389384
Average KL loss: 0.292183
Average total loss: 2.681567
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.0893e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.220061
Average KL loss: 0.293122
Average total loss: 2.513183
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.2671e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.300948
Average KL loss: 0.294103
Average total loss: 2.595051
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.2957e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.332466
Average KL loss: 0.295101
Average total loss: 2.627566
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-3.4469e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.325640
Average KL loss: 0.296026
Average total loss: 2.621666
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.0685e-07, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.238898
Average KL loss: 0.296962
Average total loss: 2.535859
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.6080e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.337085
Average KL loss: 0.297898
Average total loss: 2.634983
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.1525e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.229929
Average KL loss: 0.298824
Average total loss: 2.528753
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.4735e-07, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.292142
Average KL loss: 0.299727
Average total loss: 2.591869
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.6331e-07, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.258284
Average KL loss: 0.300692
Average total loss: 2.558975
tensor(0.0031, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.2552e-07, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.341587
Average KL loss: 0.301691
Average total loss: 2.643278
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.6442e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.217907
Average KL loss: 0.302649
Average total loss: 2.520556
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.7397e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.214563
Average KL loss: 0.303590
Average total loss: 2.518152
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.8482e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.228533
Average KL loss: 0.304104
Average total loss: 2.532637
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.1916e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.181445
Average KL loss: 0.304203
Average total loss: 2.485649
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.8153e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.290643
Average KL loss: 0.304297
Average total loss: 2.594941
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4484e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.166434
Average KL loss: 0.304388
Average total loss: 2.470822
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.8837e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.228192
Average KL loss: 0.304464
Average total loss: 2.532656
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.3036e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.235314
Average KL loss: 0.304553
Average total loss: 2.539867
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(2.9922e-09, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.294411
Average KL loss: 0.304646
Average total loss: 2.599057
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.0259e-07, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.241745
Average KL loss: 0.304746
Average total loss: 2.546491
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.7927e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.173772
Average KL loss: 0.304833
Average total loss: 2.478605
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0118e-07, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.363273
Average KL loss: 0.304930
Average total loss: 2.668203
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-8.4236e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.320569
Average KL loss: 0.305040
Average total loss: 2.625609
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.4810e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.156215
Average KL loss: 0.305138
Average total loss: 2.461353
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(2.1817e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.269056
Average KL loss: 0.305228
Average total loss: 2.574284
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.3132e-07, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.133095
Average KL loss: 0.305319
Average total loss: 2.438414
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.7658e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.174924
Average KL loss: 0.305410
Average total loss: 2.480334
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.0794e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.126908
Average KL loss: 0.305496
Average total loss: 2.432404
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.6348e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.185643
Average KL loss: 0.305581
Average total loss: 2.491224
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0334e-07, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.178868
Average KL loss: 0.305668
Average total loss: 2.484536
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.5724e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.364919
Average KL loss: 0.305760
Average total loss: 2.670679
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.0396e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.155543
Average KL loss: 0.305863
Average total loss: 2.461406
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.2382e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.203419
Average KL loss: 0.305956
Average total loss: 2.509375
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0913e-09, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.292007
Average KL loss: 0.306060
Average total loss: 2.598068
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4265e-07, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.198416
Average KL loss: 0.306163
Average total loss: 2.504579
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.5416e-08, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.243412
Average KL loss: 0.306253
Average total loss: 2.549665
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.6648e-08, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.172616
Average KL loss: 0.306349
Average total loss: 2.478965
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.5318e-08, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.297680
Average KL loss: 0.306448
Average total loss: 2.604128
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0747e-07, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.267692
Average KL loss: 0.306555
Average total loss: 2.574247
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.7530e-08, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.138366
Average KL loss: 0.306613
Average total loss: 2.444979
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.2204e-07, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.180712
Average KL loss: 0.306622
Average total loss: 2.487334
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4367e-08, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.319409
Average KL loss: 0.306631
Average total loss: 2.626040
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.7063e-08, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.262159
Average KL loss: 0.306642
Average total loss: 2.568801
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.0658e-08, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.257392
Average KL loss: 0.306652
Average total loss: 2.564045
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.1204e-08, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.325460
Average KL loss: 0.306663
Average total loss: 2.632123
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.3122e-08, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.173290
Average KL loss: 0.306672
Average total loss: 2.479962
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0048e-07, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.219483
Average KL loss: 0.306681
Average total loss: 2.526164
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.2649e-08, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.124243
Average KL loss: 0.306692
Average total loss: 2.430935
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0297e-08, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.228647
Average KL loss: 0.306700
Average total loss: 2.535347
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.4522e-09, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.082128
Average KL loss: 0.306709
Average total loss: 2.388837
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.8264e-08, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.210014
Average KL loss: 0.306717
Average total loss: 2.516732
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.8145e-08, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.250977
Average KL loss: 0.306727
Average total loss: 2.557703
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4312e-07, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.293776
Average KL loss: 0.306737
Average total loss: 2.600513
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.4469e-08, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.267890
Average KL loss: 0.306747
Average total loss: 2.574637
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0771e-07, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.131040
Average KL loss: 0.306757
Average total loss: 2.437797
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.4842e-08, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.198958
Average KL loss: 0.306767
Average total loss: 2.505725
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(9.7860e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.135268
Average KL loss: 0.306776
Average total loss: 2.442044
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4560e-07, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.236069
Average KL loss: 0.306785
Average total loss: 2.542854
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.2200e-08, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.156678
Average KL loss: 0.306795
Average total loss: 2.463473
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.6888e-07, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.352847
Average KL loss: 0.306804
Average total loss: 2.659651
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.9659e-08, device='cuda:0')
 Percentile value: 0.5040301978588104
Non-zero model percentage: 0.39063358306884766%, Non-zero mask percentage: 0.39063358306884766%

--- Pruning Level [8/12]: ---
conv1.weight         | nonzeros =     732 /    1728             ( 42.36%) | total_pruned =     996 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1126 /   36864             (  3.05%) | total_pruned =   35738 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1271 /   36864             (  3.45%) | total_pruned =   35593 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1172 /   36864             (  3.18%) | total_pruned =   35692 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1007 /   36864             (  2.73%) | total_pruned =   35857 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    1813 /   73728             (  2.46%) | total_pruned =   71915 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      17 /     128             ( 13.28%) | total_pruned =     111 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    2451 /  147456             (  1.66%) | total_pruned =  145005 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      14 /     128             ( 10.94%) | total_pruned =     114 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     863 /    8192             ( 10.53%) | total_pruned =    7329 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      14 /     128             ( 10.94%) | total_pruned =     114 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1076 /  147456             (  0.73%) | total_pruned =  146380 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1041 /  147456             (  0.71%) | total_pruned =  146415 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     120 /     128             ( 93.75%) | total_pruned =       8 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      13 /     128             ( 10.16%) | total_pruned =     115 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    3648 /  294912             (  1.24%) | total_pruned =  291264 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      65 /     256             ( 25.39%) | total_pruned =     191 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    4702 /  589824             (  0.80%) | total_pruned =  585122 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      27 /     256             ( 10.55%) | total_pruned =     229 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1377 /   32768             (  4.20%) | total_pruned =   31391 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     231 /     256             ( 90.23%) | total_pruned =      25 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      30 /     256             ( 11.72%) | total_pruned =     226 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    1034 /  589824             (  0.18%) | total_pruned =  588790 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     189 /     256             ( 73.83%) | total_pruned =      67 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      19 /     256             (  7.42%) | total_pruned =     237 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1077 /  589824             (  0.18%) | total_pruned =  588747 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     213 /     256             ( 83.20%) | total_pruned =      43 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      16 /     256             (  6.25%) | total_pruned =     240 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    5153 / 1179648             (  0.44%) | total_pruned = 1174495 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     484 /     512             ( 94.53%) | total_pruned =      28 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =      85 /     512             ( 16.60%) | total_pruned =     427 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    4337 / 2359296             (  0.18%) | total_pruned = 2354959 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     454 /     512             ( 88.67%) | total_pruned =      58 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     263 /     512             ( 51.37%) | total_pruned =     249 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1011 /  131072             (  0.77%) | total_pruned =  130061 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     329 /     512             ( 64.26%) | total_pruned =     183 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    2190 / 2359296             (  0.09%) | total_pruned = 2357106 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     329 /     512             ( 64.26%) | total_pruned =     183 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      61 /     512             ( 11.91%) | total_pruned =     451 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     727 / 2359296             (  0.03%) | total_pruned = 2358569 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     336 /     512             ( 65.62%) | total_pruned =     176 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =      12 /     512             (  2.34%) | total_pruned =     500 | shape = torch.Size([512])
linear.weight        | nonzeros =     845 /    5120             ( 16.50%) | total_pruned =    4275 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 43668, pruned : 11135094, total: 11178762, Compression rate :     255.99x  ( 99.61% pruned)
Train Epoch: 99/100 Loss: 0.002583 Accuracy: 79.11 100.00 % Best test Accuracy: 80.85%
tensor(0.0031, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4941e-07, device='cuda:0')
Epoch 1
Average batch original loss after noise: 6.137865
Average KL loss: 0.301686
Average total loss: 6.439551
tensor(0.0030, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-4.7087e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 6.303860
Average KL loss: 0.294943
Average total loss: 6.598804
tensor(0.0029, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(1.0252e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 6.213985
Average KL loss: 0.290564
Average total loss: 6.504550
tensor(0.0028, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(5.2679e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 5.892075
Average KL loss: 0.287723
Average total loss: 6.179798
tensor(0.0028, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(5.2338e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 6.249962
Average KL loss: 0.285950
Average total loss: 6.535912
tensor(0.0027, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-3.8817e-07, device='cuda:0')
Epoch 6
Average batch original loss after noise: 6.129360
Average KL loss: 0.284850
Average total loss: 6.414210
tensor(0.0027, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.1601e-07, device='cuda:0')
Epoch 7
Average batch original loss after noise: 5.947694
Average KL loss: 0.284172
Average total loss: 6.231865
tensor(0.0027, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-7.1409e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 5.834633
Average KL loss: 0.283777
Average total loss: 6.118410
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-8.2028e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 5.724741
Average KL loss: 0.283567
Average total loss: 6.008308
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-3.1071e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 5.826248
Average KL loss: 0.283498
Average total loss: 6.109746
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(5.6002e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 5.955502
Average KL loss: 0.283513
Average total loss: 6.239015
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-6.2862e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 5.682392
Average KL loss: 0.283609
Average total loss: 5.966001
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.0622e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 5.763136
Average KL loss: 0.283766
Average total loss: 6.046902
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.8676e-07, device='cuda:0')
Epoch 14
Average batch original loss after noise: 5.423126
Average KL loss: 0.283957
Average total loss: 5.707083
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.9753e-07, device='cuda:0')
Epoch 15
Average batch original loss after noise: 5.337996
Average KL loss: 0.284131
Average total loss: 5.622128
tensor(0.0026, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.7747e-07, device='cuda:0')
Epoch 16
Average batch original loss after noise: 5.470418
Average KL loss: 0.284353
Average total loss: 5.754771
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.0589e-07, device='cuda:0')
Epoch 17
Average batch original loss after noise: 5.447426
Average KL loss: 0.284596
Average total loss: 5.732021
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-9.5597e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 5.500679
Average KL loss: 0.284864
Average total loss: 5.785542
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-4.1733e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 5.574262
Average KL loss: 0.285162
Average total loss: 5.859424
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(1.5995e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 5.568126
Average KL loss: 0.285444
Average total loss: 5.853570
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.5108e-07, device='cuda:0')
Epoch 21
Average batch original loss after noise: 5.404256
Average KL loss: 0.285788
Average total loss: 5.690043
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.1935e-07, device='cuda:0')
Epoch 22
Average batch original loss after noise: 5.250216
Average KL loss: 0.286128
Average total loss: 5.536345
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.3256e-07, device='cuda:0')
Epoch 23
Average batch original loss after noise: 4.997370
Average KL loss: 0.286463
Average total loss: 5.283833
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.8122e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 5.153866
Average KL loss: 0.286824
Average total loss: 5.440689
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-3.5520e-07, device='cuda:0')
Epoch 25
Average batch original loss after noise: 5.079502
Average KL loss: 0.287240
Average total loss: 5.366743
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(2.3816e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 5.284095
Average KL loss: 0.287641
Average total loss: 5.571736
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-6.8061e-07, device='cuda:0')
Epoch 27
Average batch original loss after noise: 4.996827
Average KL loss: 0.288056
Average total loss: 5.284883
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(7.6067e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 5.183880
Average KL loss: 0.288510
Average total loss: 5.472389
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-6.9579e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 4.996071
Average KL loss: 0.288973
Average total loss: 5.285044
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.4287e-07, device='cuda:0')
Epoch 30
Average batch original loss after noise: 5.198952
Average KL loss: 0.289478
Average total loss: 5.488430
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.4922e-07, device='cuda:0')
Epoch 31
Average batch original loss after noise: 5.124638
Average KL loss: 0.289961
Average total loss: 5.414599
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-3.1212e-07, device='cuda:0')
Epoch 32
Average batch original loss after noise: 5.251655
Average KL loss: 0.290406
Average total loss: 5.542061
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.8983e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 4.820555
Average KL loss: 0.290878
Average total loss: 5.111433
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-3.4837e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 4.896704
Average KL loss: 0.291334
Average total loss: 5.188038
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.0507e-07, device='cuda:0')
Epoch 35
Average batch original loss after noise: 4.787379
Average KL loss: 0.291811
Average total loss: 5.079190
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.7022e-07, device='cuda:0')
Epoch 36
Average batch original loss after noise: 4.960757
Average KL loss: 0.292272
Average total loss: 5.253029
tensor(0.0026, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.5662e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 4.846344
Average KL loss: 0.292781
Average total loss: 5.139125
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.9812e-07, device='cuda:0')
Epoch 38
Average batch original loss after noise: 4.706414
Average KL loss: 0.293256
Average total loss: 4.999670
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0914e-07, device='cuda:0')
Epoch 39
Average batch original loss after noise: 4.979552
Average KL loss: 0.293746
Average total loss: 5.273298
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-8.8822e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 4.954085
Average KL loss: 0.294231
Average total loss: 5.248317
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.0885e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 4.941432
Average KL loss: 0.294755
Average total loss: 5.236187
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(8.8034e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 5.072208
Average KL loss: 0.295303
Average total loss: 5.367511
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.3196e-07, device='cuda:0')
Epoch 43
Average batch original loss after noise: 4.714564
Average KL loss: 0.295865
Average total loss: 5.010429
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(3.3895e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 4.939719
Average KL loss: 0.296410
Average total loss: 5.236129
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4253e-07, device='cuda:0')
Epoch 45
Average batch original loss after noise: 4.811810
Average KL loss: 0.296945
Average total loss: 5.108755
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.6800e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 5.072861
Average KL loss: 0.297490
Average total loss: 5.370352
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.6217e-07, device='cuda:0')
Epoch 47
Average batch original loss after noise: 4.903319
Average KL loss: 0.298125
Average total loss: 5.201444
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.1201e-07, device='cuda:0')
Epoch 48
Average batch original loss after noise: 4.766706
Average KL loss: 0.298706
Average total loss: 5.065412
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0973e-07, device='cuda:0')
Epoch 49
Average batch original loss after noise: 4.720625
Average KL loss: 0.299259
Average total loss: 5.019884
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.4118e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 4.577303
Average KL loss: 0.299576
Average total loss: 4.876880
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.0826e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 4.593688
Average KL loss: 0.299632
Average total loss: 4.893321
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.0155e-07, device='cuda:0')
Epoch 52
Average batch original loss after noise: 4.705500
Average KL loss: 0.299688
Average total loss: 5.005188
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(3.8537e-09, device='cuda:0')
Epoch 53
Average batch original loss after noise: 4.712267
Average KL loss: 0.299743
Average total loss: 5.012010
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.1872e-07, device='cuda:0')
Epoch 54
Average batch original loss after noise: 4.810219
Average KL loss: 0.299802
Average total loss: 5.110021
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.5063e-07, device='cuda:0')
Epoch 55
Average batch original loss after noise: 4.889819
Average KL loss: 0.299861
Average total loss: 5.189680
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.2601e-07, device='cuda:0')
Epoch 56
Average batch original loss after noise: 4.626118
Average KL loss: 0.299920
Average total loss: 4.926038
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.9465e-07, device='cuda:0')
Epoch 57
Average batch original loss after noise: 4.600006
Average KL loss: 0.299980
Average total loss: 4.899985
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.2016e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 4.588773
Average KL loss: 0.300034
Average total loss: 4.888807
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.3361e-07, device='cuda:0')
Epoch 59
Average batch original loss after noise: 4.612536
Average KL loss: 0.300091
Average total loss: 4.912626
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.3116e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 4.899164
Average KL loss: 0.300147
Average total loss: 5.199312
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.0200e-07, device='cuda:0')
Epoch 61
Average batch original loss after noise: 4.427939
Average KL loss: 0.300208
Average total loss: 4.728146
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.3684e-07, device='cuda:0')
Epoch 62
Average batch original loss after noise: 4.652279
Average KL loss: 0.300266
Average total loss: 4.952544
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(6.7420e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 4.729359
Average KL loss: 0.300321
Average total loss: 5.029680
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.8828e-07, device='cuda:0')
Epoch 64
Average batch original loss after noise: 4.769248
Average KL loss: 0.300377
Average total loss: 5.069624
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.4691e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 4.428914
Average KL loss: 0.300431
Average total loss: 4.729345
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(5.0490e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 4.459643
Average KL loss: 0.300484
Average total loss: 4.760127
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.6207e-07, device='cuda:0')
Epoch 67
Average batch original loss after noise: 4.868046
Average KL loss: 0.300540
Average total loss: 5.168586
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(8.4411e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 4.599091
Average KL loss: 0.300600
Average total loss: 4.899691
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.1856e-09, device='cuda:0')
Epoch 69
Average batch original loss after noise: 4.893648
Average KL loss: 0.300655
Average total loss: 5.194303
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.1569e-07, device='cuda:0')
Epoch 70
Average batch original loss after noise: 4.598191
Average KL loss: 0.300712
Average total loss: 4.898903
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(3.2254e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 4.725539
Average KL loss: 0.300771
Average total loss: 5.026311
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(5.4946e-11, device='cuda:0')
Epoch 72
Average batch original loss after noise: 4.694059
Average KL loss: 0.300831
Average total loss: 4.994890
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.6296e-07, device='cuda:0')
Epoch 73
Average batch original loss after noise: 4.670812
Average KL loss: 0.300864
Average total loss: 4.971675
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.0336e-07, device='cuda:0')
Epoch 74
Average batch original loss after noise: 4.412388
Average KL loss: 0.300869
Average total loss: 4.713257
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.5240e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 4.863599
Average KL loss: 0.300874
Average total loss: 5.164473
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.0480e-07, device='cuda:0')
Epoch 76
Average batch original loss after noise: 4.504670
Average KL loss: 0.300881
Average total loss: 4.805551
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.3254e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 4.685787
Average KL loss: 0.300887
Average total loss: 4.986674
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.6567e-07, device='cuda:0')
Epoch 78
Average batch original loss after noise: 4.649370
Average KL loss: 0.300892
Average total loss: 4.950262
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.9322e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 4.786779
Average KL loss: 0.300898
Average total loss: 5.087678
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.5600e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 4.570922
Average KL loss: 0.300905
Average total loss: 4.871827
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-6.0179e-09, device='cuda:0')
Epoch 81
Average batch original loss after noise: 4.509813
Average KL loss: 0.300911
Average total loss: 4.810723
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.5621e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 4.844939
Average KL loss: 0.300916
Average total loss: 5.145855
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.7260e-08, device='cuda:0')
Epoch 83
Average batch original loss after noise: 4.604958
Average KL loss: 0.300922
Average total loss: 4.905880
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.8598e-07, device='cuda:0')
Epoch 84
Average batch original loss after noise: 4.436897
Average KL loss: 0.300927
Average total loss: 4.737824
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.1205e-08, device='cuda:0')
 Percentile value: 0.7742306292057037
Non-zero model percentage: 0.19531679153442383%, Non-zero mask percentage: 0.19531679153442383%

--- Pruning Level [9/12]: ---
conv1.weight         | nonzeros =     614 /    1728             ( 35.53%) | total_pruned =    1114 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     592 /   36864             (  1.61%) | total_pruned =   36272 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     745 /   36864             (  2.02%) | total_pruned =   36119 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     701 /   36864             (  1.90%) | total_pruned =   36163 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      35 /      64             ( 54.69%) | total_pruned =      29 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     616 /   36864             (  1.67%) | total_pruned =   36248 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     945 /   73728             (  1.28%) | total_pruned =   72783 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      11 /     128             (  8.59%) | total_pruned =     117 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    1305 /  147456             (  0.89%) | total_pruned =  146151 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       8 /     128             (  6.25%) | total_pruned =     120 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     562 /    8192             (  6.86%) | total_pruned =    7630 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     109 /     128             ( 85.16%) | total_pruned =      19 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       7 /     128             (  5.47%) | total_pruned =     121 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     555 /  147456             (  0.38%) | total_pruned =  146901 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     112 /     128             ( 87.50%) | total_pruned =      16 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      12 /     128             (  9.38%) | total_pruned =     116 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     524 /  147456             (  0.36%) | total_pruned =  146932 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      11 /     128             (  8.59%) | total_pruned =     117 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    1777 /  294912             (  0.60%) | total_pruned =  293135 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      46 /     256             ( 17.97%) | total_pruned =     210 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    2182 /  589824             (  0.37%) | total_pruned =  587642 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      10 /     256             (  3.91%) | total_pruned =     246 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     685 /   32768             (  2.09%) | total_pruned =   32083 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     216 /     256             ( 84.38%) | total_pruned =      40 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      12 /     256             (  4.69%) | total_pruned =     244 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     369 /  589824             (  0.06%) | total_pruned =  589455 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       6 /     256             (  2.34%) | total_pruned =     250 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     421 /  589824             (  0.07%) | total_pruned =  589403 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     190 /     256             ( 74.22%) | total_pruned =      66 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       5 /     256             (  1.95%) | total_pruned =     251 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    2149 / 1179648             (  0.18%) | total_pruned = 1177499 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =      50 /     512             (  9.77%) | total_pruned =     462 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    1620 / 2359296             (  0.07%) | total_pruned = 2357676 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     400 /     512             ( 78.12%) | total_pruned =     112 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     153 /     512             ( 29.88%) | total_pruned =     359 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     402 /  131072             (  0.31%) | total_pruned =  130670 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     232 /     512             ( 45.31%) | total_pruned =     280 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     159 /     512             ( 31.05%) | total_pruned =     353 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     637 / 2359296             (  0.03%) | total_pruned = 2358659 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     180 /     512             ( 35.16%) | total_pruned =     332 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      14 /     512             (  2.73%) | total_pruned =     498 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     117 / 2359296             (  0.00%) | total_pruned = 2359179 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     132 /     512             ( 25.78%) | total_pruned =     380 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       2 /     512             (  0.39%) | total_pruned =     510 | shape = torch.Size([512])
linear.weight        | nonzeros =     330 /    5120             (  6.45%) | total_pruned =    4790 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 21834, pruned : 11156928, total: 11178762, Compression rate :     511.99x  ( 99.80% pruned)
Train Epoch: 99/100 Loss: 0.339843 Accuracy: 73.18 90.82 % Best test Accuracy: 74.43%
tensor(0.0026, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.4762e-07, device='cuda:0')
Epoch 1
Average batch original loss after noise: 5.115592
Average KL loss: 0.295544
Average total loss: 5.411135
tensor(0.0025, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.9948e-07, device='cuda:0')
Epoch 2
Average batch original loss after noise: 4.950012
Average KL loss: 0.286948
Average total loss: 5.236960
tensor(0.0024, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.3843e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 4.894485
Average KL loss: 0.279980
Average total loss: 5.174465
tensor(0.0023, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.0886e-07, device='cuda:0')
Epoch 4
Average batch original loss after noise: 4.859430
Average KL loss: 0.274250
Average total loss: 5.133680
tensor(0.0023, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-5.7231e-07, device='cuda:0')
Epoch 5
Average batch original loss after noise: 4.815192
Average KL loss: 0.269528
Average total loss: 5.084720
tensor(0.0022, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(1.5642e-07, device='cuda:0')
Epoch 6
Average batch original loss after noise: 4.711792
Average KL loss: 0.265682
Average total loss: 4.977474
tensor(0.0022, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(6.9667e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 4.795639
Average KL loss: 0.262542
Average total loss: 5.058180
tensor(0.0021, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(5.8373e-09, device='cuda:0')
Epoch 8
Average batch original loss after noise: 4.953379
Average KL loss: 0.259980
Average total loss: 5.213359
tensor(0.0021, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.1899e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 4.970483
Average KL loss: 0.257896
Average total loss: 5.228378
tensor(0.0020, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.4116e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 4.901710
Average KL loss: 0.256179
Average total loss: 5.157890
tensor(0.0020, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-3.5697e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 4.966720
Average KL loss: 0.254755
Average total loss: 5.221475
tensor(0.0020, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-3.0565e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 4.931099
Average KL loss: 0.253579
Average total loss: 5.184678
tensor(0.0020, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.3399e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 4.800312
Average KL loss: 0.252595
Average total loss: 5.052908
tensor(0.0019, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-7.6004e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 4.669285
Average KL loss: 0.251722
Average total loss: 4.921007
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-8.4572e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 4.647189
Average KL loss: 0.250962
Average total loss: 4.898151
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.2185e-07, device='cuda:0')
Epoch 16
Average batch original loss after noise: 4.579971
Average KL loss: 0.250320
Average total loss: 4.830292
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(1.4456e-07, device='cuda:0')
Epoch 17
Average batch original loss after noise: 4.530072
Average KL loss: 0.249782
Average total loss: 4.779854
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(1.5759e-07, device='cuda:0')
Epoch 18
Average batch original loss after noise: 4.756371
Average KL loss: 0.249295
Average total loss: 5.005666
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-3.5519e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 4.762398
Average KL loss: 0.248866
Average total loss: 5.011264
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.4508e-07, device='cuda:0')
Epoch 20
Average batch original loss after noise: 4.620323
Average KL loss: 0.248467
Average total loss: 4.868790
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-4.5234e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 4.507872
Average KL loss: 0.248125
Average total loss: 4.755997
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(5.5437e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 4.419536
Average KL loss: 0.247836
Average total loss: 4.667372
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-6.8385e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 4.398994
Average KL loss: 0.247587
Average total loss: 4.646580
tensor(0.0019, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(1.0623e-07, device='cuda:0')
Epoch 24
Average batch original loss after noise: 4.538130
Average KL loss: 0.247359
Average total loss: 4.785489
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.2182e-07, device='cuda:0')
Epoch 25
Average batch original loss after noise: 4.489386
Average KL loss: 0.247148
Average total loss: 4.736534
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-6.7282e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 4.229971
Average KL loss: 0.246955
Average total loss: 4.476926
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-2.4063e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 4.305546
Average KL loss: 0.246803
Average total loss: 4.552349
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-3.2941e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 4.368389
Average KL loss: 0.246702
Average total loss: 4.615091
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-7.1895e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 4.332056
Average KL loss: 0.246620
Average total loss: 4.578676
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.1359e-07, device='cuda:0')
Epoch 30
Average batch original loss after noise: 4.533921
Average KL loss: 0.246558
Average total loss: 4.780479
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-5.6774e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 4.538392
Average KL loss: 0.246499
Average total loss: 4.784891
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-4.1131e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 4.334314
Average KL loss: 0.246480
Average total loss: 4.580794
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-4.3071e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 4.468415
Average KL loss: 0.246496
Average total loss: 4.714911
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-7.7189e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 4.311307
Average KL loss: 0.246499
Average total loss: 4.557806
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-3.9099e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 4.086802
Average KL loss: 0.246513
Average total loss: 4.333315
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-7.5036e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 4.334027
Average KL loss: 0.246547
Average total loss: 4.580574
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(4.9788e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 4.347712
Average KL loss: 0.246613
Average total loss: 4.594324
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.7263e-07, device='cuda:0')
Epoch 38
Average batch original loss after noise: 4.248714
Average KL loss: 0.246679
Average total loss: 4.495392
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.4260e-07, device='cuda:0')
Epoch 39
Average batch original loss after noise: 4.207667
Average KL loss: 0.246745
Average total loss: 4.454412
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-7.5526e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 4.206889
Average KL loss: 0.246822
Average total loss: 4.453711
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.2141e-07, device='cuda:0')
Epoch 41
Average batch original loss after noise: 4.034274
Average KL loss: 0.246934
Average total loss: 4.281208
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.2352e-07, device='cuda:0')
Epoch 42
Average batch original loss after noise: 4.175066
Average KL loss: 0.247047
Average total loss: 4.422113
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(2.2167e-07, device='cuda:0')
Epoch 43
Average batch original loss after noise: 4.217632
Average KL loss: 0.247160
Average total loss: 4.464792
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.4949e-07, device='cuda:0')
Epoch 44
Average batch original loss after noise: 4.027188
Average KL loss: 0.247283
Average total loss: 4.274472
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.5448e-07, device='cuda:0')
Epoch 45
Average batch original loss after noise: 4.168377
Average KL loss: 0.247422
Average total loss: 4.415800
tensor(0.0018, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.7207e-07, device='cuda:0')
Epoch 46
Average batch original loss after noise: 4.155436
Average KL loss: 0.247582
Average total loss: 4.403019
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-6.8024e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 4.201954
Average KL loss: 0.247747
Average total loss: 4.449701
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(4.9678e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 3.977933
Average KL loss: 0.247896
Average total loss: 4.225830
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(7.1292e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 4.076733
Average KL loss: 0.248036
Average total loss: 4.324769
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.2616e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 4.129854
Average KL loss: 0.248206
Average total loss: 4.378060
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-4.3935e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 3.954017
Average KL loss: 0.248354
Average total loss: 4.202371
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.8411e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 4.195570
Average KL loss: 0.248505
Average total loss: 4.444075
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(1.4832e-07, device='cuda:0')
Epoch 53
Average batch original loss after noise: 3.999616
Average KL loss: 0.248677
Average total loss: 4.248294
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.2325e-07, device='cuda:0')
Epoch 54
Average batch original loss after noise: 3.971886
Average KL loss: 0.248845
Average total loss: 4.220731
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(1.6145e-07, device='cuda:0')
Epoch 55
Average batch original loss after noise: 3.861696
Average KL loss: 0.249013
Average total loss: 4.110709
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-7.5015e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 4.046788
Average KL loss: 0.249195
Average total loss: 4.295983
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-5.0701e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 3.966767
Average KL loss: 0.249396
Average total loss: 4.216162
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.1525e-07, device='cuda:0')
Epoch 58
Average batch original loss after noise: 3.933694
Average KL loss: 0.249589
Average total loss: 4.183284
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.8824e-07, device='cuda:0')
Epoch 59
Average batch original loss after noise: 3.973843
Average KL loss: 0.249800
Average total loss: 4.223643
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(8.6341e-09, device='cuda:0')
Epoch 60
Average batch original loss after noise: 4.013450
Average KL loss: 0.250016
Average total loss: 4.263466
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.9737e-07, device='cuda:0')
Epoch 61
Average batch original loss after noise: 4.063805
Average KL loss: 0.250212
Average total loss: 4.314017
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.0241e-07, device='cuda:0')
Epoch 62
Average batch original loss after noise: 3.922523
Average KL loss: 0.250413
Average total loss: 4.172936
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.6083e-07, device='cuda:0')
Epoch 63
Average batch original loss after noise: 4.032280
Average KL loss: 0.250622
Average total loss: 4.282902
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-3.2696e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 3.900005
Average KL loss: 0.250840
Average total loss: 4.150846
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.4900e-07, device='cuda:0')
Epoch 65
Average batch original loss after noise: 3.751510
Average KL loss: 0.251042
Average total loss: 4.002551
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(2.2702e-07, device='cuda:0')
Epoch 66
Average batch original loss after noise: 4.114568
Average KL loss: 0.251256
Average total loss: 4.365825
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(6.2817e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 3.950872
Average KL loss: 0.251494
Average total loss: 4.202365
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(7.9037e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 3.837166
Average KL loss: 0.251722
Average total loss: 4.088888
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(3.5719e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 3.779528
Average KL loss: 0.251954
Average total loss: 4.031481
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(1.2131e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 3.975469
Average KL loss: 0.252162
Average total loss: 4.227630
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-7.4201e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 3.850405
Average KL loss: 0.252359
Average total loss: 4.102763
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-3.3305e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 3.712395
Average KL loss: 0.252556
Average total loss: 3.964951
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.5530e-07, device='cuda:0')
Epoch 73
Average batch original loss after noise: 3.700208
Average KL loss: 0.252773
Average total loss: 3.952981
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.8472e-07, device='cuda:0')
Epoch 74
Average batch original loss after noise: 4.004487
Average KL loss: 0.253025
Average total loss: 4.257512
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(1.6137e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 3.766454
Average KL loss: 0.253267
Average total loss: 4.019721
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(6.1205e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 3.815597
Average KL loss: 0.253506
Average total loss: 4.069103
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-2.5776e-07, device='cuda:0')
Epoch 77
Average batch original loss after noise: 3.679619
Average KL loss: 0.253731
Average total loss: 3.933350
tensor(0.0018, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.8601e-07, device='cuda:0')
Epoch 78
Average batch original loss after noise: 3.685817
Average KL loss: 0.253934
Average total loss: 3.939751
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.6564e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 3.631266
Average KL loss: 0.254118
Average total loss: 3.885385
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-9.9453e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 3.733245
Average KL loss: 0.254329
Average total loss: 3.987574
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(3.4643e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 3.733611
Average KL loss: 0.254552
Average total loss: 3.988163
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-3.5696e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 3.863069
Average KL loss: 0.254786
Average total loss: 4.117855
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.6744e-07, device='cuda:0')
Epoch 83
Average batch original loss after noise: 3.708130
Average KL loss: 0.255017
Average total loss: 3.963146
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.0858e-07, device='cuda:0')
Epoch 84
Average batch original loss after noise: 3.548451
Average KL loss: 0.255235
Average total loss: 3.803686
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.1291e-07, device='cuda:0')
Epoch 85
Average batch original loss after noise: 3.642118
Average KL loss: 0.255449
Average total loss: 3.897567
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(1.6027e-07, device='cuda:0')
Epoch 86
Average batch original loss after noise: 3.709782
Average KL loss: 0.255688
Average total loss: 3.965470
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-6.9566e-08, device='cuda:0')
Epoch 87
Average batch original loss after noise: 3.663241
Average KL loss: 0.255919
Average total loss: 3.919160
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(5.2975e-08, device='cuda:0')
Epoch 88
Average batch original loss after noise: 3.578225
Average KL loss: 0.256147
Average total loss: 3.834373
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.7542e-07, device='cuda:0')
Epoch 89
Average batch original loss after noise: 3.694405
Average KL loss: 0.256367
Average total loss: 3.950772
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.3033e-07, device='cuda:0')
Epoch 90
Average batch original loss after noise: 3.615367
Average KL loss: 0.256619
Average total loss: 3.871986
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(1.0174e-07, device='cuda:0')
Epoch 91
Average batch original loss after noise: 3.610220
Average KL loss: 0.256856
Average total loss: 3.867075
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-7.6041e-08, device='cuda:0')
Epoch 92
Average batch original loss after noise: 3.546972
Average KL loss: 0.257103
Average total loss: 3.804075
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.4217e-07, device='cuda:0')
Epoch 93
Average batch original loss after noise: 3.455796
Average KL loss: 0.257351
Average total loss: 3.713147
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.2792e-07, device='cuda:0')
Epoch 94
Average batch original loss after noise: 3.547717
Average KL loss: 0.257598
Average total loss: 3.805316
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.2065e-08, device='cuda:0')
Epoch 95
Average batch original loss after noise: 3.498265
Average KL loss: 0.257832
Average total loss: 3.756098
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.9755e-07, device='cuda:0')
Epoch 96
Average batch original loss after noise: 3.581376
Average KL loss: 0.258061
Average total loss: 3.839437
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-8.1346e-08, device='cuda:0')
Epoch 97
Average batch original loss after noise: 3.550229
Average KL loss: 0.258305
Average total loss: 3.808534
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.9133e-07, device='cuda:0')
Epoch 98
Average batch original loss after noise: 3.500590
Average KL loss: 0.258544
Average total loss: 3.759134
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.0927e-07, device='cuda:0')
Epoch 99
Average batch original loss after noise: 3.535198
Average KL loss: 0.258812
Average total loss: 3.794010
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(6.9138e-08, device='cuda:0')
Epoch 100
Average batch original loss after noise: 3.784863
Average KL loss: 0.259054
Average total loss: 4.043917
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-4.5669e-08, device='cuda:0')
Epoch 101
Average batch original loss after noise: 3.527972
Average KL loss: 0.259302
Average total loss: 3.787274
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-2.3984e-07, device='cuda:0')
Epoch 102
Average batch original loss after noise: 3.372944
Average KL loss: 0.259529
Average total loss: 3.632473
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(1.0873e-08, device='cuda:0')
Epoch 103
Average batch original loss after noise: 3.442717
Average KL loss: 0.259775
Average total loss: 3.702493
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(3.6232e-08, device='cuda:0')
Epoch 104
Average batch original loss after noise: 3.392258
Average KL loss: 0.260022
Average total loss: 3.652279
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-1.8745e-08, device='cuda:0')
Epoch 105
Average batch original loss after noise: 3.622516
Average KL loss: 0.260265
Average total loss: 3.882781
tensor(0.0018, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(4.0227e-08, device='cuda:0')
Epoch 106
Average batch original loss after noise: 3.583714
Average KL loss: 0.260536
Average total loss: 3.844250
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.7007e-07, device='cuda:0')
Epoch 107
Average batch original loss after noise: 3.517711
Average KL loss: 0.260805
Average total loss: 3.778516
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.0546e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 3.579309
Average KL loss: 0.261061
Average total loss: 3.840369
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-3.8258e-08, device='cuda:0')
Epoch 109
Average batch original loss after noise: 3.426350
Average KL loss: 0.261285
Average total loss: 3.687635
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(1.5908e-08, device='cuda:0')
Epoch 110
Average batch original loss after noise: 3.362799
Average KL loss: 0.261531
Average total loss: 3.624330
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.0052e-07, device='cuda:0')
Epoch 111
Average batch original loss after noise: 3.306112
Average KL loss: 0.261803
Average total loss: 3.567915
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(3.3492e-08, device='cuda:0')
Epoch 112
Average batch original loss after noise: 3.413994
Average KL loss: 0.262057
Average total loss: 3.676050
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-6.6579e-08, device='cuda:0')
Epoch 113
Average batch original loss after noise: 3.453874
Average KL loss: 0.262324
Average total loss: 3.716198
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.7560e-08, device='cuda:0')
Epoch 114
Average batch original loss after noise: 3.398140
Average KL loss: 0.262576
Average total loss: 3.660716
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.3275e-07, device='cuda:0')
Epoch 115
Average batch original loss after noise: 3.323286
Average KL loss: 0.262831
Average total loss: 3.586117
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.1379e-08, device='cuda:0')
Epoch 116
Average batch original loss after noise: 3.272780
Average KL loss: 0.263065
Average total loss: 3.535845
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.2804e-07, device='cuda:0')
Epoch 117
Average batch original loss after noise: 3.365347
Average KL loss: 0.263323
Average total loss: 3.628670
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(4.8853e-08, device='cuda:0')
Epoch 118
Average batch original loss after noise: 3.400714
Average KL loss: 0.263569
Average total loss: 3.664283
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-7.2580e-09, device='cuda:0')
Epoch 119
Average batch original loss after noise: 3.410822
Average KL loss: 0.263825
Average total loss: 3.674647
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(7.4925e-09, device='cuda:0')
Epoch 120
Average batch original loss after noise: 3.451498
Average KL loss: 0.264084
Average total loss: 3.715582
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(7.9943e-08, device='cuda:0')
Epoch 121
Average batch original loss after noise: 3.311231
Average KL loss: 0.264331
Average total loss: 3.575561
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-5.3990e-08, device='cuda:0')
Epoch 122
Average batch original loss after noise: 3.325688
Average KL loss: 0.264587
Average total loss: 3.590275
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.6666e-07, device='cuda:0')
Epoch 123
Average batch original loss after noise: 3.426212
Average KL loss: 0.264849
Average total loss: 3.691061
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.4789e-07, device='cuda:0')
Epoch 124
Average batch original loss after noise: 3.173014
Average KL loss: 0.265124
Average total loss: 3.438137
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(4.4074e-09, device='cuda:0')
Epoch 125
Average batch original loss after noise: 3.366792
Average KL loss: 0.265387
Average total loss: 3.632180
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-2.5247e-08, device='cuda:0')
Epoch 126
Average batch original loss after noise: 3.434870
Average KL loss: 0.265646
Average total loss: 3.700515
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.7656e-07, device='cuda:0')
Epoch 127
Average batch original loss after noise: 3.219075
Average KL loss: 0.265886
Average total loss: 3.484961
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-1.3146e-08, device='cuda:0')
Epoch 128
Average batch original loss after noise: 3.419915
Average KL loss: 0.266163
Average total loss: 3.686079
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(4.5854e-08, device='cuda:0')
Epoch 129
Average batch original loss after noise: 3.324678
Average KL loss: 0.266431
Average total loss: 3.591109
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.3123e-08, device='cuda:0')
Epoch 130
Average batch original loss after noise: 3.434348
Average KL loss: 0.266698
Average total loss: 3.701046
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-6.7972e-08, device='cuda:0')
Epoch 131
Average batch original loss after noise: 3.299645
Average KL loss: 0.266975
Average total loss: 3.566620
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.9271e-08, device='cuda:0')
Epoch 132
Average batch original loss after noise: 3.319521
Average KL loss: 0.267229
Average total loss: 3.586749
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-4.0187e-08, device='cuda:0')
Epoch 133
Average batch original loss after noise: 3.217582
Average KL loss: 0.267478
Average total loss: 3.485060
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(9.0581e-08, device='cuda:0')
Epoch 134
Average batch original loss after noise: 3.310252
Average KL loss: 0.267716
Average total loss: 3.577968
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(5.1761e-08, device='cuda:0')
Epoch 135
Average batch original loss after noise: 3.344004
Average KL loss: 0.267990
Average total loss: 3.611994
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.6748e-07, device='cuda:0')
Epoch 136
Average batch original loss after noise: 3.183165
Average KL loss: 0.268150
Average total loss: 3.451315
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.5904e-07, device='cuda:0')
Epoch 137
Average batch original loss after noise: 3.309712
Average KL loss: 0.268176
Average total loss: 3.577888
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.4855e-08, device='cuda:0')
Epoch 138
Average batch original loss after noise: 3.363962
Average KL loss: 0.268201
Average total loss: 3.632163
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.3755e-07, device='cuda:0')
Epoch 139
Average batch original loss after noise: 3.306315
Average KL loss: 0.268227
Average total loss: 3.574542
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(2.1649e-08, device='cuda:0')
Epoch 140
Average batch original loss after noise: 3.276966
Average KL loss: 0.268251
Average total loss: 3.545217
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.1767e-07, device='cuda:0')
Epoch 141
Average batch original loss after noise: 3.309667
Average KL loss: 0.268277
Average total loss: 3.577944
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-9.0153e-08, device='cuda:0')
Epoch 142
Average batch original loss after noise: 3.295368
Average KL loss: 0.268304
Average total loss: 3.563672
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.7672e-07, device='cuda:0')
Epoch 143
Average batch original loss after noise: 3.256375
Average KL loss: 0.268331
Average total loss: 3.524706
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(8.2482e-08, device='cuda:0')
Epoch 144
Average batch original loss after noise: 3.158901
Average KL loss: 0.268356
Average total loss: 3.427257
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-6.8084e-08, device='cuda:0')
Epoch 145
Average batch original loss after noise: 3.348841
Average KL loss: 0.268380
Average total loss: 3.617222
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.3708e-07, device='cuda:0')
Epoch 146
Average batch original loss after noise: 3.314010
Average KL loss: 0.268409
Average total loss: 3.582419
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.5537e-08, device='cuda:0')
Epoch 147
Average batch original loss after noise: 3.154902
Average KL loss: 0.268437
Average total loss: 3.423339
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-8.5672e-09, device='cuda:0')
Epoch 148
Average batch original loss after noise: 3.216965
Average KL loss: 0.268459
Average total loss: 3.485425
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(2.6565e-07, device='cuda:0')
Epoch 149
Average batch original loss after noise: 3.301069
Average KL loss: 0.268482
Average total loss: 3.569551
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.1858e-08, device='cuda:0')
Epoch 150
Average batch original loss after noise: 3.299151
Average KL loss: 0.268506
Average total loss: 3.567657
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.7949e-08, device='cuda:0')
Epoch 151
Average batch original loss after noise: 3.203015
Average KL loss: 0.268531
Average total loss: 3.471546
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(5.8239e-09, device='cuda:0')
Epoch 152
Average batch original loss after noise: 3.242121
Average KL loss: 0.268558
Average total loss: 3.510680
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.3272e-08, device='cuda:0')
Epoch 153
Average batch original loss after noise: 3.317983
Average KL loss: 0.268586
Average total loss: 3.586569
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-6.5223e-08, device='cuda:0')
Epoch 154
Average batch original loss after noise: 3.312261
Average KL loss: 0.268614
Average total loss: 3.580875
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.7315e-08, device='cuda:0')
Epoch 155
Average batch original loss after noise: 3.365722
Average KL loss: 0.268642
Average total loss: 3.634365
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.2358e-07, device='cuda:0')
Epoch 156
Average batch original loss after noise: 3.468585
Average KL loss: 0.268672
Average total loss: 3.737257
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-2.2222e-07, device='cuda:0')
Epoch 157
Average batch original loss after noise: 3.194344
Average KL loss: 0.268699
Average total loss: 3.463043
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.2838e-08, device='cuda:0')
Epoch 158
Average batch original loss after noise: 3.292056
Average KL loss: 0.268725
Average total loss: 3.560781
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.1269e-09, device='cuda:0')
Epoch 159
Average batch original loss after noise: 3.362999
Average KL loss: 0.268739
Average total loss: 3.631739
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.7885e-08, device='cuda:0')
Epoch 160
Average batch original loss after noise: 3.157704
Average KL loss: 0.268742
Average total loss: 3.426446
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(9.8294e-09, device='cuda:0')
Epoch 161
Average batch original loss after noise: 3.287713
Average KL loss: 0.268744
Average total loss: 3.556457
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(2.3534e-08, device='cuda:0')
Epoch 162
Average batch original loss after noise: 3.138513
Average KL loss: 0.268747
Average total loss: 3.407260
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(7.2612e-09, device='cuda:0')
Epoch 163
Average batch original loss after noise: 3.339396
Average KL loss: 0.268750
Average total loss: 3.608145
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-3.0838e-08, device='cuda:0')
Epoch 164
Average batch original loss after noise: 3.177500
Average KL loss: 0.268752
Average total loss: 3.446252
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-6.0004e-08, device='cuda:0')
Epoch 165
Average batch original loss after noise: 3.356839
Average KL loss: 0.268755
Average total loss: 3.625594
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(9.8562e-09, device='cuda:0')
Epoch 166
Average batch original loss after noise: 3.272006
Average KL loss: 0.268757
Average total loss: 3.540763
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.6152e-07, device='cuda:0')
Epoch 167
Average batch original loss after noise: 3.315731
Average KL loss: 0.268760
Average total loss: 3.584492
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(1.0759e-07, device='cuda:0')
Epoch 168
Average batch original loss after noise: 3.196334
Average KL loss: 0.268763
Average total loss: 3.465097
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(7.2900e-08, device='cuda:0')
Epoch 169
Average batch original loss after noise: 3.203731
Average KL loss: 0.268766
Average total loss: 3.472497
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-7.5874e-09, device='cuda:0')
Epoch 170
Average batch original loss after noise: 3.217478
Average KL loss: 0.268768
Average total loss: 3.486245
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.3132e-08, device='cuda:0')
Epoch 171
Average batch original loss after noise: 3.190964
Average KL loss: 0.268770
Average total loss: 3.459734
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-5.2662e-08, device='cuda:0')
Epoch 172
Average batch original loss after noise: 3.226989
Average KL loss: 0.268773
Average total loss: 3.495761
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(-1.2121e-08, device='cuda:0')
 Percentile value: 1.2598270177841187
Non-zero model percentage: 0.09765839576721191%, Non-zero mask percentage: 0.09765839576721191%

--- Pruning Level [10/12]: ---
conv1.weight         | nonzeros =     487 /    1728             ( 28.18%) | total_pruned =    1241 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
bn1.bias             | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     313 /   36864             (  0.85%) | total_pruned =   36551 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     433 /   36864             (  1.17%) | total_pruned =   36431 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     434 /   36864             (  1.18%) | total_pruned =   36430 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     397 /   36864             (  1.08%) | total_pruned =   36467 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     519 /   73728             (  0.70%) | total_pruned =   73209 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     125 /     128             ( 97.66%) | total_pruned =       3 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       6 /     128             (  4.69%) | total_pruned =     122 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     675 /  147456             (  0.46%) | total_pruned =  146781 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     125 /     128             ( 97.66%) | total_pruned =       3 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       2 /     128             (  1.56%) | total_pruned =     126 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     343 /    8192             (  4.19%) | total_pruned =    7849 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       2 /     128             (  1.56%) | total_pruned =     126 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     287 /  147456             (  0.19%) | total_pruned =  147169 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       7 /     128             (  5.47%) | total_pruned =     121 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     267 /  147456             (  0.18%) | total_pruned =  147189 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     104 /     128             ( 81.25%) | total_pruned =      24 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       8 /     128             (  6.25%) | total_pruned =     120 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     842 /  294912             (  0.29%) | total_pruned =  294070 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      22 /     256             (  8.59%) | total_pruned =     234 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     974 /  589824             (  0.17%) | total_pruned =  588850 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       4 /     256             (  1.56%) | total_pruned =     252 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     309 /   32768             (  0.94%) | total_pruned =   32459 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     187 /     256             ( 73.05%) | total_pruned =      69 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       4 /     256             (  1.56%) | total_pruned =     252 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     152 /  589824             (  0.03%) | total_pruned =  589672 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       3 /     256             (  1.17%) | total_pruned =     253 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     169 /  589824             (  0.03%) | total_pruned =  589655 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       3 /     256             (  1.17%) | total_pruned =     253 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     796 / 1179648             (  0.07%) | total_pruned = 1178852 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     355 /     512             ( 69.34%) | total_pruned =     157 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =      17 /     512             (  3.32%) | total_pruned =     495 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     465 / 2359296             (  0.02%) | total_pruned = 2358831 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     210 /     512             ( 41.02%) | total_pruned =     302 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =      36 /     512             (  7.03%) | total_pruned =     476 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     122 /  131072             (  0.09%) | total_pruned =  130950 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      94 /     512             ( 18.36%) | total_pruned =     418 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =      41 /     512             (  8.01%) | total_pruned =     471 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     105 / 2359296             (  0.00%) | total_pruned = 2359191 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =      36 /     512             (  7.03%) | total_pruned =     476 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       3 /     512             (  0.59%) | total_pruned =     509 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =      16 / 2359296             (  0.00%) | total_pruned = 2359280 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =      14 /     512             (  2.73%) | total_pruned =     498 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =     110 /    5120             (  2.15%) | total_pruned =    5010 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 10917, pruned : 11167845, total: 11178762, Compression rate :    1023.98x  ( 99.90% pruned)
Train Epoch: 99/100 Loss: 0.736071 Accuracy: 60.79 68.35 % Best test Accuracy: 62.47%
tensor(0.0018, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(6.0372e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.955978
Average KL loss: 0.265258
Average total loss: 3.221237
tensor(0.0018, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.7371e-09, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.978152
Average KL loss: 0.259518
Average total loss: 3.237670
tensor(0.0017, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(-8.6230e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 3.004831
Average KL loss: 0.254539
Average total loss: 3.259369
tensor(0.0017, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-9.4164e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 3.045103
Average KL loss: 0.250082
Average total loss: 3.295185
tensor(0.0017, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(-3.8032e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.980510
Average KL loss: 0.246097
Average total loss: 3.226607
tensor(0.0017, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-8.5695e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.897364
Average KL loss: 0.242520
Average total loss: 3.139884
tensor(0.0016, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-9.2105e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.890559
Average KL loss: 0.239293
Average total loss: 3.129852
tensor(0.0016, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-7.4738e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.993244
Average KL loss: 0.236392
Average total loss: 3.229636
tensor(0.0016, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(-1.0286e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 3.069119
Average KL loss: 0.233788
Average total loss: 3.302907
tensor(0.0016, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.6540e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 3.140183
Average KL loss: 0.231453
Average total loss: 3.371636
tensor(0.0015, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.1238e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.919825
Average KL loss: 0.229344
Average total loss: 3.149169
tensor(0.0015, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-1.0703e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.898712
Average KL loss: 0.227402
Average total loss: 3.126114
tensor(0.0015, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-2.1494e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.955767
Average KL loss: 0.225650
Average total loss: 3.181417
tensor(0.0015, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(-5.0070e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.944191
Average KL loss: 0.224072
Average total loss: 3.168263
tensor(0.0015, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.1828e-07, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.854622
Average KL loss: 0.222618
Average total loss: 3.077240
tensor(0.0015, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.2218e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.923760
Average KL loss: 0.221305
Average total loss: 3.145065
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.9559e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.849063
Average KL loss: 0.220096
Average total loss: 3.069160
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-3.7815e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.964700
Average KL loss: 0.218995
Average total loss: 3.183695
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(1.0831e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.968408
Average KL loss: 0.217965
Average total loss: 3.186373
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(2.0113e-08, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.902999
Average KL loss: 0.217024
Average total loss: 3.120022
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.2828e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.934660
Average KL loss: 0.216148
Average total loss: 3.150808
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-1.2361e-07, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.911319
Average KL loss: 0.215346
Average total loss: 3.126666
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(5.4154e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.917316
Average KL loss: 0.214597
Average total loss: 3.131913
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(-8.3004e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.973388
Average KL loss: 0.213908
Average total loss: 3.187296
tensor(0.0014, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(2.7819e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.934744
Average KL loss: 0.213261
Average total loss: 3.148006
tensor(0.0014, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.6957e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 3.099311
Average KL loss: 0.212648
Average total loss: 3.311959
tensor(0.0014, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-5.1657e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.866086
Average KL loss: 0.212093
Average total loss: 3.078179
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-6.5815e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.876407
Average KL loss: 0.211560
Average total loss: 3.087967
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.4060e-07, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.924610
Average KL loss: 0.211279
Average total loss: 3.135890
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.8381e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.960050
Average KL loss: 0.211230
Average total loss: 3.171280
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(2.7273e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.824465
Average KL loss: 0.211181
Average total loss: 3.035646
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.3848e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.834775
Average KL loss: 0.211131
Average total loss: 3.045906
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-6.9143e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.853455
Average KL loss: 0.211082
Average total loss: 3.064537
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-3.5829e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.794543
Average KL loss: 0.211033
Average total loss: 3.005576
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.6646e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.793036
Average KL loss: 0.210984
Average total loss: 3.004021
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-5.8869e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.801153
Average KL loss: 0.210936
Average total loss: 3.012089
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.4864e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.755396
Average KL loss: 0.210886
Average total loss: 2.966282
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(3.4754e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.781958
Average KL loss: 0.210838
Average total loss: 2.992796
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.8048e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.721442
Average KL loss: 0.210789
Average total loss: 2.932231
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-8.0640e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.843909
Average KL loss: 0.210740
Average total loss: 3.054649
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-3.3833e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.862672
Average KL loss: 0.210695
Average total loss: 3.073366
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-9.2063e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.810268
Average KL loss: 0.210648
Average total loss: 3.020916
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.3453e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.905236
Average KL loss: 0.210600
Average total loss: 3.115835
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-3.3907e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.845252
Average KL loss: 0.210552
Average total loss: 3.055803
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-1.7291e-07, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.820820
Average KL loss: 0.210505
Average total loss: 3.031325
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(9.5269e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.904443
Average KL loss: 0.210457
Average total loss: 3.114900
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(8.6160e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.750079
Average KL loss: 0.210410
Average total loss: 2.960489
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.3594e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.933950
Average KL loss: 0.210362
Average total loss: 3.144312
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.7012e-09, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.824351
Average KL loss: 0.210316
Average total loss: 3.034667
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.5718e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.844729
Average KL loss: 0.210271
Average total loss: 3.055000
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(1.5518e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.862959
Average KL loss: 0.210246
Average total loss: 3.073205
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-1.2002e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.792247
Average KL loss: 0.210242
Average total loss: 3.002489
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-2.1409e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.874201
Average KL loss: 0.210237
Average total loss: 3.084438
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.4084e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.818394
Average KL loss: 0.210233
Average total loss: 3.028627
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-1.0728e-07, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.966470
Average KL loss: 0.210228
Average total loss: 3.176698
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-7.2051e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.823458
Average KL loss: 0.210223
Average total loss: 3.033681
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-5.1290e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.834101
Average KL loss: 0.210218
Average total loss: 3.044320
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-8.1196e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.838080
Average KL loss: 0.210214
Average total loss: 3.048293
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-5.9293e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.826026
Average KL loss: 0.210209
Average total loss: 3.036235
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(-4.4036e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.810363
Average KL loss: 0.210204
Average total loss: 3.020567
tensor(0.0013, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(3.4462e-08, device='cuda:0')
 Percentile value: 1.6521332263946533
Non-zero model percentage: 0.04883367195725441%, Non-zero mask percentage: 0.04883367195725441%

--- Pruning Level [11/12]: ---
conv1.weight         | nonzeros =     351 /    1728             ( 20.31%) | total_pruned =    1377 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
bn1.bias             | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     141 /   36864             (  0.38%) | total_pruned =   36723 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      55 /      64             ( 85.94%) | total_pruned =       9 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     218 /   36864             (  0.59%) | total_pruned =   36646 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     247 /   36864             (  0.67%) | total_pruned =   36617 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     226 /   36864             (  0.61%) | total_pruned =   36638 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     237 /   73728             (  0.32%) | total_pruned =   73491 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     112 /     128             ( 87.50%) | total_pruned =      16 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       2 /     128             (  1.56%) | total_pruned =     126 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     323 /  147456             (  0.22%) | total_pruned =  147133 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     122 /     128             ( 95.31%) | total_pruned =       6 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       1 /     128             (  0.78%) | total_pruned =     127 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     189 /    8192             (  2.31%) | total_pruned =    8003 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       1 /     128             (  0.78%) | total_pruned =     127 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     136 /  147456             (  0.09%) | total_pruned =  147320 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     111 /  147456             (  0.08%) | total_pruned =  147345 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       2 /     128             (  1.56%) | total_pruned =     126 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     342 /  294912             (  0.12%) | total_pruned =  294570 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     210 /     256             ( 82.03%) | total_pruned =      46 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =      11 /     256             (  4.30%) | total_pruned =     245 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     426 /  589824             (  0.07%) | total_pruned =  589398 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     220 /     256             ( 85.94%) | total_pruned =      36 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     111 /   32768             (  0.34%) | total_pruned =   32657 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     127 /     256             ( 49.61%) | total_pruned =     129 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      55 /  589824             (  0.01%) | total_pruned =  589769 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      58 /     256             ( 22.66%) | total_pruned =     198 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       1 /     256             (  0.39%) | total_pruned =     255 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      56 /  589824             (  0.01%) | total_pruned =  589768 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      83 /     256             ( 32.42%) | total_pruned =     173 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     242 / 1179648             (  0.02%) | total_pruned = 1179406 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     182 /     512             ( 35.55%) | total_pruned =     330 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       3 /     512             (  0.59%) | total_pruned =     509 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     111 / 2359296             (  0.00%) | total_pruned = 2359185 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =      72 /     512             ( 14.06%) | total_pruned =     440 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       4 /     512             (  0.78%) | total_pruned =     508 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =      30 /  131072             (  0.02%) | total_pruned =  131042 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      29 /     512             (  5.66%) | total_pruned =     483 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       7 /     512             (  1.37%) | total_pruned =     505 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =      11 / 2359296             (  0.00%) | total_pruned = 2359285 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =       8 /     512             (  1.56%) | total_pruned =     504 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =       4 / 2359296             (  0.00%) | total_pruned = 2359292 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =       3 /     512             (  0.59%) | total_pruned =     509 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =      29 /    5120             (  0.57%) | total_pruned =    5091 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       0 /      10             (  0.00%) | total_pruned =      10 | shape = torch.Size([10])
alive: 5459, pruned : 11173303, total: 11178762, Compression rate :    2047.77x  ( 99.95% pruned)
Train Epoch: 99/100 Loss: 1.389197 Accuracy: 44.78 46.76 % Best test Accuracy: 45.15%
