Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/24]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/100 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.4999e-06, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302930
Average KL loss: 49.295101
Average total loss: 51.598030
tensor(-0.4850, device='cuda:0') tensor(4.8069e-05, device='cuda:0') tensor(2.3585e-06, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.303218
Average KL loss: 36.830981
Average total loss: 39.134198
tensor(-0.9448, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.0155e-06, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.303080
Average KL loss: 26.944062
Average total loss: 29.247141
tensor(-1.3560, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(1.6291e-06, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.303138
Average KL loss: 19.873744
Average total loss: 22.176881
tensor(-1.7124, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.2948e-06, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.303197
Average KL loss: 15.013430
Average total loss: 17.316627
tensor(-2.0193, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.0345e-06, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302890
Average KL loss: 11.663687
Average total loss: 13.966577
tensor(-2.2851, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(8.3843e-07, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302922
Average KL loss: 9.304117
Average total loss: 11.607038
tensor(-2.5178, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(6.9055e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302863
Average KL loss: 7.595405
Average total loss: 9.898268
tensor(-2.7240, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(5.7812e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302863
Average KL loss: 6.323510
Average total loss: 8.626373
tensor(-2.9086, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.9076e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302717
Average KL loss: 5.352638
Average total loss: 7.655355
tensor(-3.0757, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.2192e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.303046
Average KL loss: 4.594841
Average total loss: 6.897888
tensor(-3.2282, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.6685e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302800
Average KL loss: 3.991697
Average total loss: 6.294497
tensor(-3.3684, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(3.2215e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302786
Average KL loss: 3.503394
Average total loss: 5.806180
tensor(-3.4982, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(2.8535e-07, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302809
Average KL loss: 3.102131
Average total loss: 5.404940
tensor(-3.6192, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.5450e-07, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302858
Average KL loss: 2.768051
Average total loss: 5.070908
tensor(-3.7324, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.2858e-07, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302808
Average KL loss: 2.486679
Average total loss: 4.789487
tensor(-3.8388, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.0648e-07, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302866
Average KL loss: 2.247257
Average total loss: 4.550122
tensor(-3.9393, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(1.8751e-07, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302784
Average KL loss: 2.041661
Average total loss: 4.344445
tensor(-4.0345, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(1.7107e-07, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302799
Average KL loss: 1.863676
Average total loss: 4.166475
tensor(-4.1251, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.5682e-07, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302800
Average KL loss: 1.708448
Average total loss: 4.011249
tensor(-4.2114, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.4408e-07, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302597
Average KL loss: 1.572163
Average total loss: 3.874760
tensor(-4.2939, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.3309e-07, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302671
Average KL loss: 1.451785
Average total loss: 3.754455
tensor(-4.3730, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.2306e-07, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302635
Average KL loss: 1.344870
Average total loss: 3.647504
tensor(-4.4489, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.1435e-07, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302782
Average KL loss: 1.249432
Average total loss: 3.552214
tensor(-4.5220, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.0651e-07, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302782
Average KL loss: 1.163844
Average total loss: 3.466626
tensor(-4.5925, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(9.9472e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302641
Average KL loss: 1.086766
Average total loss: 3.389408
tensor(-4.6606, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(9.3128e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302680
Average KL loss: 1.017077
Average total loss: 3.319757
tensor(-4.7264, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(8.7159e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302713
Average KL loss: 0.953837
Average total loss: 3.256550
tensor(-4.7902, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(8.1938e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302689
Average KL loss: 0.896255
Average total loss: 3.198945
tensor(-4.8521, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(7.7151e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302760
Average KL loss: 0.843665
Average total loss: 3.146426
tensor(-4.9122, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.2662e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302583
Average KL loss: 0.795490
Average total loss: 3.098073
tensor(-4.9706, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.8548e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302752
Average KL loss: 0.751240
Average total loss: 3.053992
tensor(-5.0275, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.4880e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302727
Average KL loss: 0.710490
Average total loss: 3.013218
tensor(-5.0830, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.1361e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302732
Average KL loss: 0.672871
Average total loss: 2.975603
tensor(-5.1371, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.8148e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302695
Average KL loss: 0.638060
Average total loss: 2.940755
tensor(-5.1899, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.5224e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302717
Average KL loss: 0.605784
Average total loss: 2.908501
tensor(-5.2416, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.2456e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302615
Average KL loss: 0.575800
Average total loss: 2.878414
tensor(-5.2921, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.9910e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302648
Average KL loss: 0.547891
Average total loss: 2.850538
tensor(-5.3415, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.7550e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302629
Average KL loss: 0.521866
Average total loss: 2.824494
tensor(-5.3900, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.5381e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302652
Average KL loss: 0.497553
Average total loss: 2.800205
tensor(-5.4375, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(4.3243e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302664
Average KL loss: 0.474809
Average total loss: 2.777473
tensor(-5.4840, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(4.1259e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302603
Average KL loss: 0.453493
Average total loss: 2.756096
tensor(-5.5298, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.9538e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302667
Average KL loss: 0.433489
Average total loss: 2.736155
tensor(-5.5747, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.7800e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302671
Average KL loss: 0.414691
Average total loss: 2.717362
tensor(-5.6188, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.6188e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302671
Average KL loss: 0.397003
Average total loss: 2.699675
tensor(-5.6622, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.4652e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302540
Average KL loss: 0.380341
Average total loss: 2.682880
tensor(-5.7050, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.3178e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302610
Average KL loss: 0.364624
Average total loss: 2.667234
tensor(-5.7470, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.1828e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302651
Average KL loss: 0.349781
Average total loss: 2.652432
tensor(-5.7884, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.0512e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302640
Average KL loss: 0.335748
Average total loss: 2.638388
tensor(-5.8292, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.9320e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302644
Average KL loss: 0.322468
Average total loss: 2.625111
tensor(-5.8694, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.8145e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302668
Average KL loss: 0.309887
Average total loss: 2.612555
tensor(-5.9091, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.7093e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302573
Average KL loss: 0.297958
Average total loss: 2.600531
tensor(-5.9482, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.6047e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302664
Average KL loss: 0.286638
Average total loss: 2.589301
tensor(-5.9868, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.5082e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302606
Average KL loss: 0.275882
Average total loss: 2.578488
tensor(-6.0249, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.4165e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302612
Average KL loss: 0.265657
Average total loss: 2.568269
tensor(-6.0626, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.3243e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302648
Average KL loss: 0.255927
Average total loss: 2.558575
tensor(-6.0998, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.2387e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302587
Average KL loss: 0.246663
Average total loss: 2.549250
tensor(-6.1366, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.1604e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302584
Average KL loss: 0.237834
Average total loss: 2.540417
tensor(-6.1729, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.0806e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302644
Average KL loss: 0.229413
Average total loss: 2.532057
tensor(-6.2089, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.0100e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302587
Average KL loss: 0.221375
Average total loss: 2.523963
tensor(-6.2444, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.9405e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302630
Average KL loss: 0.213699
Average total loss: 2.516329
tensor(-6.2796, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8720e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302609
Average KL loss: 0.206365
Average total loss: 2.508974
tensor(-6.3144, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8132e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302614
Average KL loss: 0.199353
Average total loss: 2.501967
tensor(-6.3489, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7494e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302614
Average KL loss: 0.192643
Average total loss: 2.495257
tensor(-6.3831, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6908e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302632
Average KL loss: 0.186221
Average total loss: 2.488852
tensor(-6.4169, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6355e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302614
Average KL loss: 0.180070
Average total loss: 2.482683
tensor(-6.4504, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5831e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302601
Average KL loss: 0.174175
Average total loss: 2.476776
tensor(-6.4836, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5279e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302645
Average KL loss: 0.168522
Average total loss: 2.471166
tensor(-6.5166, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4794e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302591
Average KL loss: 0.163098
Average total loss: 2.465689
tensor(-6.5492, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4310e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302553
Average KL loss: 0.157891
Average total loss: 2.460444
tensor(-6.5816, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3884e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302621
Average KL loss: 0.152892
Average total loss: 2.455514
tensor(-6.6137, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3450e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302584
Average KL loss: 0.148088
Average total loss: 2.450672
tensor(-6.6455, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2988e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302551
Average KL loss: 0.143473
Average total loss: 2.446024
tensor(-6.6771, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2608e-08, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302593
Average KL loss: 0.139035
Average total loss: 2.441628
tensor(-6.7085, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2235e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302608
Average KL loss: 0.134765
Average total loss: 2.437373
tensor(-6.7396, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1848e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302605
Average KL loss: 0.130656
Average total loss: 2.433261
tensor(-6.7705, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1455e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302604
Average KL loss: 0.126701
Average total loss: 2.429304
tensor(-6.8012, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1131e-08, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302603
Average KL loss: 0.122892
Average total loss: 2.425495
tensor(-6.8317, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0806e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302639
Average KL loss: 0.119222
Average total loss: 2.421861
tensor(-6.8619, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0482e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302590
Average KL loss: 0.115685
Average total loss: 2.418275
tensor(-6.8920, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0155e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302624
Average KL loss: 0.112276
Average total loss: 2.414900
tensor(-6.9219, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.8887e-09, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302606
Average KL loss: 0.108988
Average total loss: 2.411594
tensor(-6.9515, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.6017e-09, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302595
Average KL loss: 0.105816
Average total loss: 2.408411
tensor(-6.9810, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.3043e-09, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302564
Average KL loss: 0.102755
Average total loss: 2.405318
tensor(-7.0104, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.0306e-09, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302594
Average KL loss: 0.099800
Average total loss: 2.402395
tensor(-7.0395, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.7854e-09, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302613
Average KL loss: 0.096947
Average total loss: 2.399560
tensor(-7.0684, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.5362e-09, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302596
Average KL loss: 0.094191
Average total loss: 2.396787
tensor(-7.0972, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.3039e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302618
Average KL loss: 0.091529
Average total loss: 2.394146
tensor(-7.1259, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.1057e-09, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302581
Average KL loss: 0.088956
Average total loss: 2.391537
tensor(-7.1543, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.8241e-09, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302595
Average KL loss: 0.086469
Average total loss: 2.389064
tensor(-7.1827, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.6256e-09, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302596
Average KL loss: 0.084064
Average total loss: 2.386661
tensor(-7.2108, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.3947e-09, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302598
Average KL loss: 0.081739
Average total loss: 2.384336
tensor(-7.2389, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.2141e-09, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302613
Average KL loss: 0.079489
Average total loss: 2.382102
tensor(-7.2667, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.9992e-09, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302588
Average KL loss: 0.077312
Average total loss: 2.379900
tensor(-7.2945, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.8016e-09, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302569
Average KL loss: 0.075206
Average total loss: 2.377775
tensor(-7.3220, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.6315e-09, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302583
Average KL loss: 0.073167
Average total loss: 2.375750
tensor(-7.3495, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.4330e-09, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302615
Average KL loss: 0.071193
Average total loss: 2.373808
tensor(-7.3768, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.2586e-09, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302630
Average KL loss: 0.069281
Average total loss: 2.371910
tensor(-7.4040, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.1140e-09, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302622
Average KL loss: 0.067428
Average total loss: 2.370051
tensor(-7.4311, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.9437e-09, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302617
Average KL loss: 0.065634
Average total loss: 2.368251
tensor(-7.4580, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.7923e-09, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302611
Average KL loss: 0.063895
Average total loss: 2.366506
tensor(-7.4848, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.6385e-09, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302590
Average KL loss: 0.062210
Average total loss: 2.364799
tensor(-7.5115, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.4809e-09, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302583
Average KL loss: 0.060576
Average total loss: 2.363159
tensor(-7.5381, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.3405e-09, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302599
Average KL loss: 0.058993
Average total loss: 2.361592
tensor(-7.5645, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.2020e-09, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302560
Average KL loss: 0.057458
Average total loss: 2.360018
tensor(-7.5909, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.0650e-09, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302576
Average KL loss: 0.055970
Average total loss: 2.358547
tensor(-7.6171, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.9455e-09, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302602
Average KL loss: 0.054526
Average total loss: 2.357127
tensor(-7.6432, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.8123e-09, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.053124
Average total loss: 2.355709
tensor(-7.6692, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.6906e-09, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302561
Average KL loss: 0.051765
Average total loss: 2.354325
tensor(-7.6951, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.5633e-09, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302580
Average KL loss: 0.050445
Average total loss: 2.353026
tensor(-7.7209, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.4857e-09, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302604
Average KL loss: 0.049165
Average total loss: 2.351769
tensor(-7.7466, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.3331e-09, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302589
Average KL loss: 0.047922
Average total loss: 2.350510
tensor(-7.7722, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.2348e-09, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302573
Average KL loss: 0.046715
Average total loss: 2.349288
tensor(-7.7977, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.1195e-09, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302595
Average KL loss: 0.045543
Average total loss: 2.348139
tensor(-7.8230, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.0182e-09, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302574
Average KL loss: 0.044406
Average total loss: 2.346980
tensor(-7.8483, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.9239e-09, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302609
Average KL loss: 0.043300
Average total loss: 2.345909
tensor(-7.8735, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.8378e-09, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302574
Average KL loss: 0.042227
Average total loss: 2.344801
tensor(-7.8986, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.7157e-09, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302595
Average KL loss: 0.041184
Average total loss: 2.343779
tensor(-7.9236, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.6356e-09, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302577
Average KL loss: 0.040171
Average total loss: 2.342748
tensor(-7.9484, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.5467e-09, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302600
Average KL loss: 0.039186
Average total loss: 2.341786
tensor(-7.9732, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.4322e-09, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.038229
Average total loss: 2.340814
tensor(-7.9979, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.3758e-09, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.037299
Average total loss: 2.339883
tensor(-8.0225, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.2967e-09, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302586
Average KL loss: 0.036394
Average total loss: 2.338980
tensor(-8.0470, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.2144e-09, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302578
Average KL loss: 0.035515
Average total loss: 2.338093
tensor(-8.0715, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.1346e-09, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302598
Average KL loss: 0.034661
Average total loss: 2.337259
tensor(-8.0958, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.0511e-09, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302559
Average KL loss: 0.033830
Average total loss: 2.336389
tensor(-8.1200, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.9866e-09, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302595
Average KL loss: 0.033022
Average total loss: 2.335617
tensor(-8.1442, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.9103e-09, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302592
Average KL loss: 0.032237
Average total loss: 2.334829
tensor(-8.1682, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.8476e-09, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302577
Average KL loss: 0.031472
Average total loss: 2.334050
tensor(-8.1922, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.7667e-09, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302582
Average KL loss: 0.030729
Average total loss: 2.333311
tensor(-8.2160, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.7147e-09, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302596
Average KL loss: 0.030006
Average total loss: 2.332602
tensor(-8.2398, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.6663e-09, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302581
Average KL loss: 0.029303
Average total loss: 2.331883
tensor(-8.2635, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.5889e-09, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302578
Average KL loss: 0.028618
Average total loss: 2.331196
tensor(-8.2871, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.5303e-09, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302596
Average KL loss: 0.027952
Average total loss: 2.330548
tensor(-8.3106, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.4698e-09, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302570
Average KL loss: 0.027304
Average total loss: 2.329874
tensor(-8.3341, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.4135e-09, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.026673
Average total loss: 2.329258
tensor(-8.3574, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.3573e-09, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302578
Average KL loss: 0.026060
Average total loss: 2.328638
tensor(-8.3806, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.3075e-09, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302581
Average KL loss: 0.025463
Average total loss: 2.328044
tensor(-8.4038, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.2536e-09, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302589
Average KL loss: 0.024881
Average total loss: 2.327470
tensor(-8.4269, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.2050e-09, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302584
Average KL loss: 0.024315
Average total loss: 2.326899
tensor(-8.4499, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.1520e-09, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302587
Average KL loss: 0.023764
Average total loss: 2.326351
tensor(-8.4728, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.1074e-09, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.023227
Average total loss: 2.325813
tensor(-8.4956, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.0648e-09, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302601
Average KL loss: 0.022705
Average total loss: 2.325306
tensor(-8.5183, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.0010e-09, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302589
Average KL loss: 0.022196
Average total loss: 2.324785
tensor(-8.5409, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.9636e-09, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302596
Average KL loss: 0.021700
Average total loss: 2.324297
tensor(-8.5635, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.9177e-09, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302588
Average KL loss: 0.021218
Average total loss: 2.323805
tensor(-8.5859, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8731e-09, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302576
Average KL loss: 0.020747
Average total loss: 2.323324
tensor(-8.6083, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8417e-09, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302594
Average KL loss: 0.020289
Average total loss: 2.322883
tensor(-8.6306, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7938e-09, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302584
Average KL loss: 0.019843
Average total loss: 2.322427
tensor(-8.6528, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7546e-09, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.019409
Average total loss: 2.321993
tensor(-8.6749, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7273e-09, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302583
Average KL loss: 0.018985
Average total loss: 2.321568
tensor(-8.6969, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6768e-09, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302588
Average KL loss: 0.018573
Average total loss: 2.321161
tensor(-8.7188, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6425e-09, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302588
Average KL loss: 0.018171
Average total loss: 2.320759
tensor(-8.7407, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6063e-09, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302587
Average KL loss: 0.017779
Average total loss: 2.320366
tensor(-8.7624, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5717e-09, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302582
Average KL loss: 0.017398
Average total loss: 2.319979
tensor(-8.7841, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5380e-09, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302588
Average KL loss: 0.017026
Average total loss: 2.319613
tensor(-8.8056, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5074e-09, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302586
Average KL loss: 0.016663
Average total loss: 2.319249
tensor(-8.8271, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4712e-09, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302581
Average KL loss: 0.016310
Average total loss: 2.318891
tensor(-8.8485, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4463e-09, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.015966
Average total loss: 2.318551
tensor(-8.8698, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4124e-09, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302579
Average KL loss: 0.015630
Average total loss: 2.318209
tensor(-8.8910, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3877e-09, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302580
Average KL loss: 0.015303
Average total loss: 2.317883
tensor(-8.9121, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3556e-09, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302590
Average KL loss: 0.014984
Average total loss: 2.317574
tensor(-8.9331, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3258e-09, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302588
Average KL loss: 0.014673
Average total loss: 2.317261
tensor(-8.9540, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2987e-09, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302583
Average KL loss: 0.014370
Average total loss: 2.316953
tensor(-8.9748, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2714e-09, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302584
Average KL loss: 0.014074
Average total loss: 2.316658
tensor(-8.9956, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2442e-09, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302579
Average KL loss: 0.013786
Average total loss: 2.316366
tensor(-9.0162, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2220e-09, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302598
Average KL loss: 0.013505
Average total loss: 2.316103
tensor(-9.0367, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1933e-09, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 0.013231
Average total loss: 2.315818
tensor(-9.0572, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1701e-09, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302598
Average KL loss: 0.012964
Average total loss: 2.315562
tensor(-9.0775, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1399e-09, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302590
Average KL loss: 0.012703
Average total loss: 2.315294
tensor(-9.0978, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1217e-09, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302587
Average KL loss: 0.012449
Average total loss: 2.315036
tensor(-9.1179, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1020e-09, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302586
Average KL loss: 0.012201
Average total loss: 2.314787
tensor(-9.1380, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0738e-09, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302575
Average KL loss: 0.011959
Average total loss: 2.314534
tensor(-9.1580, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0588e-09, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302591
Average KL loss: 0.011724
Average total loss: 2.314315
tensor(-9.1778, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0379e-09, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302576
Average KL loss: 0.011493
Average total loss: 2.314070
tensor(-9.1976, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0178e-09, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302577
Average KL loss: 0.011269
Average total loss: 2.313846
tensor(-9.2173, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.9148e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.011050
Average total loss: 2.313635
tensor(-9.2368, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.7775e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.010836
Average total loss: 2.313421
tensor(-9.2563, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.5930e-10, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302584
Average KL loss: 0.010628
Average total loss: 2.313211
tensor(-9.2757, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.4086e-10, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302587
Average KL loss: 0.010424
Average total loss: 2.313012
tensor(-9.2950, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.2276e-10, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302573
Average KL loss: 0.010226
Average total loss: 2.312799
tensor(-9.3141, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.0605e-10, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302589
Average KL loss: 0.010032
Average total loss: 2.312620
tensor(-9.3332, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.9832e-10, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302591
Average KL loss: 0.009843
Average total loss: 2.312433
tensor(-9.3522, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.7271e-10, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302594
Average KL loss: 0.009658
Average total loss: 2.312252
tensor(-9.3710, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.5555e-10, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302591
Average KL loss: 0.009478
Average total loss: 2.312069
tensor(-9.3898, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.4659e-10, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302589
Average KL loss: 0.009302
Average total loss: 2.311890
tensor(-9.4085, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.2204e-10, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302590
Average KL loss: 0.009130
Average total loss: 2.311720
tensor(-9.4270, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.1093e-10, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302581
Average KL loss: 0.008962
Average total loss: 2.311543
tensor(-9.4455, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.9411e-10, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302592
Average KL loss: 0.008799
Average total loss: 2.311391
tensor(-9.4639, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.7965e-10, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302581
Average KL loss: 0.008639
Average total loss: 2.311220
tensor(-9.4821, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.6558e-10, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302590
Average KL loss: 0.008483
Average total loss: 2.311073
tensor(-9.5003, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.5167e-10, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302589
Average KL loss: 0.008331
Average total loss: 2.310920
tensor(-9.5183, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.3831e-10, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302584
Average KL loss: 0.008182
Average total loss: 2.310766
tensor(-9.5362, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.2516e-10, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302578
Average KL loss: 0.008037
Average total loss: 2.310615
tensor(-9.5541, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.1233e-10, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302587
Average KL loss: 0.007895
Average total loss: 2.310482
tensor(-9.5718, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.0022e-10, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302587
Average KL loss: 0.007756
Average total loss: 2.310344
tensor(-9.5894, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.8756e-10, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302584
Average KL loss: 0.007621
Average total loss: 2.310205
tensor(-9.6070, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.7572e-10, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302593
Average KL loss: 0.007489
Average total loss: 2.310082
tensor(-9.6244, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.5031e-10, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302584
Average KL loss: 0.007360
Average total loss: 2.309944
tensor(-9.6417, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.5251e-10, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302580
Average KL loss: 0.007234
Average total loss: 2.309813
 Percentile value: -9.659784317016602
Non-zero model percentage: 80.0%, Non-zero mask percentage: 80.0%

--- Pruning Level [1/24]: ---
conv1.weight         | nonzeros =    1663 /    1728             ( 96.24%) | total_pruned =      65 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
bn1.bias             | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   32208 /   36864             ( 87.37%) | total_pruned =    4656 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      54 /      64             ( 84.38%) | total_pruned =      10 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   26399 /   36864             ( 71.61%) | total_pruned =   10465 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   26612 /   36864             ( 72.19%) | total_pruned =   10252 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      54 /      64             ( 84.38%) | total_pruned =      10 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   24869 /   36864             ( 67.46%) | total_pruned =   11995 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   42012 /   73728             ( 56.98%) | total_pruned =   31716 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   86231 /  147456             ( 58.48%) | total_pruned =   61225 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    5013 /    8192             ( 61.19%) | total_pruned =    3179 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     104 /     128             ( 81.25%) | total_pruned =      24 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  114100 /  147456             ( 77.38%) | total_pruned =   33356 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  119542 /  147456             ( 81.07%) | total_pruned =   27914 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  171006 /  294912             ( 57.99%) | total_pruned =  123906 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     254 /     256             ( 99.22%) | total_pruned =       2 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  398343 /  589824             ( 67.54%) | total_pruned =  191481 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   21995 /   32768             ( 67.12%) | total_pruned =   10773 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     183 /     256             ( 71.48%) | total_pruned =      73 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  511287 /  589824             ( 86.68%) | total_pruned =   78537 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  532844 /  589824             ( 90.34%) | total_pruned =   56980 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  905166 / 1179648             ( 76.73%) | total_pruned =  274482 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     438 /     512             ( 85.55%) | total_pruned =      74 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     482 /     512             ( 94.14%) | total_pruned =      30 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1924203 / 2359296             ( 81.56%) | total_pruned =  435093 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     393 /     512             ( 76.76%) | total_pruned =     119 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     476 /     512             ( 92.97%) | total_pruned =      36 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  103876 /  131072             ( 79.25%) | total_pruned =   27196 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     282 /     512             ( 55.08%) | total_pruned =     230 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1935515 / 2359296             ( 82.04%) | total_pruned =  423781 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     379 /     512             ( 74.02%) | total_pruned =     133 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     405 /     512             ( 79.10%) | total_pruned =     107 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1942100 / 2359296             ( 82.32%) | total_pruned =  417196 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8943010, pruned : 2235752, total: 11178762, Compression rate :       1.25x  ( 20.00% pruned)
Train Epoch: 61/100 Loss: 0.024689 Accuracy: 88.39 100.00 % Best test Accuracy: 88.68%
tensor(-9.6589, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.4137e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302589
Average KL loss: 0.007064
Average total loss: 2.309653
tensor(-9.6881, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(6.2219e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302582
Average KL loss: 0.006857
Average total loss: 2.309439
tensor(-9.7164, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(6.0435e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302591
Average KL loss: 0.006664
Average total loss: 2.309255
tensor(-9.7440, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.8763e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302598
Average KL loss: 0.006482
Average total loss: 2.309080
tensor(-9.7709, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.7286e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302590
Average KL loss: 0.006311
Average total loss: 2.308901
tensor(-9.7971, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(5.5695e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302590
Average KL loss: 0.006148
Average total loss: 2.308738
tensor(-9.8226, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(5.4275e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302588
Average KL loss: 0.005994
Average total loss: 2.308582
tensor(-9.8476, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(5.2937e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302584
Average KL loss: 0.005848
Average total loss: 2.308431
tensor(-9.8719, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(5.1667e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302588
Average KL loss: 0.005708
Average total loss: 2.308296
tensor(-9.8956, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(4.9890e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.005575
Average total loss: 2.308160
tensor(-9.9189, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(4.9172e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302584
Average KL loss: 0.005448
Average total loss: 2.308032
tensor(-9.9416, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.8170e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302583
Average KL loss: 0.005327
Average total loss: 2.307909
tensor(-9.9638, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.7110e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302583
Average KL loss: 0.005211
Average total loss: 2.307794
tensor(-9.9855, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.6094e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302584
Average KL loss: 0.005099
Average total loss: 2.307684
tensor(-10.0068, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.5121e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302583
Average KL loss: 0.004993
Average total loss: 2.307576
tensor(-10.0277, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.4188e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302588
Average KL loss: 0.004891
Average total loss: 2.307478
tensor(-10.0482, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.3293e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302582
Average KL loss: 0.004792
Average total loss: 2.307375
tensor(-10.0682, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.1906e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.004698
Average total loss: 2.307283
tensor(-10.0879, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.1604e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302584
Average KL loss: 0.004607
Average total loss: 2.307191
tensor(-10.1072, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.0911e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302579
Average KL loss: 0.004520
Average total loss: 2.307099
tensor(-10.1261, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.0042e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302579
Average KL loss: 0.004436
Average total loss: 2.307014
tensor(-10.1447, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.9304e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302582
Average KL loss: 0.004355
Average total loss: 2.306937
tensor(-10.1630, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.8591e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302594
Average KL loss: 0.004276
Average total loss: 2.306870
tensor(-10.1809, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.7905e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302596
Average KL loss: 0.004201
Average total loss: 2.306796
tensor(-10.1986, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.7241e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302588
Average KL loss: 0.004128
Average total loss: 2.306716
tensor(-10.2159, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.6601e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302589
Average KL loss: 0.004058
Average total loss: 2.306646
tensor(-10.2330, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.5981e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302588
Average KL loss: 0.003989
Average total loss: 2.306577
tensor(-10.2498, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.5382e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302588
Average KL loss: 0.003924
Average total loss: 2.306511
tensor(-10.2663, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.4803e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302583
Average KL loss: 0.003860
Average total loss: 2.306443
tensor(-10.2825, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.4242e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302583
Average KL loss: 0.003798
Average total loss: 2.306381
tensor(-10.2985, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.3698e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302587
Average KL loss: 0.003738
Average total loss: 2.306325
tensor(-10.3143, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.3172e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302584
Average KL loss: 0.003680
Average total loss: 2.306264
tensor(-10.3298, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.2660e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302584
Average KL loss: 0.003624
Average total loss: 2.306208
tensor(-10.3451, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.2165e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302584
Average KL loss: 0.003569
Average total loss: 2.306154
tensor(-10.3601, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.1684e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302598
Average KL loss: 0.003516
Average total loss: 2.306114
tensor(-10.3750, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.1738e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302589
Average KL loss: 0.003465
Average total loss: 2.306053
tensor(-10.3896, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.0763e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.003415
Average total loss: 2.305999
tensor(-10.4040, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.0322e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302580
Average KL loss: 0.003366
Average total loss: 2.305947
tensor(-10.4183, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9895e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302577
Average KL loss: 0.003319
Average total loss: 2.305896
tensor(-10.4323, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9493e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.003273
Average total loss: 2.305858
tensor(-10.4461, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9071e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302582
Average KL loss: 0.003228
Average total loss: 2.305810
tensor(-10.4598, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.8678e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302581
Average KL loss: 0.003185
Average total loss: 2.305766
tensor(-10.4733, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.8265e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302583
Average KL loss: 0.003142
Average total loss: 2.305725
tensor(-10.4866, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7919e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302580
Average KL loss: 0.003101
Average total loss: 2.305681
tensor(-10.4997, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7555e-10, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302586
Average KL loss: 0.003061
Average total loss: 2.305647
tensor(-10.5127, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7200e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302582
Average KL loss: 0.003022
Average total loss: 2.305604
tensor(-10.5255, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6853e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.002983
Average total loss: 2.305568
tensor(-10.5382, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6721e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302580
Average KL loss: 0.002946
Average total loss: 2.305526
tensor(-10.5507, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6191e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302583
Average KL loss: 0.002910
Average total loss: 2.305493
tensor(-10.5630, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5865e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302589
Average KL loss: 0.002874
Average total loss: 2.305463
tensor(-10.5752, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5551e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302584
Average KL loss: 0.002839
Average total loss: 2.305424
tensor(-10.5873, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5244e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302580
Average KL loss: 0.002806
Average total loss: 2.305386
tensor(-10.5992, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4945e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302583
Average KL loss: 0.002773
Average total loss: 2.305356
tensor(-10.6110, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4653e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302583
Average KL loss: 0.002740
Average total loss: 2.305324
tensor(-10.6227, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4367e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.002709
Average total loss: 2.305293
tensor(-10.6342, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4088e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302599
Average KL loss: 0.002678
Average total loss: 2.305276
tensor(-10.6456, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3815e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302594
Average KL loss: 0.002648
Average total loss: 2.305241
tensor(-10.6568, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3548e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302578
Average KL loss: 0.002618
Average total loss: 2.305196
tensor(-10.6680, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3284e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302587
Average KL loss: 0.002589
Average total loss: 2.305176
tensor(-10.6790, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3031e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302588
Average KL loss: 0.002561
Average total loss: 2.305149
tensor(-10.6899, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2781e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302587
Average KL loss: 0.002533
Average total loss: 2.305120
tensor(-10.7007, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2537e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302587
Average KL loss: 0.002506
Average total loss: 2.305093
tensor(-10.7114, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2297e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302587
Average KL loss: 0.002480
Average total loss: 2.305066
tensor(-10.7220, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2063e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302582
Average KL loss: 0.002454
Average total loss: 2.305036
tensor(-10.7325, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1918e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.002428
Average total loss: 2.305013
tensor(-10.7428, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.5121e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302588
Average KL loss: 0.002403
Average total loss: 2.304992
tensor(-10.7531, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1363e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302588
Average KL loss: 0.002379
Average total loss: 2.304967
tensor(-10.7632, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1171e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.002355
Average total loss: 2.304940
tensor(-10.7733, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0959e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302587
Average KL loss: 0.002332
Average total loss: 2.304918
tensor(-10.7833, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0751e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302584
Average KL loss: 0.002309
Average total loss: 2.304893
tensor(-10.7931, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0218e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302590
Average KL loss: 0.002286
Average total loss: 2.304876
tensor(-10.8029, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0348e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302594
Average KL loss: 0.002264
Average total loss: 2.304858
tensor(-10.8126, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0152e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302582
Average KL loss: 0.002242
Average total loss: 2.304825
tensor(-10.8222, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9960e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.002221
Average total loss: 2.304806
tensor(-10.8317, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9771e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302582
Average KL loss: 0.002200
Average total loss: 2.304782
tensor(-10.8411, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9586e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.002179
Average total loss: 2.304765
tensor(-10.8504, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9404e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302577
Average KL loss: 0.002159
Average total loss: 2.304737
tensor(-10.8596, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9225e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302579
Average KL loss: 0.002140
Average total loss: 2.304719
tensor(-10.8688, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9050e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302585
Average KL loss: 0.002120
Average total loss: 2.304705
tensor(-10.8779, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9424e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.002101
Average total loss: 2.304686
tensor(-10.8869, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.8709e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302580
Average KL loss: 0.002082
Average total loss: 2.304662
tensor(-10.8958, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.6800e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302587
Average KL loss: 0.002064
Average total loss: 2.304650
tensor(-10.9047, device='cuda:0') tensor(9.9464e-05, device='cuda:0') tensor(1.8379e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302587
Average KL loss: 0.002054
Average total loss: 2.304641
tensor(-10.9055, device='cuda:0') tensor(9.9323e-05, device='cuda:0') tensor(1.8363e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302590
Average KL loss: 0.002052
Average total loss: 2.304642
tensor(-10.9064, device='cuda:0') tensor(9.9167e-05, device='cuda:0') tensor(1.8346e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302598
Average KL loss: 0.002050
Average total loss: 2.304648
tensor(-10.9073, device='cuda:0') tensor(9.9000e-05, device='cuda:0') tensor(1.8331e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302590
Average KL loss: 0.002048
Average total loss: 2.304638
tensor(-10.9082, device='cuda:0') tensor(9.8851e-05, device='cuda:0') tensor(1.8314e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302590
Average KL loss: 0.002046
Average total loss: 2.304637
tensor(-10.9091, device='cuda:0') tensor(9.8724e-05, device='cuda:0') tensor(1.8298e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302583
Average KL loss: 0.002045
Average total loss: 2.304628
tensor(-10.9100, device='cuda:0') tensor(9.8603e-05, device='cuda:0') tensor(1.8281e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302583
Average KL loss: 0.002043
Average total loss: 2.304626
tensor(-10.9109, device='cuda:0') tensor(9.8486e-05, device='cuda:0') tensor(1.8265e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302588
Average KL loss: 0.002041
Average total loss: 2.304629
tensor(-10.9118, device='cuda:0') tensor(9.8359e-05, device='cuda:0') tensor(1.8249e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.002039
Average total loss: 2.304624
tensor(-10.9127, device='cuda:0') tensor(9.8231e-05, device='cuda:0') tensor(1.8233e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.002037
Average total loss: 2.304623
tensor(-10.9135, device='cuda:0') tensor(9.8106e-05, device='cuda:0') tensor(1.8216e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302578
Average KL loss: 0.002036
Average total loss: 2.304613
tensor(-10.9144, device='cuda:0') tensor(9.8004e-05, device='cuda:0') tensor(1.8200e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302582
Average KL loss: 0.002034
Average total loss: 2.304616
tensor(-10.9153, device='cuda:0') tensor(9.7888e-05, device='cuda:0') tensor(1.8184e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302583
Average KL loss: 0.002033
Average total loss: 2.304616
tensor(-10.9154, device='cuda:0') tensor(9.7876e-05, device='cuda:0') tensor(1.8177e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302586
Average KL loss: 0.002033
Average total loss: 2.304618
tensor(-10.9155, device='cuda:0') tensor(9.7865e-05, device='cuda:0') tensor(1.8181e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302587
Average KL loss: 0.002032
Average total loss: 2.304620
tensor(-10.9156, device='cuda:0') tensor(9.7853e-05, device='cuda:0') tensor(1.8179e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302586
Average KL loss: 0.002032
Average total loss: 2.304618
tensor(-10.9157, device='cuda:0') tensor(9.7840e-05, device='cuda:0') tensor(1.8177e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302590
Average KL loss: 0.002032
Average total loss: 2.304622
tensor(-10.9158, device='cuda:0') tensor(9.7829e-05, device='cuda:0') tensor(1.8176e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302584
Average KL loss: 0.002032
Average total loss: 2.304616
tensor(-10.9159, device='cuda:0') tensor(9.7819e-05, device='cuda:0') tensor(1.8174e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302588
Average KL loss: 0.002032
Average total loss: 2.304619
tensor(-10.9160, device='cuda:0') tensor(9.7806e-05, device='cuda:0') tensor(1.8170e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302587
Average KL loss: 0.002031
Average total loss: 2.304619
tensor(-10.9161, device='cuda:0') tensor(9.7797e-05, device='cuda:0') tensor(1.8171e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302584
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-10.9162, device='cuda:0') tensor(9.7787e-05, device='cuda:0') tensor(1.8169e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-10.9162, device='cuda:0') tensor(9.7777e-05, device='cuda:0') tensor(1.8774e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302586
Average KL loss: 0.002031
Average total loss: 2.304617
tensor(-10.9163, device='cuda:0') tensor(9.7766e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302590
Average KL loss: 0.002031
Average total loss: 2.304621
tensor(-10.9163, device='cuda:0') tensor(9.7766e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302586
Average KL loss: 0.002031
Average total loss: 2.304617
tensor(-10.9163, device='cuda:0') tensor(9.7765e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302578
Average KL loss: 0.002031
Average total loss: 2.304609
tensor(-10.9163, device='cuda:0') tensor(9.7765e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-10.9163, device='cuda:0') tensor(9.7764e-05, device='cuda:0') tensor(1.8165e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302583
Average KL loss: 0.002031
Average total loss: 2.304614
tensor(-10.9163, device='cuda:0') tensor(9.7764e-05, device='cuda:0') tensor(1.7801e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302588
Average KL loss: 0.002031
Average total loss: 2.304618
tensor(-10.9163, device='cuda:0') tensor(9.7764e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-10.9163, device='cuda:0') tensor(9.7763e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302578
Average KL loss: 0.002031
Average total loss: 2.304608
tensor(-10.9163, device='cuda:0') tensor(9.7763e-05, device='cuda:0') tensor(1.8173e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302584
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-10.9163, device='cuda:0') tensor(9.7762e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302587
Average KL loss: 0.002031
Average total loss: 2.304618
tensor(-10.9163, device='cuda:0') tensor(9.7762e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
 Percentile value: -10.916531562805176
Non-zero model percentage: 64.0%, Non-zero mask percentage: 64.0%

--- Pruning Level [2/24]: ---
conv1.weight         | nonzeros =     553 /    1728             ( 32.00%) | total_pruned =    1175 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    6856 /   36864             ( 18.60%) | total_pruned =   30008 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      54 /      64             ( 84.38%) | total_pruned =      10 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   23694 /   36864             ( 64.27%) | total_pruned =   13170 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   23685 /   36864             ( 64.25%) | total_pruned =   13179 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      53 /      64             ( 82.81%) | total_pruned =      11 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   21248 /   36864             ( 57.64%) | total_pruned =   15616 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   37386 /   73728             ( 50.71%) | total_pruned =   36342 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   75021 /  147456             ( 50.88%) | total_pruned =   72435 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    4524 /    8192             ( 55.22%) | total_pruned =    3668 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   89458 /  147456             ( 60.67%) | total_pruned =   57998 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   93920 /  147456             ( 63.69%) | total_pruned =   53536 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  152058 /  294912             ( 51.56%) | total_pruned =  142854 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     254 /     256             ( 99.22%) | total_pruned =       2 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  321544 /  589824             ( 54.52%) | total_pruned =  268280 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   17943 /   32768             ( 54.76%) | total_pruned =   14825 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  400968 /  589824             ( 67.98%) | total_pruned =  188856 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  437297 /  589824             ( 74.14%) | total_pruned =  152527 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  714890 / 1179648             ( 60.60%) | total_pruned =  464758 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     438 /     512             ( 85.55%) | total_pruned =      74 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     481 /     512             ( 93.95%) | total_pruned =      31 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1551768 / 2359296             ( 65.77%) | total_pruned =  807528 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     392 /     512             ( 76.56%) | total_pruned =     120 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   82953 /  131072             ( 63.29%) | total_pruned =   48119 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1557251 / 2359296             ( 66.00%) | total_pruned =  802045 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1523534 / 2359296             ( 64.58%) | total_pruned =  835762 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 7154408, pruned : 4024354, total: 11178762, Compression rate :       1.56x  ( 36.00% pruned)
Train Epoch: 43/100 Loss: 0.020984 Accuracy: 87.20 100.00 % Best test Accuracy: 87.25%
tensor(-10.9163, device='cuda:0') tensor(9.7761e-05, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302584
Average KL loss: 0.002022
Average total loss: 2.304606
tensor(-10.9251, device='cuda:0') tensor(9.3371e-05, device='cuda:0') tensor(1.8007e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.002004
Average total loss: 2.304590
tensor(-10.9337, device='cuda:0') tensor(9.0030e-05, device='cuda:0') tensor(1.7851e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302583
Average KL loss: 0.001987
Average total loss: 2.304570
tensor(-10.9423, device='cuda:0') tensor(8.7327e-05, device='cuda:0') tensor(1.7699e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001970
Average total loss: 2.304555
tensor(-10.9508, device='cuda:0') tensor(8.4681e-05, device='cuda:0') tensor(1.7549e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302586
Average KL loss: 0.001954
Average total loss: 2.304539
tensor(-10.9592, device='cuda:0') tensor(8.2441e-05, device='cuda:0') tensor(1.7401e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302588
Average KL loss: 0.001937
Average total loss: 2.304525
tensor(-10.9675, device='cuda:0') tensor(8.0436e-05, device='cuda:0') tensor(1.7257e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302577
Average KL loss: 0.001921
Average total loss: 2.304498
tensor(-10.9758, device='cuda:0') tensor(7.8990e-05, device='cuda:0') tensor(1.7114e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302587
Average KL loss: 0.001905
Average total loss: 2.304492
tensor(-10.9840, device='cuda:0') tensor(7.7256e-05, device='cuda:0') tensor(1.6974e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302588
Average KL loss: 0.001890
Average total loss: 2.304478
tensor(-10.9922, device='cuda:0') tensor(7.5541e-05, device='cuda:0') tensor(1.6836e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001875
Average total loss: 2.304460
tensor(-11.0003, device='cuda:0') tensor(7.3830e-05, device='cuda:0') tensor(1.6700e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302591
Average KL loss: 0.001860
Average total loss: 2.304450
tensor(-11.0083, device='cuda:0') tensor(7.2416e-05, device='cuda:0') tensor(1.6566e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302586
Average KL loss: 0.001845
Average total loss: 2.304431
tensor(-11.0163, device='cuda:0') tensor(7.0977e-05, device='cuda:0') tensor(1.6437e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001837
Average total loss: 2.304422
tensor(-11.0171, device='cuda:0') tensor(7.0852e-05, device='cuda:0') tensor(1.6422e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304420
tensor(-11.0179, device='cuda:0') tensor(7.0722e-05, device='cuda:0') tensor(1.6536e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302586
Average KL loss: 0.001834
Average total loss: 2.304420
tensor(-11.0187, device='cuda:0') tensor(7.0566e-05, device='cuda:0') tensor(1.6396e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001832
Average total loss: 2.304417
tensor(-11.0194, device='cuda:0') tensor(7.0424e-05, device='cuda:0') tensor(1.6383e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302583
Average KL loss: 0.001831
Average total loss: 2.304413
tensor(-11.0202, device='cuda:0') tensor(7.0296e-05, device='cuda:0') tensor(1.6370e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302587
Average KL loss: 0.001829
Average total loss: 2.304416
tensor(-11.0210, device='cuda:0') tensor(7.0145e-05, device='cuda:0') tensor(1.6357e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302587
Average KL loss: 0.001828
Average total loss: 2.304415
tensor(-11.0218, device='cuda:0') tensor(6.9989e-05, device='cuda:0') tensor(1.6344e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302583
Average KL loss: 0.001826
Average total loss: 2.304410
tensor(-11.0226, device='cuda:0') tensor(6.9873e-05, device='cuda:0') tensor(1.6333e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001825
Average total loss: 2.304410
tensor(-11.0234, device='cuda:0') tensor(6.9746e-05, device='cuda:0') tensor(1.6318e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302586
Average KL loss: 0.001823
Average total loss: 2.304409
tensor(-11.0242, device='cuda:0') tensor(6.9631e-05, device='cuda:0') tensor(1.6291e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302584
Average KL loss: 0.001822
Average total loss: 2.304406
tensor(-11.0250, device='cuda:0') tensor(6.9501e-05, device='cuda:0') tensor(1.6292e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302584
Average KL loss: 0.001821
Average total loss: 2.304405
tensor(-11.0251, device='cuda:0') tensor(6.9494e-05, device='cuda:0') tensor(1.6290e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302588
Average KL loss: 0.001821
Average total loss: 2.304409
tensor(-11.0252, device='cuda:0') tensor(6.9486e-05, device='cuda:0') tensor(1.6289e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302591
Average KL loss: 0.001821
Average total loss: 2.304412
tensor(-11.0253, device='cuda:0') tensor(6.9476e-05, device='cuda:0') tensor(1.6287e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304405
tensor(-11.0254, device='cuda:0') tensor(6.9468e-05, device='cuda:0') tensor(1.7465e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302587
Average KL loss: 0.001821
Average total loss: 2.304408
tensor(-11.0255, device='cuda:0') tensor(6.9456e-05, device='cuda:0') tensor(1.6330e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302584
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-11.0256, device='cuda:0') tensor(6.9447e-05, device='cuda:0') tensor(1.6283e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302581
Average KL loss: 0.001820
Average total loss: 2.304401
tensor(-11.0257, device='cuda:0') tensor(6.9440e-05, device='cuda:0') tensor(1.6281e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-11.0258, device='cuda:0') tensor(6.9432e-05, device='cuda:0') tensor(1.6280e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302587
Average KL loss: 0.001820
Average total loss: 2.304406
tensor(-11.0259, device='cuda:0') tensor(6.9425e-05, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302581
Average KL loss: 0.001820
Average total loss: 2.304400
tensor(-11.0259, device='cuda:0') tensor(6.9416e-05, device='cuda:0') tensor(1.6277e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302588
Average KL loss: 0.001819
Average total loss: 2.304407
tensor(-11.0260, device='cuda:0') tensor(6.9409e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001819
Average total loss: 2.304404
tensor(-11.0260, device='cuda:0') tensor(6.9408e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302587
Average KL loss: 0.001819
Average total loss: 2.304407
tensor(-11.0260, device='cuda:0') tensor(6.9407e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302584
Average KL loss: 0.001819
Average total loss: 2.304404
tensor(-11.0260, device='cuda:0') tensor(6.9407e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302582
Average KL loss: 0.001819
Average total loss: 2.304401
tensor(-11.0260, device='cuda:0') tensor(6.9406e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302584
Average KL loss: 0.001819
Average total loss: 2.304403
tensor(-11.0260, device='cuda:0') tensor(6.9406e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302582
Average KL loss: 0.001819
Average total loss: 2.304402
tensor(-11.0260, device='cuda:0') tensor(6.9405e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302581
Average KL loss: 0.001819
Average total loss: 2.304401
tensor(-11.0260, device='cuda:0') tensor(6.9405e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302588
Average KL loss: 0.001819
Average total loss: 2.304407
tensor(-11.0260, device='cuda:0') tensor(6.9405e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001819
Average total loss: 2.304404
tensor(-11.0260, device='cuda:0') tensor(6.9404e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302587
Average KL loss: 0.001819
Average total loss: 2.304406
tensor(-11.0260, device='cuda:0') tensor(6.9404e-05, device='cuda:0') tensor(1.6279e-10, device='cuda:0')
 Percentile value: -11.026204109191895
Non-zero model percentage: 51.200008392333984%, Non-zero mask percentage: 51.200008392333984%

--- Pruning Level [3/24]: ---
conv1.weight         | nonzeros =     553 /    1728             ( 32.00%) | total_pruned =    1175 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5680 /   36864             ( 15.41%) | total_pruned =   31184 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   13059 /   36864             ( 35.42%) | total_pruned =   23805 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   13748 /   36864             ( 37.29%) | total_pruned =   23116 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   15239 /   36864             ( 41.34%) | total_pruned =   21625 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   34346 /   73728             ( 46.58%) | total_pruned =   39382 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   69104 /  147456             ( 46.86%) | total_pruned =   78352 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    4053 /    8192             ( 49.48%) | total_pruned =    4139 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   56072 /  147456             ( 38.03%) | total_pruned =   91384 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   50464 /  147456             ( 34.22%) | total_pruned =   96992 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  141473 /  294912             ( 47.97%) | total_pruned =  153439 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  255717 /  589824             ( 43.35%) | total_pruned =  334107 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   14054 /   32768             ( 42.89%) | total_pruned =   18714 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  174205 /  589824             ( 29.54%) | total_pruned =  415619 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  138459 /  589824             ( 23.47%) | total_pruned =  451365 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  436063 / 1179648             ( 36.97%) | total_pruned =  743585 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1122395 / 2359296             ( 47.57%) | total_pruned = 1236901 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     392 /     512             ( 76.56%) | total_pruned =     120 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   82953 /  131072             ( 63.29%) | total_pruned =   48119 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1557251 / 2359296             ( 66.00%) | total_pruned =  802045 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1523534 / 2359296             ( 64.58%) | total_pruned =  835762 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 5723527, pruned : 5455235, total: 11178762, Compression rate :       1.95x  ( 48.80% pruned)
Train Epoch: 87/100 Loss: 0.015889 Accuracy: 87.34 100.00 % Best test Accuracy: 87.44%
tensor(-11.0260, device='cuda:0') tensor(6.9403e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302586
Average KL loss: 0.001812
Average total loss: 2.304398
tensor(-11.0339, device='cuda:0') tensor(6.7066e-05, device='cuda:0') tensor(1.5716e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302583
Average KL loss: 0.001798
Average total loss: 2.304381
tensor(-11.0416, device='cuda:0') tensor(6.5410e-05, device='cuda:0') tensor(1.6023e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302586
Average KL loss: 0.001784
Average total loss: 2.304370
tensor(-11.0493, device='cuda:0') tensor(6.3755e-05, device='cuda:0') tensor(1.5900e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001771
Average total loss: 2.304356
tensor(-11.0570, device='cuda:0') tensor(6.2424e-05, device='cuda:0') tensor(1.5778e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302584
Average KL loss: 0.001757
Average total loss: 2.304342
tensor(-11.0646, device='cuda:0') tensor(6.1574e-05, device='cuda:0') tensor(1.5650e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001744
Average total loss: 2.304329
tensor(-11.0721, device='cuda:0') tensor(6.0526e-05, device='cuda:0') tensor(1.5542e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001731
Average total loss: 2.304316
tensor(-11.0796, device='cuda:0') tensor(5.9521e-05, device='cuda:0') tensor(1.5426e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302584
Average KL loss: 0.001718
Average total loss: 2.304302
tensor(-11.0870, device='cuda:0') tensor(5.8520e-05, device='cuda:0') tensor(1.5312e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302583
Average KL loss: 0.001706
Average total loss: 2.304288
tensor(-11.0944, device='cuda:0') tensor(5.7620e-05, device='cuda:0') tensor(1.5199e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302584
Average KL loss: 0.001693
Average total loss: 2.304277
tensor(-11.1017, device='cuda:0') tensor(5.6912e-05, device='cuda:0') tensor(1.5089e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001681
Average total loss: 2.304266
tensor(-11.1090, device='cuda:0') tensor(5.6125e-05, device='cuda:0') tensor(1.4979e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302588
Average KL loss: 0.001669
Average total loss: 2.304256
tensor(-11.1162, device='cuda:0') tensor(5.5525e-05, device='cuda:0') tensor(1.4872e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302584
Average KL loss: 0.001662
Average total loss: 2.304246
tensor(-11.1169, device='cuda:0') tensor(5.5444e-05, device='cuda:0') tensor(1.4847e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1176, device='cuda:0') tensor(5.5381e-05, device='cuda:0') tensor(1.4834e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302586
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1183, device='cuda:0') tensor(5.5286e-05, device='cuda:0') tensor(1.4841e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302582
Average KL loss: 0.001658
Average total loss: 2.304240
tensor(-11.1190, device='cuda:0') tensor(5.5197e-05, device='cuda:0') tensor(1.4830e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302583
Average KL loss: 0.001657
Average total loss: 2.304241
tensor(-11.1197, device='cuda:0') tensor(5.5133e-05, device='cuda:0') tensor(1.4820e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001656
Average total loss: 2.304241
tensor(-11.1204, device='cuda:0') tensor(5.5055e-05, device='cuda:0') tensor(1.4810e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001655
Average total loss: 2.304240
tensor(-11.1211, device='cuda:0') tensor(5.4987e-05, device='cuda:0') tensor(1.4799e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302583
Average KL loss: 0.001654
Average total loss: 2.304236
tensor(-11.1218, device='cuda:0') tensor(5.4919e-05, device='cuda:0') tensor(1.4789e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302581
Average KL loss: 0.001653
Average total loss: 2.304233
tensor(-11.1225, device='cuda:0') tensor(5.4843e-05, device='cuda:0') tensor(1.4778e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302584
Average KL loss: 0.001652
Average total loss: 2.304236
tensor(-11.1232, device='cuda:0') tensor(5.4782e-05, device='cuda:0') tensor(1.4768e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304235
tensor(-11.1239, device='cuda:0') tensor(5.4682e-05, device='cuda:0') tensor(1.4758e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304235
tensor(-11.1240, device='cuda:0') tensor(5.4680e-05, device='cuda:0') tensor(1.4756e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304234
tensor(-11.1241, device='cuda:0') tensor(5.4677e-05, device='cuda:0') tensor(1.4755e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-11.1242, device='cuda:0') tensor(5.4675e-05, device='cuda:0') tensor(1.4754e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-11.1243, device='cuda:0') tensor(5.4672e-05, device='cuda:0') tensor(1.4752e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302584
Average KL loss: 0.001649
Average total loss: 2.304233
tensor(-11.1243, device='cuda:0') tensor(5.4668e-05, device='cuda:0') tensor(1.4753e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-11.1244, device='cuda:0') tensor(5.4665e-05, device='cuda:0') tensor(1.4749e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-11.1245, device='cuda:0') tensor(5.4664e-05, device='cuda:0') tensor(1.4748e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-11.1246, device='cuda:0') tensor(5.4660e-05, device='cuda:0') tensor(1.4747e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1247, device='cuda:0') tensor(5.4659e-05, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304234
tensor(-11.1248, device='cuda:0') tensor(5.4655e-05, device='cuda:0') tensor(1.4744e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302586
Average KL loss: 0.001648
Average total loss: 2.304234
tensor(-11.1249, device='cuda:0') tensor(5.4651e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302582
Average KL loss: 0.001648
Average total loss: 2.304230
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4650e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4649e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4649e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302583
Average KL loss: 0.001648
Average total loss: 2.304231
tensor(-11.1249, device='cuda:0') tensor(5.4649e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-11.1249, device='cuda:0') tensor(5.4649e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
 Percentile value: -11.1250581741333
Non-zero model percentage: 40.96000671386719%, Non-zero mask percentage: 40.96000671386719%

--- Pruning Level [4/24]: ---
conv1.weight         | nonzeros =     553 /    1728             ( 32.00%) | total_pruned =    1175 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5680 /   36864             ( 15.41%) | total_pruned =   31184 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   13059 /   36864             ( 35.42%) | total_pruned =   23805 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   13748 /   36864             ( 37.29%) | total_pruned =   23116 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   15239 /   36864             ( 41.34%) | total_pruned =   21625 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   34346 /   73728             ( 46.58%) | total_pruned =   39382 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   69104 /  147456             ( 46.86%) | total_pruned =   78352 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    4053 /    8192             ( 49.48%) | total_pruned =    4139 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   56072 /  147456             ( 38.03%) | total_pruned =   91384 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   50464 /  147456             ( 34.22%) | total_pruned =   96992 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  141473 /  294912             ( 47.97%) | total_pruned =  153439 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  255717 /  589824             ( 43.35%) | total_pruned =  334107 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   14054 /   32768             ( 42.89%) | total_pruned =   18714 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  174205 /  589824             ( 29.54%) | total_pruned =  415619 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  138459 /  589824             ( 23.47%) | total_pruned =  451365 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  436063 / 1179648             ( 36.97%) | total_pruned =  743585 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  750306 / 2359296             ( 31.80%) | total_pruned = 1608990 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   43343 /  131072             ( 33.07%) | total_pruned =   87729 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  825271 / 2359296             ( 34.98%) | total_pruned = 1534025 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1523534 / 2359296             ( 64.58%) | total_pruned =  835762 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 4578822, pruned : 6599940, total: 11178762, Compression rate :       2.44x  ( 59.04% pruned)
Train Epoch: 49/100 Loss: 0.014948 Accuracy: 86.42 100.00 % Best test Accuracy: 86.42%
tensor(-11.1249, device='cuda:0') tensor(5.4648e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302588
Average KL loss: 0.001642
Average total loss: 2.304230
tensor(-11.1320, device='cuda:0') tensor(5.2835e-05, device='cuda:0') tensor(1.4638e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302587
Average KL loss: 0.001631
Average total loss: 2.304217
tensor(-11.1391, device='cuda:0') tensor(5.1492e-05, device='cuda:0') tensor(1.4535e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302587
Average KL loss: 0.001619
Average total loss: 2.304206
tensor(-11.1461, device='cuda:0') tensor(5.0451e-05, device='cuda:0') tensor(1.4433e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001608
Average total loss: 2.304193
tensor(-11.1530, device='cuda:0') tensor(4.9527e-05, device='cuda:0') tensor(1.4333e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001597
Average total loss: 2.304181
tensor(-11.1599, device='cuda:0') tensor(4.8652e-05, device='cuda:0') tensor(1.4235e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001586
Average total loss: 2.304171
tensor(-11.1668, device='cuda:0') tensor(4.7717e-05, device='cuda:0') tensor(1.4137e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001575
Average total loss: 2.304160
tensor(-11.1736, device='cuda:0') tensor(4.6938e-05, device='cuda:0') tensor(1.4041e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302586
Average KL loss: 0.001564
Average total loss: 2.304150
tensor(-11.1804, device='cuda:0') tensor(4.6143e-05, device='cuda:0') tensor(1.3946e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302586
Average KL loss: 0.001554
Average total loss: 2.304140
tensor(-11.1871, device='cuda:0') tensor(4.5424e-05, device='cuda:0') tensor(1.3901e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001544
Average total loss: 2.304129
tensor(-11.1938, device='cuda:0') tensor(4.4652e-05, device='cuda:0') tensor(1.3761e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302582
Average KL loss: 0.001533
Average total loss: 2.304116
tensor(-11.2004, device='cuda:0') tensor(4.4152e-05, device='cuda:0') tensor(1.3670e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-11.2070, device='cuda:0') tensor(4.3554e-05, device='cuda:0') tensor(1.3580e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302589
Average KL loss: 0.001518
Average total loss: 2.304107
tensor(-11.2077, device='cuda:0') tensor(4.3494e-05, device='cuda:0') tensor(1.3571e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2083, device='cuda:0') tensor(4.3434e-05, device='cuda:0') tensor(1.3562e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001516
Average total loss: 2.304100
tensor(-11.2090, device='cuda:0') tensor(4.3378e-05, device='cuda:0') tensor(1.3553e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001515
Average total loss: 2.304100
tensor(-11.2096, device='cuda:0') tensor(4.3320e-05, device='cuda:0') tensor(1.3544e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302588
Average KL loss: 0.001514
Average total loss: 2.304102
tensor(-11.2103, device='cuda:0') tensor(4.3261e-05, device='cuda:0') tensor(1.3536e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302584
Average KL loss: 0.001513
Average total loss: 2.304097
tensor(-11.2109, device='cuda:0') tensor(4.3222e-05, device='cuda:0') tensor(1.3527e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302582
Average KL loss: 0.001512
Average total loss: 2.304094
tensor(-11.2116, device='cuda:0') tensor(4.3167e-05, device='cuda:0') tensor(1.3518e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302584
Average KL loss: 0.001511
Average total loss: 2.304095
tensor(-11.2122, device='cuda:0') tensor(4.3126e-05, device='cuda:0') tensor(1.3509e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001510
Average total loss: 2.304095
tensor(-11.2129, device='cuda:0') tensor(4.3066e-05, device='cuda:0') tensor(1.3500e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001509
Average total loss: 2.304094
tensor(-11.2135, device='cuda:0') tensor(4.3012e-05, device='cuda:0') tensor(1.3491e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001508
Average total loss: 2.304093
tensor(-11.2142, device='cuda:0') tensor(4.2958e-05, device='cuda:0') tensor(1.3483e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-11.2142, device='cuda:0') tensor(4.2947e-05, device='cuda:0') tensor(1.3482e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-11.2143, device='cuda:0') tensor(4.2936e-05, device='cuda:0') tensor(1.3481e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302590
Average KL loss: 0.001507
Average total loss: 2.304097
tensor(-11.2143, device='cuda:0') tensor(4.2925e-05, device='cuda:0') tensor(1.3481e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-11.2144, device='cuda:0') tensor(4.2912e-05, device='cuda:0') tensor(1.3480e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-11.2144, device='cuda:0') tensor(4.2901e-05, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302578
Average KL loss: 0.001507
Average total loss: 2.304085
tensor(-11.2145, device='cuda:0') tensor(4.2892e-05, device='cuda:0') tensor(1.3260e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302584
Average KL loss: 0.001507
Average total loss: 2.304091
tensor(-11.2145, device='cuda:0') tensor(4.2881e-05, device='cuda:0') tensor(1.3478e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302583
Average KL loss: 0.001507
Average total loss: 2.304089
tensor(-11.2146, device='cuda:0') tensor(4.2870e-05, device='cuda:0') tensor(1.3477e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304091
tensor(-11.2146, device='cuda:0') tensor(4.2859e-05, device='cuda:0') tensor(1.3477e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302586
Average KL loss: 0.001507
Average total loss: 2.304093
tensor(-11.2147, device='cuda:0') tensor(4.2847e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302586
Average KL loss: 0.001506
Average total loss: 2.304093
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304092
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302583
Average KL loss: 0.001506
Average total loss: 2.304089
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302586
Average KL loss: 0.001506
Average total loss: 2.304093
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302584
Average KL loss: 0.001506
Average total loss: 2.304090
tensor(-11.2147, device='cuda:0') tensor(4.2832e-05, device='cuda:0') tensor(1.3382e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304092
tensor(-11.2147, device='cuda:0') tensor(4.2831e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302586
Average KL loss: 0.001506
Average total loss: 2.304092
tensor(-11.2147, device='cuda:0') tensor(4.2831e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
 Percentile value: -11.214844703674316
Non-zero model percentage: 32.76801300048828%, Non-zero mask percentage: 32.76801300048828%

--- Pruning Level [5/24]: ---
conv1.weight         | nonzeros =     551 /    1728             ( 31.89%) | total_pruned =    1177 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5578 /   36864             ( 15.13%) | total_pruned =   31286 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   12574 /   36864             ( 34.11%) | total_pruned =   24290 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   13205 /   36864             ( 35.82%) | total_pruned =   23659 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   14543 /   36864             ( 39.45%) | total_pruned =   22321 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   33661 /   73728             ( 45.66%) | total_pruned =   40067 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   67494 /  147456             ( 45.77%) | total_pruned =   79962 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3976 /    8192             ( 48.54%) | total_pruned =    4216 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   51092 /  147456             ( 34.65%) | total_pruned =   96364 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   45151 /  147456             ( 30.62%) | total_pruned =  102305 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  138902 /  294912             ( 47.10%) | total_pruned =  156010 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  242314 /  589824             ( 41.08%) | total_pruned =  347510 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   13308 /   32768             ( 40.61%) | total_pruned =   19460 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  161071 /  589824             ( 27.31%) | total_pruned =  428753 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  138459 /  589824             ( 23.47%) | total_pruned =  451365 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  436063 / 1179648             ( 36.97%) | total_pruned =  743585 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  750306 / 2359296             ( 31.80%) | total_pruned = 1608990 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   43343 /  131072             ( 33.07%) | total_pruned =   87729 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  727614 / 2359296             ( 30.84%) | total_pruned = 1631682 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  750799 / 2359296             ( 31.82%) | total_pruned = 1608497 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3663058, pruned : 7515704, total: 11178762, Compression rate :       3.05x  ( 67.23% pruned)
Train Epoch: 54/100 Loss: 0.017911 Accuracy: 84.32 100.00 % Best test Accuracy: 84.81%
tensor(-11.2147, device='cuda:0') tensor(4.2831e-05, device='cuda:0') tensor(1.3476e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001502
Average total loss: 2.304087
tensor(-11.2212, device='cuda:0') tensor(4.1548e-05, device='cuda:0') tensor(1.3388e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302584
Average KL loss: 0.001492
Average total loss: 2.304076
tensor(-11.2277, device='cuda:0') tensor(4.0710e-05, device='cuda:0') tensor(1.3302e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001482
Average total loss: 2.304068
tensor(-11.2341, device='cuda:0') tensor(4.0009e-05, device='cuda:0') tensor(1.3217e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302586
Average KL loss: 0.001473
Average total loss: 2.304059
tensor(-11.2405, device='cuda:0') tensor(3.9319e-05, device='cuda:0') tensor(1.3133e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001464
Average total loss: 2.304048
tensor(-11.2468, device='cuda:0') tensor(3.8671e-05, device='cuda:0') tensor(1.3050e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001454
Average total loss: 2.304039
tensor(-11.2531, device='cuda:0') tensor(3.8119e-05, device='cuda:0') tensor(1.2968e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302584
Average KL loss: 0.001445
Average total loss: 2.304029
tensor(-11.2594, device='cuda:0') tensor(3.7608e-05, device='cuda:0') tensor(1.2887e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302584
Average KL loss: 0.001436
Average total loss: 2.304020
tensor(-11.2656, device='cuda:0') tensor(3.7203e-05, device='cuda:0') tensor(1.2807e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302584
Average KL loss: 0.001427
Average total loss: 2.304011
tensor(-11.2718, device='cuda:0') tensor(3.6692e-05, device='cuda:0') tensor(1.2728e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302583
Average KL loss: 0.001419
Average total loss: 2.304002
tensor(-11.2779, device='cuda:0') tensor(3.6085e-05, device='cuda:0') tensor(1.2650e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302590
Average KL loss: 0.001410
Average total loss: 2.304000
tensor(-11.2840, device='cuda:0') tensor(3.5491e-05, device='cuda:0') tensor(1.2573e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-11.2901, device='cuda:0') tensor(3.5015e-05, device='cuda:0') tensor(1.2497e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302586
Average KL loss: 0.001397
Average total loss: 2.303983
tensor(-11.2907, device='cuda:0') tensor(3.4970e-05, device='cuda:0') tensor(1.2489e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2913, device='cuda:0') tensor(3.4926e-05, device='cuda:0') tensor(1.2482e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302586
Average KL loss: 0.001395
Average total loss: 2.303981
tensor(-11.2919, device='cuda:0') tensor(3.4864e-05, device='cuda:0') tensor(1.2474e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001394
Average total loss: 2.303979
tensor(-11.2925, device='cuda:0') tensor(3.4816e-05, device='cuda:0') tensor(1.2466e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001393
Average total loss: 2.303978
tensor(-11.2931, device='cuda:0') tensor(3.4772e-05, device='cuda:0') tensor(1.2459e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302590
Average KL loss: 0.001392
Average total loss: 2.303982
tensor(-11.2937, device='cuda:0') tensor(3.4727e-05, device='cuda:0') tensor(1.2451e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001391
Average total loss: 2.303976
tensor(-11.2943, device='cuda:0') tensor(3.4683e-05, device='cuda:0') tensor(1.2444e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001391
Average total loss: 2.303975
tensor(-11.2950, device='cuda:0') tensor(3.4643e-05, device='cuda:0') tensor(1.2436e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001390
Average total loss: 2.303975
tensor(-11.2956, device='cuda:0') tensor(3.4600e-05, device='cuda:0') tensor(1.2429e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302588
Average KL loss: 0.001389
Average total loss: 2.303977
tensor(-11.2962, device='cuda:0') tensor(3.4540e-05, device='cuda:0') tensor(1.2420e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-11.2968, device='cuda:0') tensor(3.4489e-05, device='cuda:0') tensor(1.2413e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302584
Average KL loss: 0.001388
Average total loss: 2.303972
tensor(-11.2968, device='cuda:0') tensor(3.4481e-05, device='cuda:0') tensor(1.2413e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-11.2969, device='cuda:0') tensor(3.4474e-05, device='cuda:0') tensor(1.2412e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302584
Average KL loss: 0.001388
Average total loss: 2.303971
tensor(-11.2969, device='cuda:0') tensor(3.4467e-05, device='cuda:0') tensor(1.2412e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2970, device='cuda:0') tensor(3.4460e-05, device='cuda:0') tensor(1.2411e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2970, device='cuda:0') tensor(3.4452e-05, device='cuda:0') tensor(1.2411e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2971, device='cuda:0') tensor(3.4445e-05, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2971, device='cuda:0') tensor(3.4437e-05, device='cuda:0') tensor(1.2409e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2971, device='cuda:0') tensor(3.4429e-05, device='cuda:0') tensor(1.2409e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302586
Average KL loss: 0.001387
Average total loss: 2.303973
tensor(-11.2972, device='cuda:0') tensor(3.4422e-05, device='cuda:0') tensor(1.2408e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2972, device='cuda:0') tensor(3.4414e-05, device='cuda:0') tensor(1.2408e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302587
Average KL loss: 0.001387
Average total loss: 2.303974
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302590
Average KL loss: 0.001387
Average total loss: 2.303977
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302587
Average KL loss: 0.001387
Average total loss: 2.303974
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302586
Average KL loss: 0.001387
Average total loss: 2.303973
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
 Percentile value: -11.297407150268555
Non-zero model percentage: 26.214414596557617%, Non-zero mask percentage: 26.214414596557617%

--- Pruning Level [6/24]: ---
conv1.weight         | nonzeros =     545 /    1728             ( 31.54%) | total_pruned =    1183 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5387 /   36864             ( 14.61%) | total_pruned =   31477 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   11897 /   36864             ( 32.27%) | total_pruned =   24967 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   12445 /   36864             ( 33.76%) | total_pruned =   24419 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   13614 /   36864             ( 36.93%) | total_pruned =   23250 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   32482 /   73728             ( 44.06%) | total_pruned =   41246 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   64718 /  147456             ( 43.89%) | total_pruned =   82738 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3856 /    8192             ( 47.07%) | total_pruned =    4336 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   44343 /  147456             ( 30.07%) | total_pruned =  103113 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   38117 /  147456             ( 25.85%) | total_pruned =  109339 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  134147 /  294912             ( 45.49%) | total_pruned =  160765 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  222144 /  589824             ( 37.66%) | total_pruned =  367680 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   12200 /   32768             ( 37.23%) | total_pruned =   20568 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  117928 /  589824             ( 19.99%) | total_pruned =  471896 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   88572 /  589824             ( 15.02%) | total_pruned =  501252 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     163 /     256             ( 63.67%) | total_pruned =      93 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  345863 / 1179648             ( 29.32%) | total_pruned =  833785 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  575331 / 2359296             ( 24.39%) | total_pruned = 1783965 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   34072 /  131072             ( 25.99%) | total_pruned =   97000 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  559054 / 2359296             ( 23.70%) | total_pruned = 1800242 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  600684 / 2359296             ( 25.46%) | total_pruned = 1758612 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5098 /    5120             ( 99.57%) | total_pruned =      22 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 2930447, pruned : 8248315, total: 11178762, Compression rate :       3.81x  ( 73.79% pruned)
Train Epoch: 63/100 Loss: 0.019791 Accuracy: 85.20 100.00 % Best test Accuracy: 85.46%
tensor(-11.2973, device='cuda:0') tensor(3.4406e-05, device='cuda:0') tensor(1.2407e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302584
Average KL loss: 0.001383
Average total loss: 2.303967
tensor(-11.3033, device='cuda:0') tensor(3.3598e-05, device='cuda:0') tensor(1.2333e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302582
Average KL loss: 0.001375
Average total loss: 2.303957
tensor(-11.3092, device='cuda:0') tensor(3.2913e-05, device='cuda:0') tensor(1.2260e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001367
Average total loss: 2.303951
tensor(-11.3152, device='cuda:0') tensor(3.2307e-05, device='cuda:0') tensor(1.2187e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001358
Average total loss: 2.303943
tensor(-11.3210, device='cuda:0') tensor(3.1718e-05, device='cuda:0') tensor(1.2116e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001351
Average total loss: 2.303935
tensor(-11.3269, device='cuda:0') tensor(3.1166e-05, device='cuda:0') tensor(1.2045e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302587
Average KL loss: 0.001343
Average total loss: 2.303930
tensor(-11.3327, device='cuda:0') tensor(3.0668e-05, device='cuda:0') tensor(1.1975e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001335
Average total loss: 2.303920
tensor(-11.3385, device='cuda:0') tensor(3.0228e-05, device='cuda:0') tensor(1.1906e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302586
Average KL loss: 0.001327
Average total loss: 2.303913
tensor(-11.3443, device='cuda:0') tensor(2.9658e-05, device='cuda:0') tensor(1.1838e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001320
Average total loss: 2.303904
tensor(-11.3500, device='cuda:0') tensor(2.9201e-05, device='cuda:0') tensor(1.1770e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302589
Average KL loss: 0.001312
Average total loss: 2.303901
tensor(-11.3557, device='cuda:0') tensor(2.8703e-05, device='cuda:0') tensor(1.1703e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001305
Average total loss: 2.303890
tensor(-11.3613, device='cuda:0') tensor(2.8209e-05, device='cuda:0') tensor(1.1637e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001297
Average total loss: 2.303882
tensor(-11.3669, device='cuda:0') tensor(2.7819e-05, device='cuda:0') tensor(1.1572e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001293
Average total loss: 2.303878
tensor(-11.3675, device='cuda:0') tensor(2.7781e-05, device='cuda:0') tensor(1.1566e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001293
Average total loss: 2.303877
tensor(-11.3681, device='cuda:0') tensor(2.7743e-05, device='cuda:0') tensor(1.1559e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001292
Average total loss: 2.303877
tensor(-11.3686, device='cuda:0') tensor(2.7704e-05, device='cuda:0') tensor(1.1553e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001291
Average total loss: 2.303876
tensor(-11.3692, device='cuda:0') tensor(2.7671e-05, device='cuda:0') tensor(1.1546e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001290
Average total loss: 2.303875
tensor(-11.3698, device='cuda:0') tensor(2.7633e-05, device='cuda:0') tensor(1.1540e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302587
Average KL loss: 0.001290
Average total loss: 2.303877
tensor(-11.3703, device='cuda:0') tensor(2.7596e-05, device='cuda:0') tensor(1.1533e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001289
Average total loss: 2.303874
tensor(-11.3709, device='cuda:0') tensor(2.7564e-05, device='cuda:0') tensor(1.1527e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001288
Average total loss: 2.303873
tensor(-11.3714, device='cuda:0') tensor(2.7529e-05, device='cuda:0') tensor(1.1520e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001287
Average total loss: 2.303872
tensor(-11.3720, device='cuda:0') tensor(2.7492e-05, device='cuda:0') tensor(1.1514e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001287
Average total loss: 2.303872
tensor(-11.3726, device='cuda:0') tensor(2.7454e-05, device='cuda:0') tensor(1.1507e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302590
Average KL loss: 0.001286
Average total loss: 2.303876
tensor(-11.3731, device='cuda:0') tensor(2.7406e-05, device='cuda:0') tensor(1.1501e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303870
tensor(-11.3732, device='cuda:0') tensor(2.7401e-05, device='cuda:0') tensor(1.1500e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303870
tensor(-11.3732, device='cuda:0') tensor(2.7396e-05, device='cuda:0') tensor(1.1500e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-11.3733, device='cuda:0') tensor(2.7392e-05, device='cuda:0') tensor(1.1499e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3733, device='cuda:0') tensor(2.7387e-05, device='cuda:0') tensor(1.1499e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3733, device='cuda:0') tensor(2.7381e-05, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3734, device='cuda:0') tensor(2.7377e-05, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3734, device='cuda:0') tensor(2.7373e-05, device='cuda:0') tensor(1.1497e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3735, device='cuda:0') tensor(2.7369e-05, device='cuda:0') tensor(1.1497e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3735, device='cuda:0') tensor(2.7364e-05, device='cuda:0') tensor(1.1496e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7359e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302583
Average KL loss: 0.001285
Average total loss: 2.303868
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302583
Average KL loss: 0.001285
Average total loss: 2.303868
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7355e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7354e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-11.3736, device='cuda:0') tensor(2.7354e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
 Percentile value: -11.373735427856445
Non-zero model percentage: 20.971534729003906%, Non-zero mask percentage: 20.971534729003906%

--- Pruning Level [7/24]: ---
conv1.weight         | nonzeros =     541 /    1728             ( 31.31%) | total_pruned =    1187 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4912 /   36864             ( 13.32%) | total_pruned =   31952 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10261 /   36864             ( 27.83%) | total_pruned =   26603 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10608 /   36864             ( 28.78%) | total_pruned =   26256 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   11463 /   36864             ( 31.10%) | total_pruned =   25401 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29057 /   73728             ( 39.41%) | total_pruned =   44671 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   56360 /  147456             ( 38.22%) | total_pruned =   91096 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3517 /    8192             ( 42.93%) | total_pruned =    4675 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   30490 /  147456             ( 20.68%) | total_pruned =  116966 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   25087 /  147456             ( 17.01%) | total_pruned =  122369 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  119057 /  294912             ( 40.37%) | total_pruned =  175855 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  174923 /  589824             ( 29.66%) | total_pruned =  414901 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    9868 /   32768             ( 30.11%) | total_pruned =   22900 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   80085 /  589824             ( 13.58%) | total_pruned =  509739 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   63831 /  589824             ( 10.82%) | total_pruned =  525993 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  288279 / 1179648             ( 24.44%) | total_pruned =  891369 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  467715 / 2359296             ( 19.82%) | total_pruned = 1891581 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   28345 /  131072             ( 21.63%) | total_pruned =  102727 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  459091 / 2359296             ( 19.46%) | total_pruned = 1900205 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     399 /     512             ( 77.93%) | total_pruned =     113 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  457831 / 2359296             ( 19.41%) | total_pruned = 1901465 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5097 /    5120             ( 99.55%) | total_pruned =      23 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 2344358, pruned : 8834404, total: 11178762, Compression rate :       4.77x  ( 79.03% pruned)
Train Epoch: 79/100 Loss: 0.025416 Accuracy: 83.60 100.00 % Best test Accuracy: 84.26%
tensor(-11.3736, device='cuda:0') tensor(2.7354e-05, device='cuda:0') tensor(1.1495e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001282
Average total loss: 2.303866
tensor(-11.3792, device='cuda:0') tensor(2.6589e-05, device='cuda:0') tensor(1.1431e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001274
Average total loss: 2.303859
tensor(-11.3847, device='cuda:0') tensor(2.5922e-05, device='cuda:0') tensor(1.1368e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001267
Average total loss: 2.303852
tensor(-11.3902, device='cuda:0') tensor(2.5333e-05, device='cuda:0') tensor(1.1306e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001260
Average total loss: 2.303845
tensor(-11.3957, device='cuda:0') tensor(2.4798e-05, device='cuda:0') tensor(1.1244e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001254
Average total loss: 2.303838
tensor(-11.4011, device='cuda:0') tensor(2.4304e-05, device='cuda:0') tensor(1.1183e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001247
Average total loss: 2.303832
tensor(-11.4065, device='cuda:0') tensor(2.3847e-05, device='cuda:0') tensor(1.1123e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001240
Average total loss: 2.303825
tensor(-11.4119, device='cuda:0') tensor(2.3378e-05, device='cuda:0') tensor(1.1063e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302584
Average KL loss: 0.001233
Average total loss: 2.303818
tensor(-11.4173, device='cuda:0') tensor(2.3070e-05, device='cuda:0') tensor(1.1004e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001227
Average total loss: 2.303812
tensor(-11.4226, device='cuda:0') tensor(2.2772e-05, device='cuda:0') tensor(1.0945e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001220
Average total loss: 2.303805
tensor(-11.4279, device='cuda:0') tensor(2.2480e-05, device='cuda:0') tensor(1.0888e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001214
Average total loss: 2.303799
tensor(-11.4332, device='cuda:0') tensor(2.2198e-05, device='cuda:0') tensor(1.0830e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001208
Average total loss: 2.303793
tensor(-11.4384, device='cuda:0') tensor(2.1921e-05, device='cuda:0') tensor(1.0774e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001204
Average total loss: 2.303789
tensor(-11.4389, device='cuda:0') tensor(2.1892e-05, device='cuda:0') tensor(1.0768e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001203
Average total loss: 2.303788
tensor(-11.4394, device='cuda:0') tensor(2.1864e-05, device='cuda:0') tensor(1.0763e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001203
Average total loss: 2.303788
tensor(-11.4399, device='cuda:0') tensor(2.1836e-05, device='cuda:0') tensor(1.0758e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001202
Average total loss: 2.303787
tensor(-11.4404, device='cuda:0') tensor(2.1808e-05, device='cuda:0') tensor(1.0752e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001202
Average total loss: 2.303787
tensor(-11.4410, device='cuda:0') tensor(2.1780e-05, device='cuda:0') tensor(1.0746e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001201
Average total loss: 2.303786
tensor(-11.4415, device='cuda:0') tensor(2.1752e-05, device='cuda:0') tensor(1.0741e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001200
Average total loss: 2.303785
tensor(-11.4420, device='cuda:0') tensor(2.1721e-05, device='cuda:0') tensor(1.0735e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001200
Average total loss: 2.303785
tensor(-11.4425, device='cuda:0') tensor(2.1694e-05, device='cuda:0') tensor(1.0730e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001199
Average total loss: 2.303784
tensor(-11.4430, device='cuda:0') tensor(2.1669e-05, device='cuda:0') tensor(1.0724e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001199
Average total loss: 2.303783
tensor(-11.4435, device='cuda:0') tensor(2.1642e-05, device='cuda:0') tensor(1.0719e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-11.4440, device='cuda:0') tensor(2.1615e-05, device='cuda:0') tensor(1.0713e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303782
tensor(-11.4441, device='cuda:0') tensor(2.1613e-05, device='cuda:0') tensor(1.0713e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303782
tensor(-11.4441, device='cuda:0') tensor(2.1611e-05, device='cuda:0') tensor(1.0712e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4442, device='cuda:0') tensor(2.1609e-05, device='cuda:0') tensor(1.0712e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4442, device='cuda:0') tensor(2.1607e-05, device='cuda:0') tensor(1.0711e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4443, device='cuda:0') tensor(2.1605e-05, device='cuda:0') tensor(1.0711e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4443, device='cuda:0') tensor(2.1603e-05, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4444, device='cuda:0') tensor(2.1601e-05, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4444, device='cuda:0') tensor(2.1599e-05, device='cuda:0') tensor(1.0709e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4445, device='cuda:0') tensor(2.1597e-05, device='cuda:0') tensor(1.0709e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4445, device='cuda:0') tensor(2.1595e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-11.4446, device='cuda:0') tensor(2.1593e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
 Percentile value: -11.444650650024414
Non-zero model percentage: 16.777233123779297%, Non-zero mask percentage: 16.777233123779297%

--- Pruning Level [8/24]: ---
conv1.weight         | nonzeros =     541 /    1728             ( 31.31%) | total_pruned =    1187 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4769 /   36864             ( 12.94%) | total_pruned =   32095 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    9831 /   36864             ( 26.67%) | total_pruned =   27033 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10155 /   36864             ( 27.55%) | total_pruned =   26709 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   10902 /   36864             ( 29.57%) | total_pruned =   25962 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   28051 /   73728             ( 38.05%) | total_pruned =   45677 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   53803 /  147456             ( 36.49%) | total_pruned =   93653 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3435 /    8192             ( 41.93%) | total_pruned =    4757 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   27262 /  147456             ( 18.49%) | total_pruned =  120194 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   22274 /  147456             ( 15.11%) | total_pruned =  125182 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  114396 /  294912             ( 38.79%) | total_pruned =  180516 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  162938 /  589824             ( 27.62%) | total_pruned =  426886 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    9290 /   32768             ( 28.35%) | total_pruned =   23478 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   57768 /  589824             (  9.79%) | total_pruned =  532056 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   41051 /  589824             (  6.96%) | total_pruned =  548773 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  221950 / 1179648             ( 18.81%) | total_pruned =  957698 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  351235 / 2359296             ( 14.89%) | total_pruned = 2008061 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22382 /  131072             ( 17.08%) | total_pruned =  108690 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     275 /     512             ( 53.71%) | total_pruned =     237 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  359155 / 2359296             ( 15.22%) | total_pruned = 2000141 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  351268 / 2359296             ( 14.89%) | total_pruned = 2008028 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5096 /    5120             ( 99.53%) | total_pruned =      24 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1875487, pruned : 9303275, total: 11178762, Compression rate :       5.96x  ( 83.22% pruned)
Train Epoch: 77/100 Loss: 0.041557 Accuracy: 82.41 99.99 % Best test Accuracy: 83.32%
tensor(-11.4446, device='cuda:0') tensor(2.1592e-05, device='cuda:0') tensor(1.0708e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001194
Average total loss: 2.303779
tensor(-11.4497, device='cuda:0') tensor(2.1186e-05, device='cuda:0') tensor(1.0652e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001188
Average total loss: 2.303773
tensor(-11.4549, device='cuda:0') tensor(2.0808e-05, device='cuda:0') tensor(1.0597e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001182
Average total loss: 2.303767
tensor(-11.4600, device='cuda:0') tensor(2.0458e-05, device='cuda:0') tensor(1.0543e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001176
Average total loss: 2.303761
tensor(-11.4651, device='cuda:0') tensor(2.0125e-05, device='cuda:0') tensor(1.0490e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001170
Average total loss: 2.303755
tensor(-11.4702, device='cuda:0') tensor(1.9812e-05, device='cuda:0') tensor(1.0436e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001164
Average total loss: 2.303749
tensor(-11.4753, device='cuda:0') tensor(1.9511e-05, device='cuda:0') tensor(1.0384e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001158
Average total loss: 2.303743
tensor(-11.4803, device='cuda:0') tensor(1.9224e-05, device='cuda:0') tensor(1.0332e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001152
Average total loss: 2.303737
tensor(-11.4853, device='cuda:0') tensor(1.8979e-05, device='cuda:0') tensor(1.0280e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001146
Average total loss: 2.303731
tensor(-11.4903, device='cuda:0') tensor(1.8736e-05, device='cuda:0') tensor(1.0229e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001141
Average total loss: 2.303726
tensor(-11.4952, device='cuda:0') tensor(1.8484e-05, device='cuda:0') tensor(1.0179e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001135
Average total loss: 2.303720
tensor(-11.5001, device='cuda:0') tensor(1.8239e-05, device='cuda:0') tensor(1.0128e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001130
Average total loss: 2.303714
tensor(-11.5051, device='cuda:0') tensor(1.8005e-05, device='cuda:0') tensor(1.0079e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001126
Average total loss: 2.303711
tensor(-11.5055, device='cuda:0') tensor(1.7977e-05, device='cuda:0') tensor(1.0074e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001126
Average total loss: 2.303711
tensor(-11.5060, device='cuda:0') tensor(1.7950e-05, device='cuda:0') tensor(1.0070e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-11.5065, device='cuda:0') tensor(1.7923e-05, device='cuda:0') tensor(1.0065e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-11.5069, device='cuda:0') tensor(1.7896e-05, device='cuda:0') tensor(1.0060e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001124
Average total loss: 2.303709
tensor(-11.5074, device='cuda:0') tensor(1.7869e-05, device='cuda:0') tensor(1.0055e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001124
Average total loss: 2.303709
tensor(-11.5079, device='cuda:0') tensor(1.7842e-05, device='cuda:0') tensor(1.0051e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001123
Average total loss: 2.303708
tensor(-11.5083, device='cuda:0') tensor(1.7815e-05, device='cuda:0') tensor(1.0046e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001123
Average total loss: 2.303708
tensor(-11.5088, device='cuda:0') tensor(1.7789e-05, device='cuda:0') tensor(1.0041e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001122
Average total loss: 2.303707
tensor(-11.5093, device='cuda:0') tensor(1.7762e-05, device='cuda:0') tensor(1.0037e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001122
Average total loss: 2.303707
tensor(-11.5097, device='cuda:0') tensor(1.7736e-05, device='cuda:0') tensor(1.0032e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5102, device='cuda:0') tensor(1.7709e-05, device='cuda:0') tensor(1.0027e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5102, device='cuda:0') tensor(1.7708e-05, device='cuda:0') tensor(1.0027e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5103, device='cuda:0') tensor(1.7707e-05, device='cuda:0') tensor(1.0026e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5103, device='cuda:0') tensor(1.7706e-05, device='cuda:0') tensor(1.0026e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5104, device='cuda:0') tensor(1.7705e-05, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-11.5104, device='cuda:0') tensor(1.7703e-05, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-11.5105, device='cuda:0') tensor(1.7702e-05, device='cuda:0') tensor(1.0024e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-11.5105, device='cuda:0') tensor(1.7701e-05, device='cuda:0') tensor(1.0024e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-11.5106, device='cuda:0') tensor(1.7700e-05, device='cuda:0') tensor(1.0023e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5106, device='cuda:0') tensor(1.7699e-05, device='cuda:0') tensor(1.0023e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7697e-05, device='cuda:0') tensor(1.0023e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7697e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001120
Average total loss: 2.303705
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
 Percentile value: -11.510796546936035
Non-zero model percentage: 13.42179012298584%, Non-zero mask percentage: 13.42179012298584%

--- Pruning Level [9/24]: ---
conv1.weight         | nonzeros =     541 /    1728             ( 31.31%) | total_pruned =    1187 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4335 /   36864             ( 11.76%) | total_pruned =   32529 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    8545 /   36864             ( 23.18%) | total_pruned =   28319 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    8682 /   36864             ( 23.55%) | total_pruned =   28182 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    9335 /   36864             ( 25.32%) | total_pruned =   27529 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   24601 /   73728             ( 33.37%) | total_pruned =   49127 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     104 /     128             ( 81.25%) | total_pruned =      24 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   45915 /  147456             ( 31.14%) | total_pruned =  101541 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3148 /    8192             ( 38.43%) | total_pruned =    5044 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   19138 /  147456             ( 12.98%) | total_pruned =  128318 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   15283 /  147456             ( 10.36%) | total_pruned =  132173 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   99002 /  294912             ( 33.57%) | total_pruned =  195910 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  129748 /  589824             ( 22.00%) | total_pruned =  460076 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    7655 /   32768             ( 23.36%) | total_pruned =   25113 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   35836 /  589824             (  6.08%) | total_pruned =  553988 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     170 /     256             ( 66.41%) | total_pruned =      86 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   25008 /  589824             (  4.24%) | total_pruned =  564816 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     159 /     256             ( 62.11%) | total_pruned =      97 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  164067 / 1179648             ( 13.91%) | total_pruned = 1015581 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  281460 / 2359296             ( 11.93%) | total_pruned = 2077836 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   18779 /  131072             ( 14.33%) | total_pruned =  112293 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     274 /     512             ( 53.52%) | total_pruned =     238 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  298262 / 2359296             ( 12.64%) | total_pruned = 2061034 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  288030 / 2359296             ( 12.21%) | total_pruned = 2071266 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5096 /    5120             ( 99.53%) | total_pruned =      24 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1500390, pruned : 9678372, total: 11178762, Compression rate :       7.45x  ( 86.58% pruned)
Train Epoch: 99/100 Loss: 0.137334 Accuracy: 79.77 99.92 % Best test Accuracy: 82.10%
tensor(-11.5107, device='cuda:0') tensor(1.7696e-05, device='cuda:0') tensor(1.0022e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001118
Average total loss: 2.303703
tensor(-11.5156, device='cuda:0') tensor(1.7463e-05, device='cuda:0') tensor(9.9735e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001112
Average total loss: 2.303697
tensor(-11.5204, device='cuda:0') tensor(1.7235e-05, device='cuda:0') tensor(9.9255e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001107
Average total loss: 2.303692
tensor(-11.5252, device='cuda:0') tensor(1.7015e-05, device='cuda:0') tensor(9.8778e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001102
Average total loss: 2.303687
tensor(-11.5300, device='cuda:0') tensor(1.6799e-05, device='cuda:0') tensor(9.8307e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001096
Average total loss: 2.303681
tensor(-11.5348, device='cuda:0') tensor(1.6590e-05, device='cuda:0') tensor(9.7839e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001091
Average total loss: 2.303676
tensor(-11.5395, device='cuda:0') tensor(1.6385e-05, device='cuda:0') tensor(9.7377e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302581
Average KL loss: 0.001086
Average total loss: 2.303668
tensor(-11.5442, device='cuda:0') tensor(1.6189e-05, device='cuda:0') tensor(9.6919e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001081
Average total loss: 2.303666
tensor(-11.5489, device='cuda:0') tensor(1.6008e-05, device='cuda:0') tensor(9.6465e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001076
Average total loss: 2.303661
tensor(-11.5536, device='cuda:0') tensor(1.5819e-05, device='cuda:0') tensor(9.6015e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001071
Average total loss: 2.303656
tensor(-11.5582, device='cuda:0') tensor(1.5635e-05, device='cuda:0') tensor(9.5569e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001066
Average total loss: 2.303651
tensor(-11.5629, device='cuda:0') tensor(1.5453e-05, device='cuda:0') tensor(9.5128e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001061
Average total loss: 2.303646
tensor(-11.5675, device='cuda:0') tensor(1.5276e-05, device='cuda:0') tensor(9.4689e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303643
tensor(-11.5680, device='cuda:0') tensor(1.5260e-05, device='cuda:0') tensor(9.4645e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303643
tensor(-11.5684, device='cuda:0') tensor(1.5244e-05, device='cuda:0') tensor(9.4601e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001057
Average total loss: 2.303642
tensor(-11.5689, device='cuda:0') tensor(1.5228e-05, device='cuda:0') tensor(9.4557e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001057
Average total loss: 2.303642
tensor(-11.5694, device='cuda:0') tensor(1.5212e-05, device='cuda:0') tensor(9.4513e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001056
Average total loss: 2.303641
tensor(-11.5698, device='cuda:0') tensor(1.5197e-05, device='cuda:0') tensor(9.4468e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001056
Average total loss: 2.303641
tensor(-11.5703, device='cuda:0') tensor(1.5181e-05, device='cuda:0') tensor(9.4424e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001055
Average total loss: 2.303640
tensor(-11.5708, device='cuda:0') tensor(1.5165e-05, device='cuda:0') tensor(9.4380e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001055
Average total loss: 2.303640
tensor(-11.5712, device='cuda:0') tensor(1.5149e-05, device='cuda:0') tensor(9.4336e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001054
Average total loss: 2.303639
tensor(-11.5717, device='cuda:0') tensor(1.5134e-05, device='cuda:0') tensor(9.4292e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001054
Average total loss: 2.303639
tensor(-11.5722, device='cuda:0') tensor(1.5118e-05, device='cuda:0') tensor(9.4248e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5726, device='cuda:0') tensor(1.5103e-05, device='cuda:0') tensor(9.4204e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5727, device='cuda:0') tensor(1.5102e-05, device='cuda:0') tensor(9.4200e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5727, device='cuda:0') tensor(1.5102e-05, device='cuda:0') tensor(9.4195e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5728, device='cuda:0') tensor(1.5101e-05, device='cuda:0') tensor(9.4191e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5728, device='cuda:0') tensor(1.5101e-05, device='cuda:0') tensor(9.4186e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5729, device='cuda:0') tensor(1.5100e-05, device='cuda:0') tensor(9.4182e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5729, device='cuda:0') tensor(1.5100e-05, device='cuda:0') tensor(9.4177e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5729, device='cuda:0') tensor(1.5099e-05, device='cuda:0') tensor(9.4173e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5730, device='cuda:0') tensor(1.5099e-05, device='cuda:0') tensor(9.4169e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5730, device='cuda:0') tensor(1.5098e-05, device='cuda:0') tensor(9.4164e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4160e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303637
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
 Percentile value: -11.573214530944824
Non-zero model percentage: 10.737431526184082%, Non-zero mask percentage: 10.737431526184082%

--- Pruning Level [10/24]: ---
conv1.weight         | nonzeros =     537 /    1728             ( 31.08%) | total_pruned =    1191 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3941 /   36864             ( 10.69%) | total_pruned =   32923 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    7443 /   36864             ( 20.19%) | total_pruned =   29421 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7533 /   36864             ( 20.43%) | total_pruned =   29331 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    8015 /   36864             ( 21.74%) | total_pruned =   28849 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   21505 /   73728             ( 29.17%) | total_pruned =   52223 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   38994 /  147456             ( 26.44%) | total_pruned =  108462 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2899 /    8192             ( 35.39%) | total_pruned =    5293 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13659 /  147456             (  9.26%) | total_pruned =  133797 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10794 /  147456             (  7.32%) | total_pruned =  136662 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   85014 /  294912             ( 28.83%) | total_pruned =  209898 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  117967 /  589824             ( 20.00%) | total_pruned =  471857 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    7069 /   32768             ( 21.57%) | total_pruned =   25699 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   29731 /  589824             (  5.04%) | total_pruned =  560093 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   20569 /  589824             (  3.49%) | total_pruned =  569255 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     156 /     256             ( 60.94%) | total_pruned =     100 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  145808 / 1179648             ( 12.36%) | total_pruned = 1033840 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  211950 / 2359296             (  8.98%) | total_pruned = 2147346 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     390 /     512             ( 76.17%) | total_pruned =     122 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   14450 /  131072             ( 11.02%) | total_pruned =  116622 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  225435 / 2359296             (  9.56%) | total_pruned = 2133861 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  214000 / 2359296             (  9.07%) | total_pruned = 2145296 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5096 /    5120             ( 99.53%) | total_pruned =      24 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1200312, pruned : 9978450, total: 11178762, Compression rate :       9.31x  ( 89.26% pruned)
Train Epoch: 99/100 Loss: 0.182268 Accuracy: 78.23 97.39 % Best test Accuracy: 81.17%
tensor(-11.5731, device='cuda:0') tensor(1.5097e-05, device='cuda:0') tensor(9.4155e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001050
Average total loss: 2.303635
tensor(-11.5777, device='cuda:0') tensor(1.4931e-05, device='cuda:0') tensor(9.3726e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001045
Average total loss: 2.303630
tensor(-11.5822, device='cuda:0') tensor(1.4774e-05, device='cuda:0') tensor(9.3302e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001041
Average total loss: 2.303626
tensor(-11.5868, device='cuda:0') tensor(1.4623e-05, device='cuda:0') tensor(9.2880e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001036
Average total loss: 2.303621
tensor(-11.5913, device='cuda:0') tensor(1.4471e-05, device='cuda:0') tensor(9.2464e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001031
Average total loss: 2.303616
tensor(-11.5958, device='cuda:0') tensor(1.4323e-05, device='cuda:0') tensor(9.2050e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001027
Average total loss: 2.303612
tensor(-11.6002, device='cuda:0') tensor(1.4177e-05, device='cuda:0') tensor(9.1640e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001022
Average total loss: 2.303607
tensor(-11.6047, device='cuda:0') tensor(1.4031e-05, device='cuda:0') tensor(9.1234e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001018
Average total loss: 2.303603
tensor(-11.6091, device='cuda:0') tensor(1.3891e-05, device='cuda:0') tensor(9.0830e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001013
Average total loss: 2.303598
tensor(-11.6135, device='cuda:0') tensor(1.3748e-05, device='cuda:0') tensor(9.0432e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001009
Average total loss: 2.303594
tensor(-11.6179, device='cuda:0') tensor(1.3611e-05, device='cuda:0') tensor(9.0036e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001004
Average total loss: 2.303589
tensor(-11.6222, device='cuda:0') tensor(1.3474e-05, device='cuda:0') tensor(8.9644e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001000
Average total loss: 2.303585
tensor(-11.6266, device='cuda:0') tensor(1.3339e-05, device='cuda:0') tensor(8.9255e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000998
Average total loss: 2.303582
tensor(-11.6270, device='cuda:0') tensor(1.3324e-05, device='cuda:0') tensor(8.9218e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000997
Average total loss: 2.303582
tensor(-11.6274, device='cuda:0') tensor(1.3308e-05, device='cuda:0') tensor(8.9180e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000997
Average total loss: 2.303582
tensor(-11.6278, device='cuda:0') tensor(1.3293e-05, device='cuda:0') tensor(8.9143e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000996
Average total loss: 2.303581
tensor(-11.6283, device='cuda:0') tensor(1.3278e-05, device='cuda:0') tensor(8.9105e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000996
Average total loss: 2.303581
tensor(-11.6287, device='cuda:0') tensor(1.3263e-05, device='cuda:0') tensor(8.9068e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000995
Average total loss: 2.303580
tensor(-11.6291, device='cuda:0') tensor(1.3248e-05, device='cuda:0') tensor(8.9030e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000995
Average total loss: 2.303580
tensor(-11.6295, device='cuda:0') tensor(1.3233e-05, device='cuda:0') tensor(8.8993e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000995
Average total loss: 2.303580
tensor(-11.6299, device='cuda:0') tensor(1.3217e-05, device='cuda:0') tensor(8.8955e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000994
Average total loss: 2.303579
tensor(-11.6304, device='cuda:0') tensor(1.3202e-05, device='cuda:0') tensor(8.8918e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000994
Average total loss: 2.303579
tensor(-11.6308, device='cuda:0') tensor(1.3188e-05, device='cuda:0') tensor(8.8881e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6312, device='cuda:0') tensor(1.3173e-05, device='cuda:0') tensor(8.8843e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6313, device='cuda:0') tensor(1.3172e-05, device='cuda:0') tensor(8.8839e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6313, device='cuda:0') tensor(1.3172e-05, device='cuda:0') tensor(8.8835e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6314, device='cuda:0') tensor(1.3172e-05, device='cuda:0') tensor(8.8831e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6314, device='cuda:0') tensor(1.3172e-05, device='cuda:0') tensor(8.8827e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6314, device='cuda:0') tensor(1.3171e-05, device='cuda:0') tensor(8.8822e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6315, device='cuda:0') tensor(1.3171e-05, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6315, device='cuda:0') tensor(1.3171e-05, device='cuda:0') tensor(8.8814e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6316, device='cuda:0') tensor(1.3171e-05, device='cuda:0') tensor(8.8810e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6316, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8806e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8802e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
 Percentile value: -11.631793022155762
Non-zero model percentage: 8.589949607849121%, Non-zero mask percentage: 8.589949607849121%

--- Pruning Level [11/24]: ---
conv1.weight         | nonzeros =     535 /    1728             ( 30.96%) | total_pruned =    1193 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3808 /   36864             ( 10.33%) | total_pruned =   33056 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    7099 /   36864             ( 19.26%) | total_pruned =   29765 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7166 /   36864             ( 19.44%) | total_pruned =   29698 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7606 /   36864             ( 20.63%) | total_pruned =   29258 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20477 /   73728             ( 27.77%) | total_pruned =   53251 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36808 /  147456             ( 24.96%) | total_pruned =  110648 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2816 /    8192             ( 34.38%) | total_pruned =    5376 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   12242 /  147456             (  8.30%) | total_pruned =  135214 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    9630 /  147456             (  6.53%) | total_pruned =  137826 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   80475 /  294912             ( 27.29%) | total_pruned =  214437 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   97097 /  589824             ( 16.46%) | total_pruned =  492727 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    6002 /   32768             ( 18.32%) | total_pruned =   26766 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     179 /     256             ( 69.92%) | total_pruned =      77 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   19988 /  589824             (  3.39%) | total_pruned =  569836 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   13771 /  589824             (  2.33%) | total_pruned =  576053 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  113696 / 1179648             (  9.64%) | total_pruned = 1065952 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  157317 / 2359296             (  6.67%) | total_pruned = 2201979 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     389 /     512             ( 75.98%) | total_pruned =     123 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   11333 /  131072             (  8.65%) | total_pruned =  119739 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     261 /     512             ( 50.98%) | total_pruned =     251 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  173653 / 2359296             (  7.36%) | total_pruned = 2185643 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     376 /     512             ( 73.44%) | total_pruned =     136 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  165754 / 2359296             (  7.03%) | total_pruned = 2193542 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5095 /    5120             ( 99.51%) | total_pruned =      25 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 960250, pruned : 10218512, total: 11178762, Compression rate :      11.64x  ( 91.41% pruned)
Train Epoch: 99/100 Loss: 0.375693 Accuracy: 75.43 91.65 % Best test Accuracy: 78.13%
tensor(-11.6317, device='cuda:0') tensor(1.3170e-05, device='cuda:0') tensor(8.8798e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000991
Average total loss: 2.303575
tensor(-11.6360, device='cuda:0') tensor(1.3031e-05, device='cuda:0') tensor(8.8417e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000986
Average total loss: 2.303571
tensor(-11.6403, device='cuda:0') tensor(1.2897e-05, device='cuda:0') tensor(8.8037e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000982
Average total loss: 2.303567
tensor(-11.6446, device='cuda:0') tensor(1.2762e-05, device='cuda:0') tensor(8.7663e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000978
Average total loss: 2.303563
tensor(-11.6488, device='cuda:0') tensor(1.2631e-05, device='cuda:0') tensor(8.7291e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000974
Average total loss: 2.303559
tensor(-11.6531, device='cuda:0') tensor(1.2504e-05, device='cuda:0') tensor(8.6921e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000970
Average total loss: 2.303555
tensor(-11.6573, device='cuda:0') tensor(1.2376e-05, device='cuda:0') tensor(8.6557e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000966
Average total loss: 2.303551
tensor(-11.6615, device='cuda:0') tensor(1.2253e-05, device='cuda:0') tensor(8.6193e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000962
Average total loss: 2.303546
tensor(-11.6657, device='cuda:0') tensor(1.2130e-05, device='cuda:0') tensor(8.5834e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000958
Average total loss: 2.303542
tensor(-11.6698, device='cuda:0') tensor(1.1999e-05, device='cuda:0') tensor(8.5477e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000954
Average total loss: 2.303538
tensor(-11.6740, device='cuda:0') tensor(1.1879e-05, device='cuda:0') tensor(8.5123e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000950
Average total loss: 2.303535
tensor(-11.6781, device='cuda:0') tensor(1.1762e-05, device='cuda:0') tensor(8.4773e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000946
Average total loss: 2.303531
tensor(-11.6822, device='cuda:0') tensor(1.1611e-05, device='cuda:0') tensor(8.4425e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000944
Average total loss: 2.303528
tensor(-11.6826, device='cuda:0') tensor(1.1601e-05, device='cuda:0') tensor(8.4389e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000943
Average total loss: 2.303528
tensor(-11.6831, device='cuda:0') tensor(1.1592e-05, device='cuda:0') tensor(8.4354e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000943
Average total loss: 2.303528
tensor(-11.6835, device='cuda:0') tensor(1.1583e-05, device='cuda:0') tensor(8.4318e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000942
Average total loss: 2.303527
tensor(-11.6839, device='cuda:0') tensor(1.1573e-05, device='cuda:0') tensor(8.4283e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000942
Average total loss: 2.303527
tensor(-11.6843, device='cuda:0') tensor(1.1564e-05, device='cuda:0') tensor(8.4247e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000942
Average total loss: 2.303526
tensor(-11.6847, device='cuda:0') tensor(1.1555e-05, device='cuda:0') tensor(8.4212e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000941
Average total loss: 2.303526
tensor(-11.6852, device='cuda:0') tensor(1.1546e-05, device='cuda:0') tensor(8.4177e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000941
Average total loss: 2.303526
tensor(-11.6856, device='cuda:0') tensor(1.1537e-05, device='cuda:0') tensor(8.4141e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303525
tensor(-11.6860, device='cuda:0') tensor(1.1528e-05, device='cuda:0') tensor(8.4106e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303525
tensor(-11.6864, device='cuda:0') tensor(1.1519e-05, device='cuda:0') tensor(8.4071e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303525
tensor(-11.6868, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4035e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6869, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4031e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6869, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4027e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6870, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4023e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6870, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4019e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6871, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4016e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6871, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4012e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6872, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4008e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6872, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4004e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6873, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.4000e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6873, device='cuda:0') tensor(1.1510e-05, device='cuda:0') tensor(8.3996e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
 Percentile value: -11.687418937683105
Non-zero model percentage: 6.871959686279297%, Non-zero mask percentage: 6.871959686279297%

--- Pruning Level [12/24]: ---
conv1.weight         | nonzeros =     535 /    1728             ( 30.96%) | total_pruned =    1193 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3442 /   36864             (  9.34%) | total_pruned =   33422 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6252 /   36864             ( 16.96%) | total_pruned =   30612 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6270 /   36864             ( 17.01%) | total_pruned =   30594 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    6578 /   36864             ( 17.84%) | total_pruned =   30286 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   17779 /   73728             ( 24.11%) | total_pruned =   55949 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   31126 /  147456             ( 21.11%) | total_pruned =  116330 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2602 /    8192             ( 31.76%) | total_pruned =    5590 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     101 /     128             ( 78.91%) | total_pruned =      27 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    8872 /  147456             (  6.02%) | total_pruned =  138584 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    6990 /  147456             (  4.74%) | total_pruned =  140466 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   68208 /  294912             ( 23.13%) | total_pruned =  226704 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   78732 /  589824             ( 13.35%) | total_pruned =  511092 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     236 /     256             ( 92.19%) | total_pruned =      20 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5033 /   32768             ( 15.36%) | total_pruned =   27735 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   13217 /  589824             (  2.24%) | total_pruned =  576607 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     167 /     256             ( 65.23%) | total_pruned =      89 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    9171 /  589824             (  1.55%) | total_pruned =  580653 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   88441 / 1179648             (  7.50%) | total_pruned = 1091207 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  122768 / 2359296             (  5.20%) | total_pruned = 2236528 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     387 /     512             ( 75.59%) | total_pruned =     125 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    9276 /  131072             (  7.08%) | total_pruned =  121796 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  140192 / 2359296             (  5.94%) | total_pruned = 2219104 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     375 /     512             ( 73.24%) | total_pruned =     137 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     395 /     512             ( 77.15%) | total_pruned =     117 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  129758 / 2359296             (  5.50%) | total_pruned = 2229538 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5095 /    5120             ( 99.51%) | total_pruned =      25 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 768200, pruned : 10410562, total: 11178762, Compression rate :      14.55x  ( 93.13% pruned)
Train Epoch: 99/100 Loss: 0.365835 Accuracy: 75.32 88.01 % Best test Accuracy: 76.47%
tensor(-11.6874, device='cuda:0') tensor(1.1509e-05, device='cuda:0') tensor(8.3992e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000937
Average total loss: 2.303522
tensor(-11.6914, device='cuda:0') tensor(1.1398e-05, device='cuda:0') tensor(8.3651e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000933
Average total loss: 2.303518
tensor(-11.6955, device='cuda:0') tensor(1.1291e-05, device='cuda:0') tensor(8.3312e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000929
Average total loss: 2.303514
tensor(-11.6995, device='cuda:0') tensor(1.1184e-05, device='cuda:0') tensor(8.2975e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000926
Average total loss: 2.303511
tensor(-11.7036, device='cuda:0') tensor(1.1078e-05, device='cuda:0') tensor(8.2643e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000922
Average total loss: 2.303507
tensor(-11.7076, device='cuda:0') tensor(1.0976e-05, device='cuda:0') tensor(8.2311e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000918
Average total loss: 2.303503
tensor(-11.7116, device='cuda:0') tensor(1.0874e-05, device='cuda:0') tensor(8.1983e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000915
Average total loss: 2.303500
tensor(-11.7155, device='cuda:0') tensor(1.0773e-05, device='cuda:0') tensor(8.1658e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000911
Average total loss: 2.303496
tensor(-11.7195, device='cuda:0') tensor(1.0675e-05, device='cuda:0') tensor(8.1334e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000907
Average total loss: 2.303492
tensor(-11.7235, device='cuda:0') tensor(1.0577e-05, device='cuda:0') tensor(8.1014e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000904
Average total loss: 2.303489
tensor(-11.7274, device='cuda:0') tensor(1.0480e-05, device='cuda:0') tensor(8.0697e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000900
Average total loss: 2.303485
tensor(-11.7313, device='cuda:0') tensor(1.0376e-05, device='cuda:0') tensor(8.0380e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000897
Average total loss: 2.303482
tensor(-11.7352, device='cuda:0') tensor(1.0283e-05, device='cuda:0') tensor(8.0067e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000895
Average total loss: 2.303480
tensor(-11.7356, device='cuda:0') tensor(1.0271e-05, device='cuda:0') tensor(8.0037e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000895
Average total loss: 2.303479
tensor(-11.7360, device='cuda:0') tensor(1.0260e-05, device='cuda:0') tensor(8.0008e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-11.7363, device='cuda:0') tensor(1.0249e-05, device='cuda:0') tensor(7.9978e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-11.7367, device='cuda:0') tensor(1.0238e-05, device='cuda:0') tensor(7.9948e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303478
tensor(-11.7371, device='cuda:0') tensor(1.0227e-05, device='cuda:0') tensor(7.9918e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303478
tensor(-11.7375, device='cuda:0') tensor(1.0216e-05, device='cuda:0') tensor(7.9888e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303478
tensor(-11.7378, device='cuda:0') tensor(1.0204e-05, device='cuda:0') tensor(7.9858e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303477
tensor(-11.7382, device='cuda:0') tensor(1.0193e-05, device='cuda:0') tensor(7.9828e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303477
tensor(-11.7386, device='cuda:0') tensor(1.0182e-05, device='cuda:0') tensor(7.9798e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303477
tensor(-11.7390, device='cuda:0') tensor(1.0172e-05, device='cuda:0') tensor(7.9769e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303476
tensor(-11.7393, device='cuda:0') tensor(1.0161e-05, device='cuda:0') tensor(7.9739e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7394, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9735e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7394, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9731e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7395, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9728e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7395, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9724e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7396, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9720e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7396, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9716e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7397, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9713e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7397, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9709e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7397, device='cuda:0') tensor(1.0160e-05, device='cuda:0') tensor(7.9705e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9702e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
 Percentile value: -11.739887237548828
Non-zero model percentage: 5.497567176818848%, Non-zero mask percentage: 5.497567176818848%

--- Pruning Level [13/24]: ---
conv1.weight         | nonzeros =     529 /    1728             ( 30.61%) | total_pruned =    1199 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3073 /   36864             (  8.34%) | total_pruned =   33791 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    5469 /   36864             ( 14.84%) | total_pruned =   31395 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    5463 /   36864             ( 14.82%) | total_pruned =   31401 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    5608 /   36864             ( 15.21%) | total_pruned =   31256 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   15218 /   73728             ( 20.64%) | total_pruned =   58510 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   25887 /  147456             ( 17.56%) | total_pruned =  121569 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2380 /    8192             ( 29.05%) | total_pruned =    5812 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     100 /     128             ( 78.12%) | total_pruned =      28 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    6287 /  147456             (  4.26%) | total_pruned =  141169 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    5032 /  147456             (  3.41%) | total_pruned =  142424 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   56756 /  294912             ( 19.25%) | total_pruned =  238156 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   66619 /  589824             ( 11.29%) | total_pruned =  523205 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     236 /     256             ( 92.19%) | total_pruned =      20 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4419 /   32768             ( 13.49%) | total_pruned =   28349 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     175 /     256             ( 68.36%) | total_pruned =      81 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    9837 /  589824             (  1.67%) | total_pruned =  579987 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     145 /     256             ( 56.64%) | total_pruned =     111 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     166 /     256             ( 64.84%) | total_pruned =      90 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    6766 /  589824             (  1.15%) | total_pruned =  583058 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   73624 / 1179648             (  6.24%) | total_pruned = 1106024 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   92437 / 2359296             (  3.92%) | total_pruned = 2266859 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     386 /     512             ( 75.39%) | total_pruned =     126 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    7314 /  131072             (  5.58%) | total_pruned =  123758 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     252 /     512             ( 49.22%) | total_pruned =     260 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  109106 / 2359296             (  4.62%) | total_pruned = 2250190 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     371 /     512             ( 72.46%) | total_pruned =     141 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     394 /     512             ( 76.95%) | total_pruned =     118 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   99813 / 2359296             (  4.23%) | total_pruned = 2259483 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5094 /    5120             ( 99.49%) | total_pruned =      26 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 614560, pruned : 10564202, total: 11178762, Compression rate :      18.19x  ( 94.50% pruned)
Train Epoch: 99/100 Loss: 0.536828 Accuracy: 73.95 81.08 % Best test Accuracy: 75.59%
tensor(-11.7398, device='cuda:0') tensor(1.0159e-05, device='cuda:0') tensor(7.9698e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7437, device='cuda:0') tensor(1.0071e-05, device='cuda:0') tensor(7.9389e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000886
Average total loss: 2.303471
tensor(-11.7476, device='cuda:0') tensor(9.9813e-06, device='cuda:0') tensor(7.9084e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000882
Average total loss: 2.303467
tensor(-11.7514, device='cuda:0') tensor(9.8925e-06, device='cuda:0') tensor(7.8782e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000879
Average total loss: 2.303464
tensor(-11.7552, device='cuda:0') tensor(9.8069e-06, device='cuda:0') tensor(7.8480e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000876
Average total loss: 2.303461
tensor(-11.7591, device='cuda:0') tensor(9.7223e-06, device='cuda:0') tensor(7.8181e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000872
Average total loss: 2.303457
tensor(-11.7628, device='cuda:0') tensor(9.6366e-06, device='cuda:0') tensor(7.7886e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000869
Average total loss: 2.303454
tensor(-11.7666, device='cuda:0') tensor(9.5540e-06, device='cuda:0') tensor(7.7592e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000866
Average total loss: 2.303451
tensor(-11.7704, device='cuda:0') tensor(9.4740e-06, device='cuda:0') tensor(7.7299e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000863
Average total loss: 2.303447
tensor(-11.7741, device='cuda:0') tensor(9.4158e-06, device='cuda:0') tensor(7.7010e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000859
Average total loss: 2.303444
tensor(-11.7779, device='cuda:0') tensor(9.3377e-06, device='cuda:0') tensor(7.6723e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000856
Average total loss: 2.303441
tensor(-11.7816, device='cuda:0') tensor(9.2621e-06, device='cuda:0') tensor(7.6437e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000853
Average total loss: 2.303438
tensor(-11.7853, device='cuda:0') tensor(9.1847e-06, device='cuda:0') tensor(7.6154e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303436
tensor(-11.7857, device='cuda:0') tensor(9.1777e-06, device='cuda:0') tensor(7.6125e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303436
tensor(-11.7861, device='cuda:0') tensor(9.1707e-06, device='cuda:0') tensor(7.6097e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303435
tensor(-11.7865, device='cuda:0') tensor(9.1637e-06, device='cuda:0') tensor(7.6069e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303435
tensor(-11.7868, device='cuda:0') tensor(9.1567e-06, device='cuda:0') tensor(7.6040e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303435
tensor(-11.7872, device='cuda:0') tensor(9.1498e-06, device='cuda:0') tensor(7.6012e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303434
tensor(-11.7876, device='cuda:0') tensor(9.1428e-06, device='cuda:0') tensor(7.5983e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-11.7879, device='cuda:0') tensor(9.1360e-06, device='cuda:0') tensor(7.5955e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-11.7883, device='cuda:0') tensor(9.1290e-06, device='cuda:0') tensor(7.5927e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-11.7887, device='cuda:0') tensor(9.1222e-06, device='cuda:0') tensor(7.5898e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-11.7891, device='cuda:0') tensor(9.1154e-06, device='cuda:0') tensor(7.5870e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-11.7894, device='cuda:0') tensor(9.1085e-06, device='cuda:0') tensor(7.5841e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-11.7895, device='cuda:0') tensor(9.1085e-06, device='cuda:0') tensor(7.5838e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-11.7895, device='cuda:0') tensor(9.1084e-06, device='cuda:0') tensor(7.5834e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-11.7896, device='cuda:0') tensor(9.1084e-06, device='cuda:0') tensor(7.5831e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-11.7896, device='cuda:0') tensor(9.1083e-06, device='cuda:0') tensor(7.5827e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-11.7897, device='cuda:0') tensor(9.1083e-06, device='cuda:0') tensor(7.5824e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-11.7897, device='cuda:0') tensor(9.1082e-06, device='cuda:0') tensor(7.5820e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-11.7898, device='cuda:0') tensor(9.1082e-06, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-11.7898, device='cuda:0') tensor(9.1080e-06, device='cuda:0') tensor(7.5813e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7899, device='cuda:0') tensor(9.1080e-06, device='cuda:0') tensor(7.5809e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7899, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5806e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000847
Average total loss: 2.303432
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
 Percentile value: -11.789983749389648
Non-zero model percentage: 4.398054122924805%, Non-zero mask percentage: 4.398054122924805%

--- Pruning Level [14/24]: ---
conv1.weight         | nonzeros =     526 /    1728             ( 30.44%) | total_pruned =    1202 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2833 /   36864             (  7.69%) | total_pruned =   34031 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4967 /   36864             ( 13.47%) | total_pruned =   31897 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4865 /   36864             ( 13.20%) | total_pruned =   31999 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    5009 /   36864             ( 13.59%) | total_pruned =   31855 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      58 /      64             ( 90.62%) | total_pruned =       6 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   13385 /   73728             ( 18.15%) | total_pruned =   60343 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   22358 /  147456             ( 15.16%) | total_pruned =  125098 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2248 /    8192             ( 27.44%) | total_pruned =    5944 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4930 /  147456             (  3.34%) | total_pruned =  142526 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    4067 /  147456             (  2.76%) | total_pruned =  143389 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   49961 /  294912             ( 16.94%) | total_pruned =  244951 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   55092 /  589824             (  9.34%) | total_pruned =  534732 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     235 /     256             ( 91.80%) | total_pruned =      21 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    3748 /   32768             ( 11.44%) | total_pruned =   29020 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    6663 /  589824             (  1.13%) | total_pruned =  583161 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     166 /     256             ( 64.84%) | total_pruned =      90 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4665 /  589824             (  0.79%) | total_pruned =  585159 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     136 /     256             ( 53.12%) | total_pruned =     120 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   58075 / 1179648             (  4.92%) | total_pruned = 1121573 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     423 /     512             ( 82.62%) | total_pruned =      89 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   68765 / 2359296             (  2.91%) | total_pruned = 2290531 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    5671 /  131072             (  4.33%) | total_pruned =  125401 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     243 /     512             ( 47.46%) | total_pruned =     269 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     471 /     512             ( 91.99%) | total_pruned =      41 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   84524 / 2359296             (  3.58%) | total_pruned = 2274772 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     370 /     512             ( 72.27%) | total_pruned =     142 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     392 /     512             ( 76.56%) | total_pruned =     120 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   76412 / 2359296             (  3.24%) | total_pruned = 2282884 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5093 /    5120             ( 99.47%) | total_pruned =      27 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 491648, pruned : 10687114, total: 11178762, Compression rate :      22.74x  ( 95.60% pruned)
Train Epoch: 99/100 Loss: 0.606958 Accuracy: 73.00 79.75 % Best test Accuracy: 73.43%
tensor(-11.7900, device='cuda:0') tensor(9.1079e-06, device='cuda:0') tensor(7.5802e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000846
Average total loss: 2.303431
tensor(-11.7936, device='cuda:0') tensor(9.0345e-06, device='cuda:0') tensor(7.5523e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000843
Average total loss: 2.303428
tensor(-11.7973, device='cuda:0') tensor(8.9591e-06, device='cuda:0') tensor(7.5246e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000840
Average total loss: 2.303425
tensor(-11.8010, device='cuda:0') tensor(8.8860e-06, device='cuda:0') tensor(7.4973e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000837
Average total loss: 2.303421
tensor(-11.8046, device='cuda:0') tensor(8.8133e-06, device='cuda:0') tensor(7.4700e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000834
Average total loss: 2.303418
tensor(-11.8082, device='cuda:0') tensor(8.7432e-06, device='cuda:0') tensor(7.4428e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000831
Average total loss: 2.303415
tensor(-11.8119, device='cuda:0') tensor(8.6695e-06, device='cuda:0') tensor(7.4161e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000828
Average total loss: 2.303412
tensor(-11.8155, device='cuda:0') tensor(8.5983e-06, device='cuda:0') tensor(7.3894e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000825
Average total loss: 2.303409
tensor(-11.8190, device='cuda:0') tensor(8.5468e-06, device='cuda:0') tensor(7.3629e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000822
Average total loss: 2.303407
tensor(-11.8226, device='cuda:0') tensor(8.4823e-06, device='cuda:0') tensor(7.3366e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000819
Average total loss: 2.303404
tensor(-11.8262, device='cuda:0') tensor(8.4134e-06, device='cuda:0') tensor(7.3105e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000816
Average total loss: 2.303401
tensor(-11.8297, device='cuda:0') tensor(8.3470e-06, device='cuda:0') tensor(7.2846e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000813
Average total loss: 2.303398
tensor(-11.8333, device='cuda:0') tensor(8.2831e-06, device='cuda:0') tensor(7.2588e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-11.8337, device='cuda:0') tensor(8.2792e-06, device='cuda:0') tensor(7.2561e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-11.8340, device='cuda:0') tensor(8.2753e-06, device='cuda:0') tensor(7.2534e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-11.8344, device='cuda:0') tensor(8.2714e-06, device='cuda:0') tensor(7.2507e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303395
tensor(-11.8348, device='cuda:0') tensor(8.2675e-06, device='cuda:0') tensor(7.2479e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303395
tensor(-11.8352, device='cuda:0') tensor(8.2637e-06, device='cuda:0') tensor(7.2452e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303395
tensor(-11.8355, device='cuda:0') tensor(8.2599e-06, device='cuda:0') tensor(7.2425e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-11.8359, device='cuda:0') tensor(8.2561e-06, device='cuda:0') tensor(7.2398e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-11.8363, device='cuda:0') tensor(8.2523e-06, device='cuda:0') tensor(7.2371e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-11.8367, device='cuda:0') tensor(8.2486e-06, device='cuda:0') tensor(7.2344e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303393
tensor(-11.8370, device='cuda:0') tensor(8.2448e-06, device='cuda:0') tensor(7.2317e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8374, device='cuda:0') tensor(8.2411e-06, device='cuda:0') tensor(7.2290e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8374, device='cuda:0') tensor(8.2411e-06, device='cuda:0') tensor(7.2287e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8375, device='cuda:0') tensor(8.2412e-06, device='cuda:0') tensor(7.2283e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8375, device='cuda:0') tensor(8.2411e-06, device='cuda:0') tensor(7.2280e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8376, device='cuda:0') tensor(8.2411e-06, device='cuda:0') tensor(7.2277e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8376, device='cuda:0') tensor(8.2410e-06, device='cuda:0') tensor(7.2273e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8377, device='cuda:0') tensor(8.2410e-06, device='cuda:0') tensor(7.2270e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8377, device='cuda:0') tensor(8.2409e-06, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8378, device='cuda:0') tensor(8.2409e-06, device='cuda:0') tensor(7.2263e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8378, device='cuda:0') tensor(8.2408e-06, device='cuda:0') tensor(7.2260e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2408e-06, device='cuda:0') tensor(7.2256e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
 Percentile value: -11.837921142578125
Non-zero model percentage: 3.5184483528137207%, Non-zero mask percentage: 3.5184483528137207%

--- Pruning Level [15/24]: ---
conv1.weight         | nonzeros =     523 /    1728             ( 30.27%) | total_pruned =    1205 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2607 /   36864             (  7.07%) | total_pruned =   34257 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4436 /   36864             ( 12.03%) | total_pruned =   32428 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4265 /   36864             ( 11.57%) | total_pruned =   32599 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4381 /   36864             ( 11.88%) | total_pruned =   32483 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   11594 /   73728             ( 15.73%) | total_pruned =   62134 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     101 /     128             ( 78.91%) | total_pruned =      27 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   18786 /  147456             ( 12.74%) | total_pruned =  128670 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2057 /    8192             ( 25.11%) | total_pruned =    6135 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    3646 /  147456             (  2.47%) | total_pruned =  143810 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3000 /  147456             (  2.03%) | total_pruned =  144456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   41241 /  294912             ( 13.98%) | total_pruned =  253671 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   45176 /  589824             (  7.66%) | total_pruned =  544648 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     235 /     256             ( 91.80%) | total_pruned =      21 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    3201 /   32768             (  9.77%) | total_pruned =   29567 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     241 /     256             ( 94.14%) | total_pruned =      15 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4540 /  589824             (  0.77%) | total_pruned =  585284 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     140 /     256             ( 54.69%) | total_pruned =     116 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3226 /  589824             (  0.55%) | total_pruned =  586598 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   46244 / 1179648             (  3.92%) | total_pruned = 1133404 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     422 /     512             ( 82.42%) | total_pruned =      90 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   51585 / 2359296             (  2.19%) | total_pruned = 2307711 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     380 /     512             ( 74.22%) | total_pruned =     132 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     468 /     512             ( 91.41%) | total_pruned =      44 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4495 /  131072             (  3.43%) | total_pruned =  126577 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     237 /     512             ( 46.29%) | total_pruned =     275 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   66089 / 2359296             (  2.80%) | total_pruned = 2293207 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     387 /     512             ( 75.59%) | total_pruned =     125 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   59390 / 2359296             (  2.52%) | total_pruned = 2299906 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     478 /     512             ( 93.36%) | total_pruned =      34 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5092 /    5120             ( 99.45%) | total_pruned =      28 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 393319, pruned : 10785443, total: 11178762, Compression rate :      28.42x  ( 96.48% pruned)
Train Epoch: 99/100 Loss: 0.818992 Accuracy: 70.25 75.10 % Best test Accuracy: 70.63%
tensor(-11.8379, device='cuda:0') tensor(8.2407e-06, device='cuda:0') tensor(7.2253e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000806
Average total loss: 2.303391
tensor(-11.8414, device='cuda:0') tensor(8.1750e-06, device='cuda:0') tensor(7.2000e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000803
Average total loss: 2.303388
tensor(-11.8449, device='cuda:0') tensor(8.1116e-06, device='cuda:0') tensor(7.1748e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000801
Average total loss: 2.303386
tensor(-11.8484, device='cuda:0') tensor(8.0498e-06, device='cuda:0') tensor(7.1497e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000798
Average total loss: 2.303383
tensor(-11.8519, device='cuda:0') tensor(7.9849e-06, device='cuda:0') tensor(7.1251e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000795
Average total loss: 2.303380
tensor(-11.8553, device='cuda:0') tensor(7.9224e-06, device='cuda:0') tensor(7.1005e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000792
Average total loss: 2.303377
tensor(-11.8588, device='cuda:0') tensor(7.8621e-06, device='cuda:0') tensor(7.0759e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000790
Average total loss: 2.303375
tensor(-11.8622, device='cuda:0') tensor(7.8030e-06, device='cuda:0') tensor(7.0516e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000787
Average total loss: 2.303372
tensor(-11.8657, device='cuda:0') tensor(7.7412e-06, device='cuda:0') tensor(7.0276e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000784
Average total loss: 2.303369
tensor(-11.8691, device='cuda:0') tensor(7.6827e-06, device='cuda:0') tensor(7.0036e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000782
Average total loss: 2.303367
tensor(-11.8725, device='cuda:0') tensor(7.6261e-06, device='cuda:0') tensor(6.9798e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000779
Average total loss: 2.303364
tensor(-11.8759, device='cuda:0') tensor(7.5700e-06, device='cuda:0') tensor(6.9560e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000776
Average total loss: 2.303361
tensor(-11.8793, device='cuda:0') tensor(7.5109e-06, device='cuda:0') tensor(6.9327e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000775
Average total loss: 2.303360
tensor(-11.8796, device='cuda:0') tensor(7.5039e-06, device='cuda:0') tensor(6.9304e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000775
Average total loss: 2.303359
tensor(-11.8799, device='cuda:0') tensor(7.4971e-06, device='cuda:0') tensor(6.9281e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-11.8802, device='cuda:0') tensor(7.4902e-06, device='cuda:0') tensor(6.9259e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-11.8806, device='cuda:0') tensor(7.4833e-06, device='cuda:0') tensor(6.9236e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-11.8809, device='cuda:0') tensor(7.4765e-06, device='cuda:0') tensor(6.9213e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303358
tensor(-11.8812, device='cuda:0') tensor(7.4692e-06, device='cuda:0') tensor(6.9191e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-11.8815, device='cuda:0') tensor(7.4624e-06, device='cuda:0') tensor(6.9168e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-11.8819, device='cuda:0') tensor(7.4557e-06, device='cuda:0') tensor(6.9146e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-11.8822, device='cuda:0') tensor(7.4489e-06, device='cuda:0') tensor(6.9123e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303357
tensor(-11.8825, device='cuda:0') tensor(7.4422e-06, device='cuda:0') tensor(6.9100e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8829, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9078e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8829, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9074e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8829, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9071e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8830, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9068e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8830, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9065e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8831, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9062e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8831, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9058e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8832, device='cuda:0') tensor(7.4348e-06, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8832, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9052e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8833, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9049e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8833, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9045e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4347e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
 Percentile value: -11.883358001708984
Non-zero model percentage: 2.8147480487823486%, Non-zero mask percentage: 2.8147659301757812%

--- Pruning Level [16/24]: ---
conv1.weight         | nonzeros =     522 /    1728             ( 30.21%) | total_pruned =    1206 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2323 /   36864             (  6.30%) | total_pruned =   34541 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3860 /   36864             ( 10.47%) | total_pruned =   33004 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3700 /   36864             ( 10.04%) | total_pruned =   33164 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3756 /   36864             ( 10.19%) | total_pruned =   33108 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    9725 /   73728             ( 13.19%) | total_pruned =   64003 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   15567 /  147456             ( 10.56%) | total_pruned =  131889 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1879 /    8192             ( 22.94%) | total_pruned =    6313 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      98 /     128             ( 76.56%) | total_pruned =      30 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2693 /  147456             (  1.83%) | total_pruned =  144763 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2238 /  147456             (  1.52%) | total_pruned =  145218 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   34246 /  294912             ( 11.61%) | total_pruned =  260666 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   36932 /  589824             (  6.26%) | total_pruned =  552892 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     233 /     256             ( 91.02%) | total_pruned =      23 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2717 /   32768             (  8.29%) | total_pruned =   30051 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     168 /     256             ( 65.62%) | total_pruned =      88 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     241 /     256             ( 94.14%) | total_pruned =      15 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    3035 /  589824             (  0.51%) | total_pruned =  586789 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     140 /     256             ( 54.69%) | total_pruned =     116 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    2207 /  589824             (  0.37%) | total_pruned =  587617 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     126 /     256             ( 49.22%) | total_pruned =     130 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     247 /     256             ( 96.48%) | total_pruned =       9 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   36718 / 1179648             (  3.11%) | total_pruned = 1142930 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     421 /     512             ( 82.23%) | total_pruned =      91 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   38609 / 2359296             (  1.64%) | total_pruned = 2320687 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3552 /  131072             (  2.71%) | total_pruned =  127520 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     467 /     512             ( 91.21%) | total_pruned =      45 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   51488 / 2359296             (  2.18%) | total_pruned = 2307808 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     365 /     512             ( 71.29%) | total_pruned =     147 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     386 /     512             ( 75.39%) | total_pruned =     126 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   46094 / 2359296             (  1.95%) | total_pruned = 2313202 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     478 /     512             ( 93.36%) | total_pruned =      34 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5090 /    5120             ( 99.41%) | total_pruned =      30 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 314654, pruned : 10864108, total: 11178762, Compression rate :      35.53x  ( 97.19% pruned)
Train Epoch: 99/100 Loss: 0.853389 Accuracy: 66.86 70.91 % Best test Accuracy: 68.06%
tensor(-11.8834, device='cuda:0') tensor(7.4346e-06, device='cuda:0') tensor(6.9042e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000771
Average total loss: 2.303355
tensor(-11.8867, device='cuda:0') tensor(7.3802e-06, device='cuda:0') tensor(6.8810e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000768
Average total loss: 2.303353
tensor(-11.8901, device='cuda:0') tensor(7.3266e-06, device='cuda:0') tensor(6.8580e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000765
Average total loss: 2.303350
tensor(-11.8934, device='cuda:0') tensor(7.2702e-06, device='cuda:0') tensor(6.8353e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000763
Average total loss: 2.303348
tensor(-11.8967, device='cuda:0') tensor(7.2162e-06, device='cuda:0') tensor(6.8126e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000760
Average total loss: 2.303345
tensor(-11.9000, device='cuda:0') tensor(7.1639e-06, device='cuda:0') tensor(6.7900e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000758
Average total loss: 2.303343
tensor(-11.9034, device='cuda:0') tensor(7.1134e-06, device='cuda:0') tensor(6.7675e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000755
Average total loss: 2.303340
tensor(-11.9066, device='cuda:0') tensor(7.0597e-06, device='cuda:0') tensor(6.7454e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000753
Average total loss: 2.303338
tensor(-11.9099, device='cuda:0') tensor(7.0074e-06, device='cuda:0') tensor(6.7234e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000750
Average total loss: 2.303335
tensor(-11.9132, device='cuda:0') tensor(6.9569e-06, device='cuda:0') tensor(6.7014e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000748
Average total loss: 2.303333
tensor(-11.9165, device='cuda:0') tensor(6.9083e-06, device='cuda:0') tensor(6.6795e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000746
Average total loss: 2.303330
tensor(-11.9197, device='cuda:0') tensor(6.8585e-06, device='cuda:0') tensor(6.6579e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000743
Average total loss: 2.303328
tensor(-11.9229, device='cuda:0') tensor(6.8078e-06, device='cuda:0') tensor(6.6365e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000742
Average total loss: 2.303327
tensor(-11.9232, device='cuda:0') tensor(6.8035e-06, device='cuda:0') tensor(6.6343e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000742
Average total loss: 2.303326
tensor(-11.9236, device='cuda:0') tensor(6.7993e-06, device='cuda:0') tensor(6.6321e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-11.9239, device='cuda:0') tensor(6.7951e-06, device='cuda:0') tensor(6.6299e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-11.9242, device='cuda:0') tensor(6.7908e-06, device='cuda:0') tensor(6.6278e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-11.9246, device='cuda:0') tensor(6.7867e-06, device='cuda:0') tensor(6.6256e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303325
tensor(-11.9249, device='cuda:0') tensor(6.7825e-06, device='cuda:0') tensor(6.6234e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-11.9252, device='cuda:0') tensor(6.7784e-06, device='cuda:0') tensor(6.6213e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-11.9255, device='cuda:0') tensor(6.7742e-06, device='cuda:0') tensor(6.6191e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-11.9259, device='cuda:0') tensor(6.7701e-06, device='cuda:0') tensor(6.6169e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303324
tensor(-11.9262, device='cuda:0') tensor(6.7660e-06, device='cuda:0') tensor(6.6148e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9265, device='cuda:0') tensor(6.7620e-06, device='cuda:0') tensor(6.6126e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9266, device='cuda:0') tensor(6.7619e-06, device='cuda:0') tensor(6.6123e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9266, device='cuda:0') tensor(6.7619e-06, device='cuda:0') tensor(6.6120e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9267, device='cuda:0') tensor(6.7619e-06, device='cuda:0') tensor(6.6117e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9267, device='cuda:0') tensor(6.7619e-06, device='cuda:0') tensor(6.6114e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9268, device='cuda:0') tensor(6.7618e-06, device='cuda:0') tensor(6.6111e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9268, device='cuda:0') tensor(6.7618e-06, device='cuda:0') tensor(6.6108e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9268, device='cuda:0') tensor(6.7618e-06, device='cuda:0') tensor(6.6105e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9269, device='cuda:0') tensor(6.7618e-06, device='cuda:0') tensor(6.6101e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9269, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6098e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6095e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
 Percentile value: -11.927005767822266
Non-zero model percentage: 2.2517967224121094%, Non-zero mask percentage: 2.251814603805542%

--- Pruning Level [17/24]: ---
conv1.weight         | nonzeros =     520 /    1728             ( 30.09%) | total_pruned =    1208 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2071 /   36864             (  5.62%) | total_pruned =   34793 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3434 /   36864             (  9.32%) | total_pruned =   33430 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3196 /   36864             (  8.67%) | total_pruned =   33668 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3257 /   36864             (  8.84%) | total_pruned =   33607 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    8242 /   73728             ( 11.18%) | total_pruned =   65486 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12747 /  147456             (  8.64%) | total_pruned =  134709 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     112 /     128             ( 87.50%) | total_pruned =      16 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1722 /    8192             ( 21.02%) | total_pruned =    6470 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1952 /  147456             (  1.32%) | total_pruned =  145504 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1637 /  147456             (  1.11%) | total_pruned =  145819 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   27939 /  294912             (  9.47%) | total_pruned =  266973 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   29869 /  589824             (  5.06%) | total_pruned =  559955 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     232 /     256             ( 90.62%) | total_pruned =      24 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2304 /   32768             (  7.03%) | total_pruned =   30464 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     241 /     256             ( 94.14%) | total_pruned =      15 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2026 /  589824             (  0.34%) | total_pruned =  587798 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     135 /     256             ( 52.73%) | total_pruned =     121 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1467 /  589824             (  0.25%) | total_pruned =  588357 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     122 /     256             ( 47.66%) | total_pruned =     134 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     247 /     256             ( 96.48%) | total_pruned =       9 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   28944 / 1179648             (  2.45%) | total_pruned = 1150704 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     419 /     512             ( 81.84%) | total_pruned =      93 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   28722 / 2359296             (  1.22%) | total_pruned = 2330574 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     376 /     512             ( 73.44%) | total_pruned =     136 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2849 /  131072             (  2.17%) | total_pruned =  128223 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     223 /     512             ( 43.55%) | total_pruned =     289 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     463 /     512             ( 90.43%) | total_pruned =      49 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   40221 / 2359296             (  1.70%) | total_pruned = 2319075 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     360 /     512             ( 70.31%) | total_pruned =     152 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     379 /     512             ( 74.02%) | total_pruned =     133 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   35875 / 2359296             (  1.52%) | total_pruned = 2323421 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     476 /     512             ( 92.97%) | total_pruned =      36 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5089 /    5120             ( 99.39%) | total_pruned =      31 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 251723, pruned : 10927039, total: 11178762, Compression rate :      44.41x  ( 97.75% pruned)
Train Epoch: 98/100 Loss: 0.698154 Accuracy: 64.21 67.32 % Best test Accuracy: 64.49%
tensor(-11.9270, device='cuda:0') tensor(6.7617e-06, device='cuda:0') tensor(6.6092e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000738
Average total loss: 2.303323
tensor(-11.9303, device='cuda:0') tensor(6.7150e-06, device='cuda:0') tensor(6.5879e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000735
Average total loss: 2.303320
tensor(-11.9335, device='cuda:0') tensor(6.6685e-06, device='cuda:0') tensor(6.5668e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000733
Average total loss: 2.303318
tensor(-11.9366, device='cuda:0') tensor(6.6196e-06, device='cuda:0') tensor(6.5460e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000731
Average total loss: 2.303315
tensor(-11.9398, device='cuda:0') tensor(6.5725e-06, device='cuda:0') tensor(6.5252e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000728
Average total loss: 2.303313
tensor(-11.9430, device='cuda:0') tensor(6.5272e-06, device='cuda:0') tensor(6.5045e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000726
Average total loss: 2.303311
tensor(-11.9462, device='cuda:0') tensor(6.4834e-06, device='cuda:0') tensor(6.4839e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000724
Average total loss: 2.303309
tensor(-11.9493, device='cuda:0') tensor(6.4378e-06, device='cuda:0') tensor(6.4635e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000721
Average total loss: 2.303306
tensor(-11.9525, device='cuda:0') tensor(6.3917e-06, device='cuda:0') tensor(6.4433e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000719
Average total loss: 2.303304
tensor(-11.9556, device='cuda:0') tensor(6.3474e-06, device='cuda:0') tensor(6.4231e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000717
Average total loss: 2.303302
tensor(-11.9587, device='cuda:0') tensor(6.3047e-06, device='cuda:0') tensor(6.4030e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000715
Average total loss: 2.303300
tensor(-11.9619, device='cuda:0') tensor(6.2637e-06, device='cuda:0') tensor(6.3830e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000712
Average total loss: 2.303297
tensor(-11.9649, device='cuda:0') tensor(6.2194e-06, device='cuda:0') tensor(6.3633e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-11.9653, device='cuda:0') tensor(6.2172e-06, device='cuda:0') tensor(6.3612e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-11.9656, device='cuda:0') tensor(6.2149e-06, device='cuda:0') tensor(6.3592e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-11.9659, device='cuda:0') tensor(6.2127e-06, device='cuda:0') tensor(6.3571e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303295
tensor(-11.9663, device='cuda:0') tensor(6.2104e-06, device='cuda:0') tensor(6.3550e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-11.9666, device='cuda:0') tensor(6.2082e-06, device='cuda:0') tensor(6.3529e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-11.9669, device='cuda:0') tensor(6.2060e-06, device='cuda:0') tensor(6.3508e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-11.9672, device='cuda:0') tensor(6.2038e-06, device='cuda:0') tensor(6.3488e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303294
tensor(-11.9676, device='cuda:0') tensor(6.2016e-06, device='cuda:0') tensor(6.3467e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9679, device='cuda:0') tensor(6.1994e-06, device='cuda:0') tensor(6.3446e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9682, device='cuda:0') tensor(6.1972e-06, device='cuda:0') tensor(6.3425e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9685, device='cuda:0') tensor(6.1966e-06, device='cuda:0') tensor(6.3405e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9686, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3402e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9686, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3399e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-11.9687, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3396e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9687, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3393e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9688, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3390e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9688, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3387e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9689, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3384e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9689, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3381e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-11.9690, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3378e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9690, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3375e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303293
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
 Percentile value: -11.969005584716797
Non-zero model percentage: 1.801433801651001%, Non-zero mask percentage: 1.8014516830444336%

--- Pruning Level [18/24]: ---
conv1.weight         | nonzeros =     517 /    1728             ( 29.92%) | total_pruned =    1211 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1859 /   36864             (  5.04%) | total_pruned =   35005 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3045 /   36864             (  8.26%) | total_pruned =   33819 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2749 /   36864             (  7.46%) | total_pruned =   34115 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2745 /   36864             (  7.45%) | total_pruned =   34119 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      56 /      64             ( 87.50%) | total_pruned =       8 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      56 /      64             ( 87.50%) | total_pruned =       8 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    6794 /   73728             (  9.21%) | total_pruned =   66934 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   10158 /  147456             (  6.89%) | total_pruned =  137298 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1576 /    8192             ( 19.24%) | total_pruned =    6616 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1393 /  147456             (  0.94%) | total_pruned =  146063 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1204 /  147456             (  0.82%) | total_pruned =  146252 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   22508 /  294912             (  7.63%) | total_pruned =  272404 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   24199 /  589824             (  4.10%) | total_pruned =  565625 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     231 /     256             ( 90.23%) | total_pruned =      25 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1952 /   32768             (  5.96%) | total_pruned =   30816 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    1288 /  589824             (  0.22%) | total_pruned =  588536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     991 /  589824             (  0.17%) | total_pruned =  588833 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     113 /     256             ( 44.14%) | total_pruned =     143 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   22866 / 1179648             (  1.94%) | total_pruned = 1156782 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     417 /     512             ( 81.45%) | total_pruned =      95 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   21481 / 2359296             (  0.91%) | total_pruned = 2337815 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     463 /     512             ( 90.43%) | total_pruned =      49 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2193 /  131072             (  1.67%) | total_pruned =  128879 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     213 /     512             ( 41.60%) | total_pruned =     299 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     460 /     512             ( 89.84%) | total_pruned =      52 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   30690 / 2359296             (  1.30%) | total_pruned = 2328606 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     357 /     512             ( 69.73%) | total_pruned =     155 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     371 /     512             ( 72.46%) | total_pruned =     141 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   28523 / 2359296             (  1.21%) | total_pruned = 2330773 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     475 /     512             ( 92.77%) | total_pruned =      37 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5087 /    5120             ( 99.36%) | total_pruned =      33 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 201378, pruned : 10977384, total: 11178762, Compression rate :      55.51x  ( 98.20% pruned)
Train Epoch: 99/100 Loss: 1.193716 Accuracy: 61.98 63.90 % Best test Accuracy: 62.09%
tensor(-11.9691, device='cuda:0') tensor(6.1967e-06, device='cuda:0') tensor(6.3372e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000707
Average total loss: 2.303292
tensor(-11.9721, device='cuda:0') tensor(6.1551e-06, device='cuda:0') tensor(6.3177e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000705
Average total loss: 2.303290
tensor(-11.9752, device='cuda:0') tensor(6.1149e-06, device='cuda:0') tensor(6.2982e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000703
Average total loss: 2.303288
tensor(-11.9783, device='cuda:0') tensor(6.0749e-06, device='cuda:0') tensor(6.2789e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000701
Average total loss: 2.303286
tensor(-11.9813, device='cuda:0') tensor(6.0323e-06, device='cuda:0') tensor(6.2599e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000699
Average total loss: 2.303284
tensor(-11.9844, device='cuda:0') tensor(5.9913e-06, device='cuda:0') tensor(6.2409e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000697
Average total loss: 2.303282
tensor(-11.9874, device='cuda:0') tensor(5.9517e-06, device='cuda:0') tensor(6.2219e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000695
Average total loss: 2.303279
tensor(-11.9905, device='cuda:0') tensor(5.9099e-06, device='cuda:0') tensor(6.2031e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000692
Average total loss: 2.303277
tensor(-11.9935, device='cuda:0') tensor(5.8944e-06, device='cuda:0') tensor(6.1843e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000690
Average total loss: 2.303275
tensor(-11.9965, device='cuda:0') tensor(5.8553e-06, device='cuda:0') tensor(6.1658e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000688
Average total loss: 2.303273
tensor(-11.9995, device='cuda:0') tensor(5.8169e-06, device='cuda:0') tensor(6.1474e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000686
Average total loss: 2.303271
tensor(-12.0025, device='cuda:0') tensor(5.7799e-06, device='cuda:0') tensor(6.1290e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000684
Average total loss: 2.303269
tensor(-12.0055, device='cuda:0') tensor(5.7442e-06, device='cuda:0') tensor(6.1107e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303268
tensor(-12.0057, device='cuda:0') tensor(5.7386e-06, device='cuda:0') tensor(6.1090e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303268
tensor(-12.0060, device='cuda:0') tensor(5.7330e-06, device='cuda:0') tensor(6.1073e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303267
tensor(-12.0063, device='cuda:0') tensor(5.7274e-06, device='cuda:0') tensor(6.1056e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-12.0066, device='cuda:0') tensor(5.7218e-06, device='cuda:0') tensor(6.1039e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-12.0069, device='cuda:0') tensor(5.7162e-06, device='cuda:0') tensor(6.1022e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-12.0071, device='cuda:0') tensor(5.7107e-06, device='cuda:0') tensor(6.1005e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-12.0074, device='cuda:0') tensor(5.7051e-06, device='cuda:0') tensor(6.0988e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-12.0077, device='cuda:0') tensor(5.6996e-06, device='cuda:0') tensor(6.0971e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0080, device='cuda:0') tensor(5.6941e-06, device='cuda:0') tensor(6.0953e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0083, device='cuda:0') tensor(5.6886e-06, device='cuda:0') tensor(6.0936e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0085, device='cuda:0') tensor(5.6831e-06, device='cuda:0') tensor(6.0919e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0086, device='cuda:0') tensor(5.6832e-06, device='cuda:0') tensor(6.0916e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0086, device='cuda:0') tensor(5.6831e-06, device='cuda:0') tensor(6.0914e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0087, device='cuda:0') tensor(5.6832e-06, device='cuda:0') tensor(6.0911e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0087, device='cuda:0') tensor(5.6831e-06, device='cuda:0') tensor(6.0908e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0088, device='cuda:0') tensor(5.6832e-06, device='cuda:0') tensor(6.0905e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0088, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0902e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0089, device='cuda:0') tensor(5.6831e-06, device='cuda:0') tensor(6.0899e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0089, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0896e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6831e-06, device='cuda:0') tensor(6.0894e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0891e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
 Percentile value: -12.008963584899902
Non-zero model percentage: 1.44114351272583%, Non-zero mask percentage: 1.4411613941192627%

--- Pruning Level [19/24]: ---
conv1.weight         | nonzeros =     513 /    1728             ( 29.69%) | total_pruned =    1215 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1675 /   36864             (  4.54%) | total_pruned =   35189 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2667 /   36864             (  7.23%) | total_pruned =   34197 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2383 /   36864             (  6.46%) | total_pruned =   34481 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2359 /   36864             (  6.40%) | total_pruned =   34505 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      54 /      64             ( 84.38%) | total_pruned =      10 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      55 /      64             ( 85.94%) | total_pruned =       9 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    5565 /   73728             (  7.55%) | total_pruned =   68163 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    8199 /  147456             (  5.56%) | total_pruned =  139257 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1431 /    8192             ( 17.47%) | total_pruned =    6761 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1011 /  147456             (  0.69%) | total_pruned =  146445 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     903 /  147456             (  0.61%) | total_pruned =  146553 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   18047 /  294912             (  6.12%) | total_pruned =  276865 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   19651 /  589824             (  3.33%) | total_pruned =  570173 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     228 /     256             ( 89.06%) | total_pruned =      28 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1650 /   32768             (  5.04%) | total_pruned =   31118 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     239 /     256             ( 93.36%) | total_pruned =      17 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     832 /  589824             (  0.14%) | total_pruned =  588992 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     129 /     256             ( 50.39%) | total_pruned =     127 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     677 /  589824             (  0.11%) | total_pruned =  589147 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     244 /     256             ( 95.31%) | total_pruned =      12 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   18083 / 1179648             (  1.53%) | total_pruned = 1161565 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     415 /     512             ( 81.05%) | total_pruned =      97 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   16048 / 2359296             (  0.68%) | total_pruned = 2343248 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     369 /     512             ( 72.07%) | total_pruned =     143 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     461 /     512             ( 90.04%) | total_pruned =      51 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1768 /  131072             (  1.35%) | total_pruned =  129304 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     208 /     512             ( 40.62%) | total_pruned =     304 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     453 /     512             ( 88.48%) | total_pruned =      59 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   23865 / 2359296             (  1.01%) | total_pruned = 2335431 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     352 /     512             ( 68.75%) | total_pruned =     160 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     366 /     512             ( 71.48%) | total_pruned =     146 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   21200 / 2359296             (  0.90%) | total_pruned = 2338096 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5085 /    5120             ( 99.32%) | total_pruned =      35 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 161102, pruned : 11017660, total: 11178762, Compression rate :      69.39x  ( 98.56% pruned)
Train Epoch: 99/100 Loss: 1.267963 Accuracy: 58.69 59.82 % Best test Accuracy: 58.70%
tensor(-12.0090, device='cuda:0') tensor(5.6830e-06, device='cuda:0') tensor(6.0888e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000680
Average total loss: 2.303265
tensor(-12.0120, device='cuda:0') tensor(5.6442e-06, device='cuda:0') tensor(6.0709e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000678
Average total loss: 2.303263
tensor(-12.0149, device='cuda:0') tensor(5.6067e-06, device='cuda:0') tensor(6.0530e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000676
Average total loss: 2.303261
tensor(-12.0179, device='cuda:0') tensor(5.5709e-06, device='cuda:0') tensor(6.0353e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000674
Average total loss: 2.303259
tensor(-12.0208, device='cuda:0') tensor(5.5360e-06, device='cuda:0') tensor(6.0175e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000672
Average total loss: 2.303257
tensor(-12.0238, device='cuda:0') tensor(5.5024e-06, device='cuda:0') tensor(5.9998e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000670
Average total loss: 2.303255
tensor(-12.0267, device='cuda:0') tensor(5.4666e-06, device='cuda:0') tensor(5.9824e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000668
Average total loss: 2.303253
tensor(-12.0296, device='cuda:0') tensor(5.4301e-06, device='cuda:0') tensor(5.9651e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000666
Average total loss: 2.303251
tensor(-12.0325, device='cuda:0') tensor(5.3951e-06, device='cuda:0') tensor(5.9478e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000664
Average total loss: 2.303249
tensor(-12.0354, device='cuda:0') tensor(5.3615e-06, device='cuda:0') tensor(5.9306e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000662
Average total loss: 2.303247
tensor(-12.0383, device='cuda:0') tensor(5.3291e-06, device='cuda:0') tensor(5.9134e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000660
Average total loss: 2.303245
tensor(-12.0412, device='cuda:0') tensor(5.2974e-06, device='cuda:0') tensor(5.8963e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000658
Average total loss: 2.303243
tensor(-12.0440, device='cuda:0') tensor(5.2621e-06, device='cuda:0') tensor(5.8795e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-12.0443, device='cuda:0') tensor(5.2583e-06, device='cuda:0') tensor(5.8779e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-12.0446, device='cuda:0') tensor(5.2544e-06, device='cuda:0') tensor(5.8763e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-12.0449, device='cuda:0') tensor(5.2505e-06, device='cuda:0') tensor(5.8746e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303241
tensor(-12.0451, device='cuda:0') tensor(5.2467e-06, device='cuda:0') tensor(5.8730e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-12.0454, device='cuda:0') tensor(5.2429e-06, device='cuda:0') tensor(5.8713e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-12.0457, device='cuda:0') tensor(5.2391e-06, device='cuda:0') tensor(5.8697e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-12.0460, device='cuda:0') tensor(5.2353e-06, device='cuda:0') tensor(5.8680e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-12.0463, device='cuda:0') tensor(5.2315e-06, device='cuda:0') tensor(5.8664e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-12.0465, device='cuda:0') tensor(5.2277e-06, device='cuda:0') tensor(5.8647e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303240
tensor(-12.0468, device='cuda:0') tensor(5.2240e-06, device='cuda:0') tensor(5.8631e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0471, device='cuda:0') tensor(5.2203e-06, device='cuda:0') tensor(5.8614e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302605
Average KL loss: 0.000655
Average total loss: 2.303261
tensor(-12.0472, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8612e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0472, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8609e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0472, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8606e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0473, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8603e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0473, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8601e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0474, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8598e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0474, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8595e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0475, device='cuda:0') tensor(5.2200e-06, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0475, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8590e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8587e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
 Percentile value: -12.047494888305664
Non-zero model percentage: 1.1529183387756348%, Non-zero mask percentage: 1.1529362201690674%

--- Pruning Level [20/24]: ---
conv1.weight         | nonzeros =     506 /    1728             ( 29.28%) | total_pruned =    1222 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1507 /   36864             (  4.09%) | total_pruned =   35357 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2292 /   36864             (  6.22%) | total_pruned =   34572 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2039 /   36864             (  5.53%) | total_pruned =   34825 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2004 /   36864             (  5.44%) | total_pruned =   34860 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      53 /      64             ( 82.81%) | total_pruned =      11 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    4567 /   73728             (  6.19%) | total_pruned =   69161 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    6470 /  147456             (  4.39%) | total_pruned =  140986 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1308 /    8192             ( 15.97%) | total_pruned =    6884 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     112 /     128             ( 87.50%) | total_pruned =      16 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     725 /  147456             (  0.49%) | total_pruned =  146731 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     653 /  147456             (  0.44%) | total_pruned =  146803 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   14213 /  294912             (  4.82%) | total_pruned =  280699 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   15764 /  589824             (  2.67%) | total_pruned =  574060 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     228 /     256             ( 89.06%) | total_pruned =      28 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     245 /     256             ( 95.70%) | total_pruned =      11 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1378 /   32768             (  4.21%) | total_pruned =   31390 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     154 /     256             ( 60.16%) | total_pruned =     102 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     568 /  589824             (  0.10%) | total_pruned =  589256 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     113 /     256             ( 44.14%) | total_pruned =     143 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     458 /  589824             (  0.08%) | total_pruned =  589366 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     243 /     256             ( 94.92%) | total_pruned =      13 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   14194 / 1179648             (  1.20%) | total_pruned = 1165454 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     413 /     512             ( 80.66%) | total_pruned =      99 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     464 /     512             ( 90.62%) | total_pruned =      48 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   11972 / 2359296             (  0.51%) | total_pruned = 2347324 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     359 /     512             ( 70.12%) | total_pruned =     153 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     456 /     512             ( 89.06%) | total_pruned =      56 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1397 /  131072             (  1.07%) | total_pruned =  129675 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     196 /     512             ( 38.28%) | total_pruned =     316 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     451 /     512             ( 88.09%) | total_pruned =      61 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   18183 / 2359296             (  0.77%) | total_pruned = 2341113 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     346 /     512             ( 67.58%) | total_pruned =     166 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     353 /     512             ( 68.95%) | total_pruned =     159 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   16219 / 2359296             (  0.69%) | total_pruned = 2343077 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     470 /     512             ( 91.80%) | total_pruned =      42 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5083 /    5120             ( 99.28%) | total_pruned =      37 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 128882, pruned : 11049880, total: 11178762, Compression rate :      86.74x  ( 98.85% pruned)
Train Epoch: 99/100 Loss: 1.345829 Accuracy: 52.50 53.51 % Best test Accuracy: 52.50%
tensor(-12.0476, device='cuda:0') tensor(5.2199e-06, device='cuda:0') tensor(5.8584e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000654
Average total loss: 2.303239
tensor(-12.0505, device='cuda:0') tensor(5.1876e-06, device='cuda:0') tensor(5.8418e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000652
Average total loss: 2.303237
tensor(-12.0533, device='cuda:0') tensor(5.1563e-06, device='cuda:0') tensor(5.8251e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000650
Average total loss: 2.303235
tensor(-12.0562, device='cuda:0') tensor(5.1261e-06, device='cuda:0') tensor(5.8085e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000648
Average total loss: 2.303233
tensor(-12.0590, device='cuda:0') tensor(5.0944e-06, device='cuda:0') tensor(5.7921e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000647
Average total loss: 2.303231
tensor(-12.0618, device='cuda:0') tensor(5.0616e-06, device='cuda:0') tensor(5.7759e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000645
Average total loss: 2.303230
tensor(-12.0646, device='cuda:0') tensor(5.0299e-06, device='cuda:0') tensor(5.7598e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000643
Average total loss: 2.303228
tensor(-12.0674, device='cuda:0') tensor(4.9994e-06, device='cuda:0') tensor(5.7436e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000641
Average total loss: 2.303226
tensor(-12.0702, device='cuda:0') tensor(4.9700e-06, device='cuda:0') tensor(5.7275e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000639
Average total loss: 2.303224
tensor(-12.0730, device='cuda:0') tensor(4.9415e-06, device='cuda:0') tensor(5.7115e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000638
Average total loss: 2.303223
tensor(-12.0758, device='cuda:0') tensor(4.9114e-06, device='cuda:0') tensor(5.6957e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000636
Average total loss: 2.303221
tensor(-12.0786, device='cuda:0') tensor(4.8803e-06, device='cuda:0') tensor(5.6800e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000634
Average total loss: 2.303219
tensor(-12.0813, device='cuda:0') tensor(4.8503e-06, device='cuda:0') tensor(5.6643e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-12.0816, device='cuda:0') tensor(4.8481e-06, device='cuda:0') tensor(5.6628e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-12.0819, device='cuda:0') tensor(4.8460e-06, device='cuda:0') tensor(5.6612e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-12.0822, device='cuda:0') tensor(4.8439e-06, device='cuda:0') tensor(5.6596e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303217
tensor(-12.0824, device='cuda:0') tensor(4.8418e-06, device='cuda:0') tensor(5.6580e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-12.0827, device='cuda:0') tensor(4.8397e-06, device='cuda:0') tensor(5.6564e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-12.0830, device='cuda:0') tensor(4.8376e-06, device='cuda:0') tensor(5.6548e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-12.0833, device='cuda:0') tensor(4.8355e-06, device='cuda:0') tensor(5.6532e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-12.0836, device='cuda:0') tensor(4.8334e-06, device='cuda:0') tensor(5.6516e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-12.0838, device='cuda:0') tensor(4.8314e-06, device='cuda:0') tensor(5.6501e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303216
tensor(-12.0841, device='cuda:0') tensor(4.8293e-06, device='cuda:0') tensor(5.6485e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0844, device='cuda:0') tensor(4.8273e-06, device='cuda:0') tensor(5.6469e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0844, device='cuda:0') tensor(4.8273e-06, device='cuda:0') tensor(5.6466e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0845, device='cuda:0') tensor(4.8273e-06, device='cuda:0') tensor(5.6464e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0845, device='cuda:0') tensor(4.8273e-06, device='cuda:0') tensor(5.6461e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0846, device='cuda:0') tensor(4.8273e-06, device='cuda:0') tensor(5.6458e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0846, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6456e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0847, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6453e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0847, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6451e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0848, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0848, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6445e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6443e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
 Percentile value: -12.08474063873291
Non-zero model percentage: 0.9223381876945496%, Non-zero mask percentage: 0.9223561882972717%

--- Pruning Level [21/24]: ---
conv1.weight         | nonzeros =     501 /    1728             ( 28.99%) | total_pruned =    1227 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1327 /   36864             (  3.60%) | total_pruned =   35537 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1958 /   36864             (  5.31%) | total_pruned =   34906 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1720 /   36864             (  4.67%) | total_pruned =   35144 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1694 /   36864             (  4.60%) | total_pruned =   35170 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    3599 /   73728             (  4.88%) | total_pruned =   70129 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    5040 /  147456             (  3.42%) | total_pruned =  142416 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     110 /     128             ( 85.94%) | total_pruned =      18 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1180 /    8192             ( 14.40%) | total_pruned =    7012 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     111 /     128             ( 86.72%) | total_pruned =      17 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     532 /  147456             (  0.36%) | total_pruned =  146924 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      72 /     128             ( 56.25%) | total_pruned =      56 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     450 /  147456             (  0.31%) | total_pruned =  147006 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   10932 /  294912             (  3.71%) | total_pruned =  283980 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   12492 /  589824             (  2.12%) | total_pruned =  577332 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     225 /     256             ( 87.89%) | total_pruned =      31 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     245 /     256             ( 95.70%) | total_pruned =      11 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1158 /   32768             (  3.53%) | total_pruned =   31610 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     364 /  589824             (  0.06%) | total_pruned =  589460 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     140 /     256             ( 54.69%) | total_pruned =     116 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     295 /  589824             (  0.05%) | total_pruned =  589529 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      95 /     256             ( 37.11%) | total_pruned =     161 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   11074 / 1179648             (  0.94%) | total_pruned = 1168574 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     409 /     512             ( 79.88%) | total_pruned =     103 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     458 /     512             ( 89.45%) | total_pruned =      54 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    8780 / 2359296             (  0.37%) | total_pruned = 2350516 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     353 /     512             ( 68.95%) | total_pruned =     159 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     452 /     512             ( 88.28%) | total_pruned =      60 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1099 /  131072             (  0.84%) | total_pruned =  129973 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     184 /     512             ( 35.94%) | total_pruned =     328 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     446 /     512             ( 87.11%) | total_pruned =      66 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   13950 / 2359296             (  0.59%) | total_pruned = 2345346 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     342 /     512             ( 66.80%) | total_pruned =     170 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     340 /     512             ( 66.41%) | total_pruned =     172 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   12614 / 2359296             (  0.53%) | total_pruned = 2346682 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5079 /    5120             ( 99.20%) | total_pruned =      41 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 103106, pruned : 11075656, total: 11178762, Compression rate :     108.42x  ( 99.08% pruned)
Train Epoch: 99/100 Loss: 1.334793 Accuracy: 49.97 51.05 % Best test Accuracy: 50.35%
tensor(-12.0849, device='cuda:0') tensor(4.8272e-06, device='cuda:0') tensor(5.6440e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000630
Average total loss: 2.303215
tensor(-12.0877, device='cuda:0') tensor(4.7995e-06, device='cuda:0') tensor(5.6285e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000628
Average total loss: 2.303213
tensor(-12.0904, device='cuda:0') tensor(4.7727e-06, device='cuda:0') tensor(5.6130e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000627
Average total loss: 2.303212
tensor(-12.0932, device='cuda:0') tensor(4.7438e-06, device='cuda:0') tensor(5.5977e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000625
Average total loss: 2.303210
tensor(-12.0959, device='cuda:0') tensor(4.7173e-06, device='cuda:0') tensor(5.5825e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000623
Average total loss: 2.303208
tensor(-12.0986, device='cuda:0') tensor(4.6897e-06, device='cuda:0') tensor(5.5674e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000622
Average total loss: 2.303206
tensor(-12.1013, device='cuda:0') tensor(4.6624e-06, device='cuda:0') tensor(5.5523e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000620
Average total loss: 2.303205
tensor(-12.1040, device='cuda:0') tensor(4.6360e-06, device='cuda:0') tensor(5.5373e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000618
Average total loss: 2.303203
tensor(-12.1067, device='cuda:0') tensor(4.6106e-06, device='cuda:0') tensor(5.5223e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000617
Average total loss: 2.303201
tensor(-12.1094, device='cuda:0') tensor(4.5846e-06, device='cuda:0') tensor(5.5075e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000615
Average total loss: 2.303200
tensor(-12.1121, device='cuda:0') tensor(4.5564e-06, device='cuda:0') tensor(5.4928e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000613
Average total loss: 2.303198
tensor(-12.1147, device='cuda:0') tensor(4.5291e-06, device='cuda:0') tensor(5.4782e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000612
Average total loss: 2.303196
tensor(-12.1174, device='cuda:0') tensor(4.5028e-06, device='cuda:0') tensor(5.4636e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000611
Average total loss: 2.303196
tensor(-12.1177, device='cuda:0') tensor(4.5016e-06, device='cuda:0') tensor(5.4620e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000611
Average total loss: 2.303195
tensor(-12.1180, device='cuda:0') tensor(4.5006e-06, device='cuda:0') tensor(5.4606e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-12.1182, device='cuda:0') tensor(4.4994e-06, device='cuda:0') tensor(5.4590e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-12.1185, device='cuda:0') tensor(4.4983e-06, device='cuda:0') tensor(5.4575e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-12.1188, device='cuda:0') tensor(4.4972e-06, device='cuda:0') tensor(5.4560e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-12.1191, device='cuda:0') tensor(4.4961e-06, device='cuda:0') tensor(5.4544e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-12.1194, device='cuda:0') tensor(4.4949e-06, device='cuda:0') tensor(5.4529e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1196, device='cuda:0') tensor(4.4938e-06, device='cuda:0') tensor(5.4514e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1199, device='cuda:0') tensor(4.4927e-06, device='cuda:0') tensor(5.4498e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1202, device='cuda:0') tensor(4.4916e-06, device='cuda:0') tensor(5.4483e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1205, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4468e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1205, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4465e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1206, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4463e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1206, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4460e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1207, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4458e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1207, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4455e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1208, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4453e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1208, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4450e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1209, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1209, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4445e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1209, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4442e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4906e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
 Percentile value: -12.12076187133789
Non-zero model percentage: 0.7378724217414856%, Non-zero mask percentage: 0.7378903031349182%

--- Pruning Level [22/24]: ---
conv1.weight         | nonzeros =     491 /    1728             ( 28.41%) | total_pruned =    1237 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1160 /   36864             (  3.15%) | total_pruned =   35704 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1708 /   36864             (  4.63%) | total_pruned =   35156 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1440 /   36864             (  3.91%) | total_pruned =   35424 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1450 /   36864             (  3.93%) | total_pruned =   35414 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2786 /   73728             (  3.78%) | total_pruned =   70942 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    3924 /  147456             (  2.66%) | total_pruned =  143532 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     108 /     128             ( 84.38%) | total_pruned =      20 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1068 /    8192             ( 13.04%) | total_pruned =    7124 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     108 /     128             ( 84.38%) | total_pruned =      20 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     397 /  147456             (  0.27%) | total_pruned =  147059 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     321 /  147456             (  0.22%) | total_pruned =  147135 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8335 /  294912             (  2.83%) | total_pruned =  286577 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    9964 /  589824             (  1.69%) | total_pruned =  579860 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     223 /     256             ( 87.11%) | total_pruned =      33 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     970 /   32768             (  2.96%) | total_pruned =   31798 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     220 /  589824             (  0.04%) | total_pruned =  589604 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     203 /  589824             (  0.03%) | total_pruned =  589621 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      88 /     256             ( 34.38%) | total_pruned =     168 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8631 / 1179648             (  0.73%) | total_pruned = 1171017 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     457 /     512             ( 89.26%) | total_pruned =      55 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    6553 / 2359296             (  0.28%) | total_pruned = 2352743 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     345 /     512             ( 67.38%) | total_pruned =     167 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     858 /  131072             (  0.65%) | total_pruned =  130214 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     167 /     512             ( 32.62%) | total_pruned =     345 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     444 /     512             ( 86.72%) | total_pruned =      68 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   10408 / 2359296             (  0.44%) | total_pruned = 2348888 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     336 /     512             ( 65.62%) | total_pruned =     176 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     329 /     512             ( 64.26%) | total_pruned =     183 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    9386 / 2359296             (  0.40%) | total_pruned = 2349910 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     462 /     512             ( 90.23%) | total_pruned =      50 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     511 /     512             ( 99.80%) | total_pruned =       1 | shape = torch.Size([512])
linear.weight        | nonzeros =    5076 /    5120             ( 99.14%) | total_pruned =      44 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 82485, pruned : 11096277, total: 11178762, Compression rate :     135.52x  ( 99.26% pruned)
Train Epoch: 99/100 Loss: 1.566966 Accuracy: 45.67 46.01 % Best test Accuracy: 45.94%
tensor(-12.1210, device='cuda:0') tensor(4.4905e-06, device='cuda:0') tensor(5.4440e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000608
Average total loss: 2.303193
tensor(-12.1237, device='cuda:0') tensor(4.4661e-06, device='cuda:0') tensor(5.4295e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000606
Average total loss: 2.303191
tensor(-12.1263, device='cuda:0') tensor(4.4426e-06, device='cuda:0') tensor(5.4151e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000605
Average total loss: 2.303189
tensor(-12.1289, device='cuda:0') tensor(4.4151e-06, device='cuda:0') tensor(5.4009e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000603
Average total loss: 2.303188
tensor(-12.1315, device='cuda:0') tensor(4.3888e-06, device='cuda:0') tensor(5.3868e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000601
Average total loss: 2.303186
tensor(-12.1342, device='cuda:0') tensor(4.3635e-06, device='cuda:0') tensor(5.3727e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000600
Average total loss: 2.303185
tensor(-12.1368, device='cuda:0') tensor(4.3390e-06, device='cuda:0') tensor(5.3587e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000598
Average total loss: 2.303183
tensor(-12.1394, device='cuda:0') tensor(4.3153e-06, device='cuda:0') tensor(5.3447e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000597
Average total loss: 2.303182
tensor(-12.1420, device='cuda:0') tensor(4.2925e-06, device='cuda:0') tensor(5.3307e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000595
Average total loss: 2.303180
tensor(-12.1446, device='cuda:0') tensor(4.2697e-06, device='cuda:0') tensor(5.3168e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000594
Average total loss: 2.303179
tensor(-12.1472, device='cuda:0') tensor(4.2441e-06, device='cuda:0') tensor(5.3032e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000592
Average total loss: 2.303177
tensor(-12.1498, device='cuda:0') tensor(4.2194e-06, device='cuda:0') tensor(5.2895e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000591
Average total loss: 2.303175
tensor(-12.1523, device='cuda:0') tensor(4.1955e-06, device='cuda:0') tensor(5.2760e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000590
Average total loss: 2.303175
tensor(-12.1526, device='cuda:0') tensor(4.1949e-06, device='cuda:0') tensor(5.2745e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000590
Average total loss: 2.303174
tensor(-12.1529, device='cuda:0') tensor(4.1936e-06, device='cuda:0') tensor(5.2730e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1531, device='cuda:0') tensor(4.1884e-06, device='cuda:0') tensor(5.2718e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1534, device='cuda:0') tensor(4.1833e-06, device='cuda:0') tensor(5.2706e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1536, device='cuda:0') tensor(4.1783e-06, device='cuda:0') tensor(5.2693e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1538, device='cuda:0') tensor(4.1732e-06, device='cuda:0') tensor(5.2681e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1541, device='cuda:0') tensor(4.1683e-06, device='cuda:0') tensor(5.2669e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-12.1543, device='cuda:0') tensor(4.1634e-06, device='cuda:0') tensor(5.2657e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303173
tensor(-12.1545, device='cuda:0') tensor(4.1585e-06, device='cuda:0') tensor(5.2644e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1548, device='cuda:0') tensor(4.1536e-06, device='cuda:0') tensor(5.2632e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1550, device='cuda:0') tensor(4.1488e-06, device='cuda:0') tensor(5.2620e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1550, device='cuda:0') tensor(4.1487e-06, device='cuda:0') tensor(5.2617e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1551, device='cuda:0') tensor(4.1488e-06, device='cuda:0') tensor(5.2615e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1551, device='cuda:0') tensor(4.1487e-06, device='cuda:0') tensor(5.2612e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1552, device='cuda:0') tensor(4.1488e-06, device='cuda:0') tensor(5.2610e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1552, device='cuda:0') tensor(4.1487e-06, device='cuda:0') tensor(5.2607e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302584
Average KL loss: 0.000588
Average total loss: 2.303172
tensor(-12.1553, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2605e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1553, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2602e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1554, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2600e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1554, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2597e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2595e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-12.1555, device='cuda:0') tensor(4.1491e-06, device='cuda:0') tensor(5.2593e-11, device='cuda:0')
 Percentile value: -12.155271530151367
Non-zero model percentage: 0.5902978777885437%, Non-zero mask percentage: 0.5903157591819763%

--- Pruning Level [23/24]: ---
conv1.weight         | nonzeros =     477 /    1728             ( 27.60%) | total_pruned =    1251 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     993 /   36864             (  2.69%) | total_pruned =   35871 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1449 /   36864             (  3.93%) | total_pruned =   35415 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1192 /   36864             (  3.23%) | total_pruned =   35672 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1204 /   36864             (  3.27%) | total_pruned =   35660 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2104 /   73728             (  2.85%) | total_pruned =   71624 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    2982 /  147456             (  2.02%) | total_pruned =  144474 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     945 /    8192             ( 11.54%) | total_pruned =    7247 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     108 /     128             ( 84.38%) | total_pruned =      20 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     293 /  147456             (  0.20%) | total_pruned =  147163 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     238 /  147456             (  0.16%) | total_pruned =  147218 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    6239 /  294912             (  2.12%) | total_pruned =  288673 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     245 /     256             ( 95.70%) | total_pruned =      11 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    7702 /  589824             (  1.31%) | total_pruned =  582122 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     222 /     256             ( 86.72%) | total_pruned =      34 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     790 /   32768             (  2.41%) | total_pruned =   31978 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     235 /     256             ( 91.80%) | total_pruned =      21 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     144 /  589824             (  0.02%) | total_pruned =  589680 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     123 /     256             ( 48.05%) | total_pruned =     133 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     139 /  589824             (  0.02%) | total_pruned =  589685 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      78 /     256             ( 30.47%) | total_pruned =     178 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    6653 / 1179648             (  0.56%) | total_pruned = 1172995 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     453 /     512             ( 88.48%) | total_pruned =      59 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    4844 / 2359296             (  0.21%) | total_pruned = 2354452 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     339 /     512             ( 66.21%) | total_pruned =     173 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     434 /     512             ( 84.77%) | total_pruned =      78 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     680 /  131072             (  0.52%) | total_pruned =  130392 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     158 /     512             ( 30.86%) | total_pruned =     354 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     435 /     512             ( 84.96%) | total_pruned =      77 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    7746 / 2359296             (  0.33%) | total_pruned = 2351550 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     331 /     512             ( 64.65%) | total_pruned =     181 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     318 /     512             ( 62.11%) | total_pruned =     194 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    7100 / 2359296             (  0.30%) | total_pruned = 2352196 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     458 /     512             ( 89.45%) | total_pruned =      54 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     511 /     512             ( 99.80%) | total_pruned =       1 | shape = torch.Size([512])
linear.weight        | nonzeros =    5072 /    5120             ( 99.06%) | total_pruned =      48 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 65988, pruned : 11112774, total: 11178762, Compression rate :     169.41x  ( 99.41% pruned)
Train Epoch: 99/100 Loss: 1.849242 Accuracy: 29.46 29.76 % Best test Accuracy: 29.76%
