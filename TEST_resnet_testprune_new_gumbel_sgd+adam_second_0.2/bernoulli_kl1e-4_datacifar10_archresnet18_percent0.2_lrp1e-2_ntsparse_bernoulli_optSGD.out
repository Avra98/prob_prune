Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/24]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/100 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.5000e-05, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302934
Average KL loss: 492.690900
Average total loss: 494.993824
tensor(-0.4870, device='cuda:0') tensor(2.1692e-06, device='cuda:0') tensor(2.3574e-05, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.303026
Average KL loss: 367.618488
Average total loss: 369.921507
tensor(-0.9485, device='cuda:0') tensor(7.8711e-06, device='cuda:0') tensor(2.0124e-05, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302907
Average KL loss: 268.550750
Average total loss: 270.853651
tensor(-1.3609, device='cuda:0') tensor(1.4644e-05, device='cuda:0') tensor(1.6244e-05, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302838
Average KL loss: 197.831934
Average total loss: 200.134767
tensor(-1.7183, device='cuda:0') tensor(2.0772e-05, device='cuda:0') tensor(1.2896e-05, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302705
Average KL loss: 149.297097
Average total loss: 151.599799
tensor(-2.0259, device='cuda:0') tensor(2.5592e-05, device='cuda:0') tensor(1.0293e-05, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302702
Average KL loss: 115.890175
Average total loss: 118.192874
tensor(-2.2923, device='cuda:0') tensor(2.9074e-05, device='cuda:0') tensor(8.3346e-06, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302675
Average KL loss: 92.381409
Average total loss: 94.684081
tensor(-2.5254, device='cuda:0') tensor(3.1570e-05, device='cuda:0') tensor(6.8608e-06, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302616
Average KL loss: 75.370196
Average total loss: 77.672810
tensor(-2.7319, device='cuda:0') tensor(3.3255e-05, device='cuda:0') tensor(5.7381e-06, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302670
Average KL loss: 62.715472
Average total loss: 65.018139
tensor(-2.9169, device='cuda:0') tensor(3.4457e-05, device='cuda:0') tensor(4.8691e-06, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302561
Average KL loss: 53.060589
Average total loss: 55.363149
tensor(-3.0843, device='cuda:0') tensor(3.5234e-05, device='cuda:0') tensor(4.1847e-06, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302572
Average KL loss: 45.527766
Average total loss: 47.830337
tensor(-3.2370, device='cuda:0') tensor(3.5765e-05, device='cuda:0') tensor(3.6367e-06, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302598
Average KL loss: 39.534439
Average total loss: 41.837037
tensor(-3.3775, device='cuda:0') tensor(3.6200e-05, device='cuda:0') tensor(3.1915e-06, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 34.683880
Average total loss: 36.986464
tensor(-3.5077, device='cuda:0') tensor(3.6554e-05, device='cuda:0') tensor(2.8249e-06, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302571
Average KL loss: 30.699102
Average total loss: 33.001673
tensor(-3.6288, device='cuda:0') tensor(3.6809e-05, device='cuda:0') tensor(2.5192e-06, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302608
Average KL loss: 27.382391
Average total loss: 29.684999
tensor(-3.7423, device='cuda:0') tensor(3.6852e-05, device='cuda:0') tensor(2.2615e-06, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302578
Average KL loss: 24.589666
Average total loss: 26.892244
tensor(-3.8490, device='cuda:0') tensor(3.6897e-05, device='cuda:0') tensor(2.0422e-06, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302573
Average KL loss: 22.213913
Average total loss: 24.516486
tensor(-3.9498, device='cuda:0') tensor(3.6892e-05, device='cuda:0') tensor(1.8539e-06, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302588
Average KL loss: 20.174330
Average total loss: 22.476918
tensor(-4.0453, device='cuda:0') tensor(3.6869e-05, device='cuda:0') tensor(1.6908e-06, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302605
Average KL loss: 18.408964
Average total loss: 20.711568
tensor(-4.1361, device='cuda:0') tensor(3.6788e-05, device='cuda:0') tensor(1.5487e-06, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302596
Average KL loss: 16.869638
Average total loss: 19.172233
tensor(-4.2226, device='cuda:0') tensor(3.6688e-05, device='cuda:0') tensor(1.4240e-06, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302578
Average KL loss: 15.518442
Average total loss: 17.821020
tensor(-4.3054, device='cuda:0') tensor(3.6662e-05, device='cuda:0') tensor(1.3139e-06, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302600
Average KL loss: 14.325186
Average total loss: 16.627786
tensor(-4.3848, device='cuda:0') tensor(3.6636e-05, device='cuda:0') tensor(1.2161e-06, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302582
Average KL loss: 13.265612
Average total loss: 15.568194
tensor(-4.4610, device='cuda:0') tensor(3.6646e-05, device='cuda:0') tensor(1.1289e-06, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302588
Average KL loss: 12.319979
Average total loss: 14.622567
tensor(-4.5343, device='cuda:0') tensor(3.6673e-05, device='cuda:0') tensor(1.0508e-06, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302573
Average KL loss: 11.472133
Average total loss: 13.774706
tensor(-4.6051, device='cuda:0') tensor(3.6701e-05, device='cuda:0') tensor(9.8040e-07, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302584
Average KL loss: 10.708710
Average total loss: 13.011293
tensor(-4.6734, device='cuda:0') tensor(3.6701e-05, device='cuda:0') tensor(9.1687e-07, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302569
Average KL loss: 10.018598
Average total loss: 12.321167
tensor(-4.7395, device='cuda:0') tensor(3.6667e-05, device='cuda:0') tensor(8.5923e-07, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302607
Average KL loss: 9.392473
Average total loss: 11.695080
tensor(-4.8035, device='cuda:0') tensor(3.6637e-05, device='cuda:0') tensor(8.0679e-07, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302588
Average KL loss: 8.822473
Average total loss: 11.125061
tensor(-4.8657, device='cuda:0') tensor(3.6571e-05, device='cuda:0') tensor(7.5891e-07, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302582
Average KL loss: 8.301939
Average total loss: 10.604521
tensor(-4.9261, device='cuda:0') tensor(3.6497e-05, device='cuda:0') tensor(7.1509e-07, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302588
Average KL loss: 7.825184
Average total loss: 10.127772
tensor(-4.9848, device='cuda:0') tensor(3.6472e-05, device='cuda:0') tensor(6.7486e-07, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302574
Average KL loss: 7.387327
Average total loss: 9.689900
tensor(-5.0420, device='cuda:0') tensor(3.6468e-05, device='cuda:0') tensor(6.3783e-07, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302586
Average KL loss: 6.984160
Average total loss: 9.286746
tensor(-5.0978, device='cuda:0') tensor(3.6398e-05, device='cuda:0') tensor(6.0366e-07, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302587
Average KL loss: 6.612037
Average total loss: 8.914624
tensor(-5.1522, device='cuda:0') tensor(3.6288e-05, device='cuda:0') tensor(5.7207e-07, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302583
Average KL loss: 6.267786
Average total loss: 8.570369
tensor(-5.2053, device='cuda:0') tensor(3.6266e-05, device='cuda:0') tensor(5.4279e-07, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302587
Average KL loss: 5.948642
Average total loss: 8.251228
tensor(-5.2572, device='cuda:0') tensor(3.6190e-05, device='cuda:0') tensor(5.1560e-07, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302577
Average KL loss: 5.652172
Average total loss: 7.954749
tensor(-5.3080, device='cuda:0') tensor(3.6155e-05, device='cuda:0') tensor(4.9030e-07, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302590
Average KL loss: 5.376250
Average total loss: 7.678841
tensor(-5.3578, device='cuda:0') tensor(3.6117e-05, device='cuda:0') tensor(4.6672e-07, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302581
Average KL loss: 5.118985
Average total loss: 7.421566
tensor(-5.4066, device='cuda:0') tensor(3.6098e-05, device='cuda:0') tensor(4.4472e-07, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302579
Average KL loss: 4.878704
Average total loss: 7.181283
tensor(-5.4544, device='cuda:0') tensor(3.6057e-05, device='cuda:0') tensor(4.2413e-07, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302582
Average KL loss: 4.653924
Average total loss: 6.956506
tensor(-5.5013, device='cuda:0') tensor(3.6030e-05, device='cuda:0') tensor(4.0485e-07, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302584
Average KL loss: 4.443329
Average total loss: 6.745912
tensor(-5.5473, device='cuda:0') tensor(3.6012e-05, device='cuda:0') tensor(3.8677e-07, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 4.245729
Average total loss: 6.548314
tensor(-5.5926, device='cuda:0') tensor(3.6007e-05, device='cuda:0') tensor(3.6978e-07, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302595
Average KL loss: 4.060065
Average total loss: 6.362660
tensor(-5.6371, device='cuda:0') tensor(3.5919e-05, device='cuda:0') tensor(3.5380e-07, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302587
Average KL loss: 3.885390
Average total loss: 6.187977
tensor(-5.6809, device='cuda:0') tensor(3.5867e-05, device='cuda:0') tensor(3.3876e-07, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302584
Average KL loss: 3.720842
Average total loss: 6.023425
tensor(-5.7239, device='cuda:0') tensor(3.5855e-05, device='cuda:0') tensor(3.2457e-07, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302581
Average KL loss: 3.565649
Average total loss: 5.868230
tensor(-5.7663, device='cuda:0') tensor(3.5826e-05, device='cuda:0') tensor(3.1118e-07, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302584
Average KL loss: 3.419115
Average total loss: 5.721699
tensor(-5.8081, device='cuda:0') tensor(3.5879e-05, device='cuda:0') tensor(2.9853e-07, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302581
Average KL loss: 3.280602
Average total loss: 5.583182
tensor(-5.8493, device='cuda:0') tensor(3.5831e-05, device='cuda:0') tensor(2.8655e-07, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302590
Average KL loss: 3.149539
Average total loss: 5.452129
tensor(-5.8899, device='cuda:0') tensor(3.5791e-05, device='cuda:0') tensor(2.7522e-07, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302584
Average KL loss: 3.025391
Average total loss: 5.327975
tensor(-5.9299, device='cuda:0') tensor(3.5759e-05, device='cuda:0') tensor(2.6447e-07, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 2.907684
Average total loss: 5.210269
tensor(-5.9694, device='cuda:0') tensor(3.5744e-05, device='cuda:0') tensor(2.5427e-07, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302583
Average KL loss: 2.795982
Average total loss: 5.098565
tensor(-6.0085, device='cuda:0') tensor(3.5733e-05, device='cuda:0') tensor(2.4459e-07, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 2.689883
Average total loss: 4.992469
tensor(-6.0470, device='cuda:0') tensor(3.5714e-05, device='cuda:0') tensor(2.3539e-07, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302587
Average KL loss: 2.589022
Average total loss: 4.891609
tensor(-6.0851, device='cuda:0') tensor(3.5652e-05, device='cuda:0') tensor(2.2664e-07, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302590
Average KL loss: 2.493062
Average total loss: 4.795652
tensor(-6.1227, device='cuda:0') tensor(3.5642e-05, device='cuda:0') tensor(2.1830e-07, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302591
Average KL loss: 2.401693
Average total loss: 4.704284
tensor(-6.1599, device='cuda:0') tensor(3.5643e-05, device='cuda:0') tensor(2.1036e-07, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302589
Average KL loss: 2.314630
Average total loss: 4.617219
tensor(-6.1967, device='cuda:0') tensor(3.5650e-05, device='cuda:0') tensor(2.0280e-07, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302586
Average KL loss: 2.231609
Average total loss: 4.534195
tensor(-6.2331, device='cuda:0') tensor(3.5648e-05, device='cuda:0') tensor(1.9557e-07, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302586
Average KL loss: 2.152385
Average total loss: 4.454971
tensor(-6.2691, device='cuda:0') tensor(3.5655e-05, device='cuda:0') tensor(1.8866e-07, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302584
Average KL loss: 2.076741
Average total loss: 4.379325
tensor(-6.3048, device='cuda:0') tensor(3.5671e-05, device='cuda:0') tensor(1.8210e-07, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302586
Average KL loss: 2.004462
Average total loss: 4.307048
tensor(-6.3401, device='cuda:0') tensor(3.5658e-05, device='cuda:0') tensor(1.7580e-07, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302588
Average KL loss: 1.935357
Average total loss: 4.237946
tensor(-6.3751, device='cuda:0') tensor(3.5642e-05, device='cuda:0') tensor(1.6978e-07, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 1.869247
Average total loss: 4.171832
tensor(-6.4097, device='cuda:0') tensor(3.5635e-05, device='cuda:0') tensor(1.6402e-07, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 1.805966
Average total loss: 4.108551
tensor(-6.4440, device='cuda:0') tensor(3.5664e-05, device='cuda:0') tensor(1.5850e-07, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302593
Average KL loss: 1.745359
Average total loss: 4.047951
tensor(-6.4781, device='cuda:0') tensor(3.5624e-05, device='cuda:0') tensor(1.5321e-07, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302590
Average KL loss: 1.687278
Average total loss: 3.989869
tensor(-6.5118, device='cuda:0') tensor(3.5583e-05, device='cuda:0') tensor(1.4814e-07, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 1.631593
Average total loss: 3.934179
tensor(-6.5453, device='cuda:0') tensor(3.5548e-05, device='cuda:0') tensor(1.4328e-07, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302580
Average KL loss: 1.578178
Average total loss: 3.880758
tensor(-6.5785, device='cuda:0') tensor(3.5553e-05, device='cuda:0') tensor(1.3861e-07, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302578
Average KL loss: 1.526913
Average total loss: 3.829492
tensor(-6.6114, device='cuda:0') tensor(3.5537e-05, device='cuda:0') tensor(1.3413e-07, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302584
Average KL loss: 1.477691
Average total loss: 3.780275
tensor(-6.6441, device='cuda:0') tensor(3.5508e-05, device='cuda:0') tensor(1.2983e-07, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302586
Average KL loss: 1.430406
Average total loss: 3.732992
tensor(-6.6765, device='cuda:0') tensor(3.5435e-05, device='cuda:0') tensor(1.2570e-07, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302584
Average KL loss: 1.384963
Average total loss: 3.687548
tensor(-6.7088, device='cuda:0') tensor(3.5436e-05, device='cuda:0') tensor(1.2172e-07, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302588
Average KL loss: 1.341271
Average total loss: 3.643859
tensor(-6.7407, device='cuda:0') tensor(3.5428e-05, device='cuda:0') tensor(1.1790e-07, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 1.299248
Average total loss: 3.601833
tensor(-6.7725, device='cuda:0') tensor(3.5439e-05, device='cuda:0') tensor(1.1423e-07, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302582
Average KL loss: 1.258812
Average total loss: 3.561394
tensor(-6.8040, device='cuda:0') tensor(3.5496e-05, device='cuda:0') tensor(1.1071e-07, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302586
Average KL loss: 1.219888
Average total loss: 3.522474
tensor(-6.8354, device='cuda:0') tensor(3.5435e-05, device='cuda:0') tensor(1.0728e-07, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302584
Average KL loss: 1.182405
Average total loss: 3.484989
tensor(-6.8665, device='cuda:0') tensor(3.5489e-05, device='cuda:0') tensor(1.0400e-07, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302584
Average KL loss: 1.146296
Average total loss: 3.448880
tensor(-6.8975, device='cuda:0') tensor(3.5466e-05, device='cuda:0') tensor(1.0083e-07, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302584
Average KL loss: 1.111500
Average total loss: 3.414084
tensor(-6.9282, device='cuda:0') tensor(3.5478e-05, device='cuda:0') tensor(9.7785e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 1.077958
Average total loss: 3.380544
tensor(-6.9588, device='cuda:0') tensor(3.5450e-05, device='cuda:0') tensor(9.4845e-08, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 1.045613
Average total loss: 3.348198
tensor(-6.9892, device='cuda:0') tensor(3.5450e-05, device='cuda:0') tensor(9.2010e-08, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302589
Average KL loss: 1.014412
Average total loss: 3.317002
tensor(-7.0194, device='cuda:0') tensor(3.5399e-05, device='cuda:0') tensor(8.9275e-08, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302583
Average KL loss: 0.984306
Average total loss: 3.286889
tensor(-7.0495, device='cuda:0') tensor(3.5396e-05, device='cuda:0') tensor(8.6635e-08, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302584
Average KL loss: 0.955247
Average total loss: 3.257831
tensor(-7.0794, device='cuda:0') tensor(3.5403e-05, device='cuda:0') tensor(8.4086e-08, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302589
Average KL loss: 0.927192
Average total loss: 3.229780
tensor(-7.1092, device='cuda:0') tensor(3.5376e-05, device='cuda:0') tensor(8.1625e-08, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302581
Average KL loss: 0.900097
Average total loss: 3.202677
tensor(-7.1388, device='cuda:0') tensor(3.5444e-05, device='cuda:0') tensor(7.9247e-08, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302584
Average KL loss: 0.873921
Average total loss: 3.176505
tensor(-7.1682, device='cuda:0') tensor(3.5474e-05, device='cuda:0') tensor(7.6950e-08, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302590
Average KL loss: 0.848628
Average total loss: 3.151218
tensor(-7.1976, device='cuda:0') tensor(3.5476e-05, device='cuda:0') tensor(7.4730e-08, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302590
Average KL loss: 0.824182
Average total loss: 3.126771
tensor(-7.2267, device='cuda:0') tensor(3.5415e-05, device='cuda:0') tensor(7.2584e-08, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302586
Average KL loss: 0.800547
Average total loss: 3.103133
tensor(-7.2558, device='cuda:0') tensor(3.5388e-05, device='cuda:0') tensor(7.0509e-08, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302588
Average KL loss: 0.777692
Average total loss: 3.080280
tensor(-7.2847, device='cuda:0') tensor(3.5374e-05, device='cuda:0') tensor(6.8502e-08, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 0.755585
Average total loss: 3.058170
tensor(-7.3135, device='cuda:0') tensor(3.5381e-05, device='cuda:0') tensor(6.6560e-08, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302584
Average KL loss: 0.734197
Average total loss: 3.036780
tensor(-7.3422, device='cuda:0') tensor(3.5411e-05, device='cuda:0') tensor(6.4681e-08, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302588
Average KL loss: 0.713500
Average total loss: 3.016087
tensor(-7.3707, device='cuda:0') tensor(3.5365e-05, device='cuda:0') tensor(6.2863e-08, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.693466
Average total loss: 2.996051
tensor(-7.3992, device='cuda:0') tensor(3.5356e-05, device='cuda:0') tensor(6.1102e-08, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302588
Average KL loss: 0.674072
Average total loss: 2.976660
tensor(-7.4275, device='cuda:0') tensor(3.5359e-05, device='cuda:0') tensor(5.9398e-08, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.655292
Average total loss: 2.957877
tensor(-7.4557, device='cuda:0') tensor(3.5290e-05, device='cuda:0') tensor(5.7747e-08, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302590
Average KL loss: 0.637104
Average total loss: 2.939694
tensor(-7.4838, device='cuda:0') tensor(3.5209e-05, device='cuda:0') tensor(5.6148e-08, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302586
Average KL loss: 0.619485
Average total loss: 2.922070
tensor(-7.5118, device='cuda:0') tensor(3.5208e-05, device='cuda:0') tensor(5.4599e-08, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302586
Average KL loss: 0.602414
Average total loss: 2.905000
tensor(-7.5397, device='cuda:0') tensor(3.5207e-05, device='cuda:0') tensor(5.3098e-08, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.585872
Average total loss: 2.888457
tensor(-7.5676, device='cuda:0') tensor(3.5197e-05, device='cuda:0') tensor(5.1643e-08, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302586
Average KL loss: 0.569838
Average total loss: 2.872424
tensor(-7.5953, device='cuda:0') tensor(3.5179e-05, device='cuda:0') tensor(5.0233e-08, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302588
Average KL loss: 0.554295
Average total loss: 2.856882
tensor(-7.6229, device='cuda:0') tensor(3.5179e-05, device='cuda:0') tensor(4.8866e-08, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302593
Average KL loss: 0.539224
Average total loss: 2.841818
tensor(-7.6504, device='cuda:0') tensor(3.5130e-05, device='cuda:0') tensor(4.7540e-08, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302582
Average KL loss: 0.524610
Average total loss: 2.827192
tensor(-7.6779, device='cuda:0') tensor(3.5127e-05, device='cuda:0') tensor(4.6254e-08, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302581
Average KL loss: 0.510436
Average total loss: 2.813018
tensor(-7.7052, device='cuda:0') tensor(3.5127e-05, device='cuda:0') tensor(4.5007e-08, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302586
Average KL loss: 0.496687
Average total loss: 2.799273
tensor(-7.7325, device='cuda:0') tensor(3.5119e-05, device='cuda:0') tensor(4.3797e-08, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.483347
Average total loss: 2.785932
tensor(-7.7597, device='cuda:0') tensor(3.5125e-05, device='cuda:0') tensor(4.2623e-08, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.470404
Average total loss: 2.772988
tensor(-7.7868, device='cuda:0') tensor(3.5127e-05, device='cuda:0') tensor(4.1484e-08, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.457842
Average total loss: 2.760427
tensor(-7.8139, device='cuda:0') tensor(3.5128e-05, device='cuda:0') tensor(4.0378e-08, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302584
Average KL loss: 0.445650
Average total loss: 2.748234
tensor(-7.8408, device='cuda:0') tensor(3.5155e-05, device='cuda:0') tensor(3.9305e-08, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.433815
Average total loss: 2.736399
tensor(-7.8677, device='cuda:0') tensor(3.5163e-05, device='cuda:0') tensor(3.8263e-08, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.422324
Average total loss: 2.724909
tensor(-7.8945, device='cuda:0') tensor(3.5165e-05, device='cuda:0') tensor(3.7251e-08, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302580
Average KL loss: 0.411167
Average total loss: 2.713746
tensor(-7.9213, device='cuda:0') tensor(3.5221e-05, device='cuda:0') tensor(3.6269e-08, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302582
Average KL loss: 0.400332
Average total loss: 2.702913
tensor(-7.9480, device='cuda:0') tensor(3.5254e-05, device='cuda:0') tensor(3.5314e-08, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302582
Average KL loss: 0.389808
Average total loss: 2.692391
tensor(-7.9746, device='cuda:0') tensor(3.5283e-05, device='cuda:0') tensor(3.4388e-08, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302586
Average KL loss: 0.379586
Average total loss: 2.682172
tensor(-8.0011, device='cuda:0') tensor(3.5233e-05, device='cuda:0') tensor(3.3487e-08, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.369657
Average total loss: 2.672242
tensor(-8.0276, device='cuda:0') tensor(3.5234e-05, device='cuda:0') tensor(3.2613e-08, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302586
Average KL loss: 0.360009
Average total loss: 2.662595
tensor(-8.0540, device='cuda:0') tensor(3.5176e-05, device='cuda:0') tensor(3.1763e-08, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302586
Average KL loss: 0.350634
Average total loss: 2.653220
tensor(-8.0804, device='cuda:0') tensor(3.5141e-05, device='cuda:0') tensor(3.0937e-08, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.341524
Average total loss: 2.644108
tensor(-8.1067, device='cuda:0') tensor(3.5143e-05, device='cuda:0') tensor(3.0134e-08, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.332669
Average total loss: 2.635254
tensor(-8.1329, device='cuda:0') tensor(3.5150e-05, device='cuda:0') tensor(2.9354e-08, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302588
Average KL loss: 0.324063
Average total loss: 2.626651
tensor(-8.1591, device='cuda:0') tensor(3.5157e-05, device='cuda:0') tensor(2.8596e-08, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302583
Average KL loss: 0.315697
Average total loss: 2.618280
tensor(-8.1853, device='cuda:0') tensor(3.5160e-05, device='cuda:0') tensor(2.7858e-08, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.307564
Average total loss: 2.610149
tensor(-8.2113, device='cuda:0') tensor(3.5171e-05, device='cuda:0') tensor(2.7142e-08, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.299656
Average total loss: 2.602241
tensor(-8.2374, device='cuda:0') tensor(3.5173e-05, device='cuda:0') tensor(2.6445e-08, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.291967
Average total loss: 2.594552
tensor(-8.2633, device='cuda:0') tensor(3.5166e-05, device='cuda:0') tensor(2.5767e-08, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.284490
Average total loss: 2.587075
tensor(-8.2893, device='cuda:0') tensor(3.5167e-05, device='cuda:0') tensor(2.5108e-08, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.277218
Average total loss: 2.579803
tensor(-8.3151, device='cuda:0') tensor(3.5168e-05, device='cuda:0') tensor(2.4467e-08, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.270146
Average total loss: 2.572730
tensor(-8.3410, device='cuda:0') tensor(3.5156e-05, device='cuda:0') tensor(2.3843e-08, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.263266
Average total loss: 2.565851
tensor(-8.3667, device='cuda:0') tensor(3.5155e-05, device='cuda:0') tensor(2.3237e-08, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302580
Average KL loss: 0.256574
Average total loss: 2.559154
tensor(-8.3925, device='cuda:0') tensor(3.5186e-05, device='cuda:0') tensor(2.2647e-08, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.250063
Average total loss: 2.552648
tensor(-8.4182, device='cuda:0') tensor(3.5177e-05, device='cuda:0') tensor(2.2073e-08, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.243729
Average total loss: 2.546314
tensor(-8.4438, device='cuda:0') tensor(3.5179e-05, device='cuda:0') tensor(2.1514e-08, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.237566
Average total loss: 2.540151
tensor(-8.4694, device='cuda:0') tensor(3.5175e-05, device='cuda:0') tensor(2.0971e-08, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.231569
Average total loss: 2.534154
tensor(-8.4949, device='cuda:0') tensor(3.5146e-05, device='cuda:0') tensor(2.0442e-08, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.225733
Average total loss: 2.528317
tensor(-8.5205, device='cuda:0') tensor(3.5145e-05, device='cuda:0') tensor(1.9927e-08, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.220053
Average total loss: 2.522637
tensor(-8.5459, device='cuda:0') tensor(3.5191e-05, device='cuda:0') tensor(1.9427e-08, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302582
Average KL loss: 0.214525
Average total loss: 2.517107
tensor(-8.5713, device='cuda:0') tensor(3.5222e-05, device='cuda:0') tensor(1.8939e-08, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.209144
Average total loss: 2.511729
tensor(-8.5967, device='cuda:0') tensor(3.5223e-05, device='cuda:0') tensor(1.8465e-08, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302586
Average KL loss: 0.203907
Average total loss: 2.506493
tensor(-8.6221, device='cuda:0') tensor(3.5194e-05, device='cuda:0') tensor(1.8003e-08, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302586
Average KL loss: 0.198809
Average total loss: 2.501394
tensor(-8.6474, device='cuda:0') tensor(3.5189e-05, device='cuda:0') tensor(1.7553e-08, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.193846
Average total loss: 2.496430
tensor(-8.6726, device='cuda:0') tensor(3.5178e-05, device='cuda:0') tensor(1.7115e-08, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.189014
Average total loss: 2.491598
tensor(-8.6979, device='cuda:0') tensor(3.5176e-05, device='cuda:0') tensor(1.6689e-08, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.184309
Average total loss: 2.486894
tensor(-8.7231, device='cuda:0') tensor(3.5173e-05, device='cuda:0') tensor(1.6274e-08, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.179728
Average total loss: 2.482313
tensor(-8.7482, device='cuda:0') tensor(3.5221e-05, device='cuda:0') tensor(1.5870e-08, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302586
Average KL loss: 0.175267
Average total loss: 2.477854
tensor(-8.7733, device='cuda:0') tensor(3.5209e-05, device='cuda:0') tensor(1.5476e-08, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.170924
Average total loss: 2.473509
tensor(-8.7984, device='cuda:0') tensor(3.5206e-05, device='cuda:0') tensor(1.5093e-08, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302584
Average KL loss: 0.166694
Average total loss: 2.469278
tensor(-8.8235, device='cuda:0') tensor(3.5304e-05, device='cuda:0') tensor(1.4720e-08, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.162574
Average total loss: 2.465159
tensor(-8.8485, device='cuda:0') tensor(3.5304e-05, device='cuda:0') tensor(1.4356e-08, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.158562
Average total loss: 2.461147
tensor(-8.8734, device='cuda:0') tensor(3.5305e-05, device='cuda:0') tensor(1.4002e-08, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.154654
Average total loss: 2.457239
tensor(-8.8984, device='cuda:0') tensor(3.5303e-05, device='cuda:0') tensor(1.3658e-08, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302586
Average KL loss: 0.150848
Average total loss: 2.453434
tensor(-8.9233, device='cuda:0') tensor(3.5283e-05, device='cuda:0') tensor(1.3322e-08, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.147141
Average total loss: 2.449725
tensor(-8.9482, device='cuda:0') tensor(3.5263e-05, device='cuda:0') tensor(1.2995e-08, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.143529
Average total loss: 2.446114
tensor(-8.9730, device='cuda:0') tensor(3.5257e-05, device='cuda:0') tensor(1.2676e-08, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.140011
Average total loss: 2.442595
tensor(-8.9978, device='cuda:0') tensor(3.5252e-05, device='cuda:0') tensor(1.2365e-08, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302586
Average KL loss: 0.136583
Average total loss: 2.439169
tensor(-9.0226, device='cuda:0') tensor(3.5239e-05, device='cuda:0') tensor(1.2063e-08, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.133243
Average total loss: 2.435828
tensor(-9.0473, device='cuda:0') tensor(3.5165e-05, device='cuda:0') tensor(1.1768e-08, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.129990
Average total loss: 2.432575
tensor(-9.0720, device='cuda:0') tensor(3.5158e-05, device='cuda:0') tensor(1.1481e-08, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.126820
Average total loss: 2.429405
tensor(-9.0967, device='cuda:0') tensor(3.5153e-05, device='cuda:0') tensor(1.1201e-08, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.123731
Average total loss: 2.426316
tensor(-9.1213, device='cuda:0') tensor(3.5144e-05, device='cuda:0') tensor(1.0929e-08, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.120722
Average total loss: 2.423307
tensor(-9.1460, device='cuda:0') tensor(3.5134e-05, device='cuda:0') tensor(1.0663e-08, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.117789
Average total loss: 2.420374
tensor(-9.1705, device='cuda:0') tensor(3.5127e-05, device='cuda:0') tensor(1.0404e-08, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.114931
Average total loss: 2.417516
tensor(-9.1951, device='cuda:0') tensor(3.5117e-05, device='cuda:0') tensor(1.0152e-08, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.112146
Average total loss: 2.414731
tensor(-9.2196, device='cuda:0') tensor(3.5105e-05, device='cuda:0') tensor(9.9062e-09, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302586
Average KL loss: 0.109433
Average total loss: 2.412018
tensor(-9.2441, device='cuda:0') tensor(3.5008e-05, device='cuda:0') tensor(9.6666e-09, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 0.106787
Average total loss: 2.409373
tensor(-9.2685, device='cuda:0') tensor(3.4995e-05, device='cuda:0') tensor(9.4332e-09, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.104209
Average total loss: 2.406794
tensor(-9.2930, device='cuda:0') tensor(3.4984e-05, device='cuda:0') tensor(9.2056e-09, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.101697
Average total loss: 2.404282
tensor(-9.3173, device='cuda:0') tensor(3.4965e-05, device='cuda:0') tensor(8.9838e-09, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.099248
Average total loss: 2.401833
tensor(-9.3417, device='cuda:0') tensor(3.5002e-05, device='cuda:0') tensor(8.7677e-09, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.096861
Average total loss: 2.399446
tensor(-9.3660, device='cuda:0') tensor(3.5000e-05, device='cuda:0') tensor(8.5570e-09, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.094535
Average total loss: 2.397120
tensor(-9.3903, device='cuda:0') tensor(3.4985e-05, device='cuda:0') tensor(8.3516e-09, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.092267
Average total loss: 2.394852
tensor(-9.4146, device='cuda:0') tensor(3.4952e-05, device='cuda:0') tensor(8.1514e-09, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302582
Average KL loss: 0.090057
Average total loss: 2.392638
tensor(-9.4388, device='cuda:0') tensor(3.4934e-05, device='cuda:0') tensor(7.9562e-09, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.087902
Average total loss: 2.390487
tensor(-9.4630, device='cuda:0') tensor(3.4922e-05, device='cuda:0') tensor(7.7660e-09, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.085801
Average total loss: 2.388386
tensor(-9.4872, device='cuda:0') tensor(3.4921e-05, device='cuda:0') tensor(7.5806e-09, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.083753
Average total loss: 2.386338
tensor(-9.5114, device='cuda:0') tensor(3.4902e-05, device='cuda:0') tensor(7.3998e-09, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.081757
Average total loss: 2.384342
tensor(-9.5355, device='cuda:0') tensor(3.4882e-05, device='cuda:0') tensor(7.2235e-09, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.079811
Average total loss: 2.382395
tensor(-9.5595, device='cuda:0') tensor(3.4860e-05, device='cuda:0') tensor(7.0517e-09, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.077913
Average total loss: 2.380498
tensor(-9.5836, device='cuda:0') tensor(3.4815e-05, device='cuda:0') tensor(6.8841e-09, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302585
Average KL loss: 0.076063
Average total loss: 2.378648
tensor(-9.6076, device='cuda:0') tensor(3.4791e-05, device='cuda:0') tensor(6.7208e-09, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302585
Average KL loss: 0.074259
Average total loss: 2.376844
tensor(-9.6316, device='cuda:0') tensor(3.4762e-05, device='cuda:0') tensor(6.5616e-09, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302587
Average KL loss: 0.072501
Average total loss: 2.375088
tensor(-9.6555, device='cuda:0') tensor(3.4648e-05, device='cuda:0') tensor(6.4063e-09, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302587
Average KL loss: 0.070786
Average total loss: 2.373373
tensor(-9.6795, device='cuda:0') tensor(3.4422e-05, device='cuda:0') tensor(6.2548e-09, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302585
Average KL loss: 0.069114
Average total loss: 2.371699
tensor(-9.7034, device='cuda:0') tensor(3.4402e-05, device='cuda:0') tensor(6.1072e-09, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302585
Average KL loss: 0.067484
Average total loss: 2.370068
tensor(-9.7272, device='cuda:0') tensor(3.4434e-05, device='cuda:0') tensor(5.9633e-09, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302585
Average KL loss: 0.065894
Average total loss: 2.368479
tensor(-9.7510, device='cuda:0') tensor(3.4449e-05, device='cuda:0') tensor(5.8229e-09, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302585
Average KL loss: 0.064344
Average total loss: 2.366929
tensor(-9.7748, device='cuda:0') tensor(3.4432e-05, device='cuda:0') tensor(5.6861e-09, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302585
Average KL loss: 0.062833
Average total loss: 2.365417
tensor(-9.7986, device='cuda:0') tensor(3.4414e-05, device='cuda:0') tensor(5.5526e-09, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302585
Average KL loss: 0.061359
Average total loss: 2.363944
tensor(-9.8223, device='cuda:0') tensor(3.4396e-05, device='cuda:0') tensor(5.4224e-09, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302585
Average KL loss: 0.059922
Average total loss: 2.362506
tensor(-9.8460, device='cuda:0') tensor(3.4377e-05, device='cuda:0') tensor(5.2955e-09, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302586
Average KL loss: 0.058520
Average total loss: 2.361106
tensor(-9.8696, device='cuda:0') tensor(3.4356e-05, device='cuda:0') tensor(5.1717e-09, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302585
Average KL loss: 0.057153
Average total loss: 2.359738
tensor(-9.8933, device='cuda:0') tensor(3.4334e-05, device='cuda:0') tensor(5.0510e-09, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302585
Average KL loss: 0.055820
Average total loss: 2.358405
tensor(-9.9168, device='cuda:0') tensor(3.4311e-05, device='cuda:0') tensor(4.9333e-09, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302585
Average KL loss: 0.054520
Average total loss: 2.357105
tensor(-9.9404, device='cuda:0') tensor(3.4297e-05, device='cuda:0') tensor(4.8185e-09, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302585
Average KL loss: 0.053252
Average total loss: 2.355837
tensor(-9.9639, device='cuda:0') tensor(3.4273e-05, device='cuda:0') tensor(4.7066e-09, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302585
Average KL loss: 0.052015
Average total loss: 2.354600
tensor(-9.9874, device='cuda:0') tensor(3.4247e-05, device='cuda:0') tensor(4.5974e-09, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302585
Average KL loss: 0.050809
Average total loss: 2.353394
tensor(-10.0108, device='cuda:0') tensor(3.4220e-05, device='cuda:0') tensor(4.4909e-09, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302582
Average KL loss: 0.049633
Average total loss: 2.352216
 Percentile value: -10.034283638000488
Non-zero model percentage: 80.0%, Non-zero mask percentage: 80.0%

--- Pruning Level [1/24]: ---
conv1.weight         | nonzeros =    1455 /    1728             ( 84.20%) | total_pruned =     273 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      56 /      64             ( 87.50%) | total_pruned =       8 | shape = torch.Size([64])
bn1.bias             | nonzeros =      54 /      64             ( 84.38%) | total_pruned =      10 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   31522 /   36864             ( 85.51%) | total_pruned =    5342 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   24823 /   36864             ( 67.34%) | total_pruned =   12041 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   25176 /   36864             ( 68.29%) | total_pruned =   11688 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   23096 /   36864             ( 62.65%) | total_pruned =   13768 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   39168 /   73728             ( 53.12%) | total_pruned =   34560 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   80840 /  147456             ( 54.82%) | total_pruned =   66616 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    4495 /    8192             ( 54.87%) | total_pruned =    3697 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  105466 /  147456             ( 71.52%) | total_pruned =   41990 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  112106 /  147456             ( 76.03%) | total_pruned =   35350 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  161600 /  294912             ( 54.80%) | total_pruned =  133312 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     193 /     256             ( 75.39%) | total_pruned =      63 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  382334 /  589824             ( 64.82%) | total_pruned =  207490 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     178 /     256             ( 69.53%) | total_pruned =      78 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   20838 /   32768             ( 63.59%) | total_pruned =   11930 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  486598 /  589824             ( 82.50%) | total_pruned =  103226 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     129 /     256             ( 50.39%) | total_pruned =     127 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  511908 /  589824             ( 86.79%) | total_pruned =   77916 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     140 /     256             ( 54.69%) | total_pruned =     116 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  870936 / 1179648             ( 73.83%) | total_pruned =  308712 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     340 /     512             ( 66.41%) | total_pruned =     172 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1992176 / 2359296             ( 84.44%) | total_pruned =  367120 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     239 /     512             ( 46.68%) | total_pruned =     273 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     414 /     512             ( 80.86%) | total_pruned =      98 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  108037 /  131072             ( 82.43%) | total_pruned =   23035 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     208 /     512             ( 40.62%) | total_pruned =     304 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1972794 / 2359296             ( 83.62%) | total_pruned =  386502 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8943010, pruned : 2235752, total: 11178762, Compression rate :       1.25x  ( 20.00% pruned)
Train Epoch: 65/100 Loss: 0.020199 Accuracy: 87.21 100.00 % Best test Accuracy: 87.32%
tensor(-10.0342, device='cuda:0') tensor(3.4199e-05, device='cuda:0') tensor(4.3870e-09, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.045705
Average total loss: 2.348290
tensor(-10.1775, device='cuda:0') tensor(2.7392e-05, device='cuda:0') tensor(3.8015e-09, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.039888
Average total loss: 2.342473
tensor(-10.3058, device='cuda:0') tensor(2.2577e-05, device='cuda:0') tensor(3.3436e-09, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.035311
Average total loss: 2.337896
tensor(-10.4212, device='cuda:0') tensor(1.9143e-05, device='cuda:0') tensor(2.9794e-09, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302584
Average KL loss: 0.031632
Average total loss: 2.334216
tensor(-10.5258, device='cuda:0') tensor(1.6583e-05, device='cuda:0') tensor(2.6833e-09, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.028614
Average total loss: 2.331199
tensor(-10.6216, device='cuda:0') tensor(1.4611e-05, device='cuda:0') tensor(2.4382e-09, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.026097
Average total loss: 2.328681
tensor(-10.7099, device='cuda:0') tensor(1.3050e-05, device='cuda:0') tensor(2.2321e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.023967
Average total loss: 2.326552
tensor(-10.7918, device='cuda:0') tensor(1.1777e-05, device='cuda:0') tensor(2.0566e-09, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.022143
Average total loss: 2.324728
tensor(-10.8681, device='cuda:0') tensor(1.0726e-05, device='cuda:0') tensor(1.9055e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.020565
Average total loss: 2.323150
tensor(-10.9396, device='cuda:0') tensor(9.8495e-06, device='cuda:0') tensor(1.7741e-09, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.019187
Average total loss: 2.321772
tensor(-11.0068, device='cuda:0') tensor(9.1009e-06, device='cuda:0') tensor(1.6588e-09, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.017974
Average total loss: 2.320559
tensor(-11.0702, device='cuda:0') tensor(8.4592e-06, device='cuda:0') tensor(1.5569e-09, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.016898
Average total loss: 2.319483
tensor(-11.1302, device='cuda:0') tensor(7.9039e-06, device='cuda:0') tensor(1.4662e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.015938
Average total loss: 2.318522
tensor(-11.1872, device='cuda:0') tensor(7.4194e-06, device='cuda:0') tensor(1.3850e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.015076
Average total loss: 2.317661
tensor(-11.2414, device='cuda:0') tensor(6.9936e-06, device='cuda:0') tensor(1.3119e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.014298
Average total loss: 2.316883
tensor(-11.2931, device='cuda:0') tensor(6.6171e-06, device='cuda:0') tensor(1.2458e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.013593
Average total loss: 2.316178
tensor(-11.3425, device='cuda:0') tensor(6.2823e-06, device='cuda:0') tensor(1.1858e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.012951
Average total loss: 2.315536
tensor(-11.3898, device='cuda:0') tensor(5.9821e-06, device='cuda:0') tensor(1.1310e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.012364
Average total loss: 2.314949
tensor(-11.4352, device='cuda:0') tensor(5.7119e-06, device='cuda:0') tensor(1.0808e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.011826
Average total loss: 2.314411
tensor(-11.4789, device='cuda:0') tensor(5.4678e-06, device='cuda:0') tensor(1.0346e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.011330
Average total loss: 2.313915
tensor(-11.5208, device='cuda:0') tensor(5.2463e-06, device='cuda:0') tensor(9.9211e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.010872
Average total loss: 2.313457
tensor(-11.5613, device='cuda:0') tensor(5.0444e-06, device='cuda:0') tensor(9.5277e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.010449
Average total loss: 2.313034
tensor(-11.6003, device='cuda:0') tensor(4.8598e-06, device='cuda:0') tensor(9.1630e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.010055
Average total loss: 2.312640
tensor(-11.6380, device='cuda:0') tensor(4.6904e-06, device='cuda:0') tensor(8.8238e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.009689
Average total loss: 2.312274
tensor(-11.6745, device='cuda:0') tensor(4.5345e-06, device='cuda:0') tensor(8.5078e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.009347
Average total loss: 2.311932
tensor(-11.7098, device='cuda:0') tensor(4.3906e-06, device='cuda:0') tensor(8.2125e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.009028
Average total loss: 2.311613
tensor(-11.7441, device='cuda:0') tensor(4.2573e-06, device='cuda:0') tensor(7.9362e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.008728
Average total loss: 2.311313
tensor(-11.7773, device='cuda:0') tensor(4.1336e-06, device='cuda:0') tensor(7.6770e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.008447
Average total loss: 2.311032
tensor(-11.8095, device='cuda:0') tensor(4.0184e-06, device='cuda:0') tensor(7.4334e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.008183
Average total loss: 2.310768
tensor(-11.8409, device='cuda:0') tensor(3.9110e-06, device='cuda:0') tensor(7.2041e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.007934
Average total loss: 2.310519
tensor(-11.8713, device='cuda:0') tensor(3.8106e-06, device='cuda:0') tensor(6.9879e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.007699
Average total loss: 2.310284
tensor(-11.9010, device='cuda:0') tensor(3.7166e-06, device='cuda:0') tensor(6.7836e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.007477
Average total loss: 2.310061
tensor(-11.9299, device='cuda:0') tensor(3.6283e-06, device='cuda:0') tensor(6.5905e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.007266
Average total loss: 2.309851
tensor(-11.9580, device='cuda:0') tensor(3.5454e-06, device='cuda:0') tensor(6.4076e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.007067
Average total loss: 2.309652
tensor(-11.9855, device='cuda:0') tensor(3.4672e-06, device='cuda:0') tensor(6.2340e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.006878
Average total loss: 2.309463
tensor(-12.0123, device='cuda:0') tensor(3.3935e-06, device='cuda:0') tensor(6.0693e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.006698
Average total loss: 2.309283
tensor(-12.0384, device='cuda:0') tensor(3.3238e-06, device='cuda:0') tensor(5.9126e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.006527
Average total loss: 2.309112
tensor(-12.0640, device='cuda:0') tensor(3.2578e-06, device='cuda:0') tensor(5.7634e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.006364
Average total loss: 2.308949
tensor(-12.0889, device='cuda:0') tensor(3.1952e-06, device='cuda:0') tensor(5.6212e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.006209
Average total loss: 2.308794
tensor(-12.1134, device='cuda:0') tensor(3.1358e-06, device='cuda:0') tensor(5.4856e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.006061
Average total loss: 2.308646
tensor(-12.1373, device='cuda:0') tensor(3.0793e-06, device='cuda:0') tensor(5.3561e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.005919
Average total loss: 2.308504
tensor(-12.1607, device='cuda:0') tensor(3.0256e-06, device='cuda:0') tensor(5.2323e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.005784
Average total loss: 2.308369
tensor(-12.1836, device='cuda:0') tensor(2.9744e-06, device='cuda:0') tensor(5.1138e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.005654
Average total loss: 2.308239
tensor(-12.2060, device='cuda:0') tensor(2.9255e-06, device='cuda:0') tensor(5.0004e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.005530
Average total loss: 2.308115
tensor(-12.2280, device='cuda:0') tensor(2.8788e-06, device='cuda:0') tensor(4.8916e-10, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.005411
Average total loss: 2.307996
tensor(-12.2495, device='cuda:0') tensor(2.8341e-06, device='cuda:0') tensor(4.7873e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.005296
Average total loss: 2.307881
tensor(-12.2707, device='cuda:0') tensor(2.7913e-06, device='cuda:0') tensor(4.6871e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.005187
Average total loss: 2.307772
tensor(-12.2914, device='cuda:0') tensor(2.7503e-06, device='cuda:0') tensor(4.5909e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.005081
Average total loss: 2.307666
tensor(-12.3118, device='cuda:0') tensor(2.7109e-06, device='cuda:0') tensor(4.4984e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.004980
Average total loss: 2.307564
tensor(-12.3318, device='cuda:0') tensor(2.6731e-06, device='cuda:0') tensor(4.4093e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.004882
Average total loss: 2.307467
tensor(-12.3514, device='cuda:0') tensor(2.6345e-06, device='cuda:0') tensor(4.3236e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.004788
Average total loss: 2.307373
tensor(-12.3707, device='cuda:0') tensor(2.5995e-06, device='cuda:0') tensor(4.2410e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.004697
Average total loss: 2.307282
tensor(-12.3896, device='cuda:0') tensor(2.5658e-06, device='cuda:0') tensor(4.1614e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.004610
Average total loss: 2.307195
tensor(-12.4083, device='cuda:0') tensor(2.5334e-06, device='cuda:0') tensor(4.0846e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.004525
Average total loss: 2.307110
tensor(-12.4266, device='cuda:0') tensor(2.5020e-06, device='cuda:0') tensor(4.0105e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.004444
Average total loss: 2.307029
tensor(-12.4446, device='cuda:0') tensor(2.4718e-06, device='cuda:0') tensor(3.9388e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.004365
Average total loss: 2.306950
tensor(-12.4623, device='cuda:0') tensor(2.4425e-06, device='cuda:0') tensor(3.8696e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.004289
Average total loss: 2.306874
tensor(-12.4798, device='cuda:0') tensor(2.4142e-06, device='cuda:0') tensor(3.8027e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.004215
Average total loss: 2.306800
tensor(-12.4970, device='cuda:0') tensor(2.3869e-06, device='cuda:0') tensor(3.7380e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.004144
Average total loss: 2.306729
tensor(-12.5139, device='cuda:0') tensor(2.3603e-06, device='cuda:0') tensor(3.6754e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.004075
Average total loss: 2.306660
tensor(-12.5305, device='cuda:0') tensor(2.3346e-06, device='cuda:0') tensor(3.6147e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.004008
Average total loss: 2.306593
tensor(-12.5469, device='cuda:0') tensor(2.3098e-06, device='cuda:0') tensor(3.5559e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.003944
Average total loss: 2.306529
tensor(-12.5630, device='cuda:0') tensor(2.2855e-06, device='cuda:0') tensor(3.4989e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.003881
Average total loss: 2.306466
tensor(-12.5790, device='cuda:0') tensor(2.2620e-06, device='cuda:0') tensor(3.4437e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.003820
Average total loss: 2.306405
tensor(-12.5947, device='cuda:0') tensor(2.2391e-06, device='cuda:0') tensor(3.3901e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.003761
Average total loss: 2.306346
tensor(-12.6101, device='cuda:0') tensor(2.2168e-06, device='cuda:0') tensor(3.3381e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.003704
Average total loss: 2.306289
tensor(-12.6254, device='cuda:0') tensor(2.1951e-06, device='cuda:0') tensor(3.2876e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.003648
Average total loss: 2.306233
tensor(-12.6404, device='cuda:0') tensor(2.1740e-06, device='cuda:0') tensor(3.2385e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.003594
Average total loss: 2.306179
tensor(-12.6552, device='cuda:0') tensor(2.1534e-06, device='cuda:0') tensor(3.1909e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302585
Average KL loss: 0.003542
Average total loss: 2.306126
tensor(-12.6698, device='cuda:0') tensor(2.1333e-06, device='cuda:0') tensor(3.1445e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.003490
Average total loss: 2.306075
tensor(-12.6843, device='cuda:0') tensor(2.1137e-06, device='cuda:0') tensor(3.0995e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.003441
Average total loss: 2.306026
tensor(-12.6985, device='cuda:0') tensor(2.0946e-06, device='cuda:0') tensor(3.0556e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.003392
Average total loss: 2.305977
tensor(-12.7126, device='cuda:0') tensor(2.0759e-06, device='cuda:0') tensor(3.0130e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.003345
Average total loss: 2.305930
tensor(-12.7265, device='cuda:0') tensor(2.0576e-06, device='cuda:0') tensor(2.9715e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.003300
Average total loss: 2.305884
tensor(-12.7402, device='cuda:0') tensor(2.0397e-06, device='cuda:0') tensor(2.9310e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.003255
Average total loss: 2.305840
tensor(-12.7537, device='cuda:0') tensor(2.0222e-06, device='cuda:0') tensor(2.8916e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.003211
Average total loss: 2.305796
tensor(-12.7670, device='cuda:0') tensor(2.0050e-06, device='cuda:0') tensor(2.8533e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 0.003169
Average total loss: 2.305754
tensor(-12.7802, device='cuda:0') tensor(1.9882e-06, device='cuda:0') tensor(2.8159e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.003128
Average total loss: 2.305713
tensor(-12.7933, device='cuda:0') tensor(1.9717e-06, device='cuda:0') tensor(2.7794e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302585
Average KL loss: 0.003087
Average total loss: 2.305672
tensor(-12.8062, device='cuda:0') tensor(1.9555e-06, device='cuda:0') tensor(2.7438e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.003048
Average total loss: 2.305633
tensor(-12.8189, device='cuda:0') tensor(1.9397e-06, device='cuda:0') tensor(2.7091e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.003010
Average total loss: 2.305595
tensor(-12.8315, device='cuda:0') tensor(1.9241e-06, device='cuda:0') tensor(2.6753e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.002972
Average total loss: 2.305557
tensor(-12.8439, device='cuda:0') tensor(1.9088e-06, device='cuda:0') tensor(2.6422e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.002936
Average total loss: 2.305521
tensor(-12.8562, device='cuda:0') tensor(1.8937e-06, device='cuda:0') tensor(2.6099e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 0.002900
Average total loss: 2.305485
tensor(-12.8683, device='cuda:0') tensor(1.8789e-06, device='cuda:0') tensor(2.5784e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.002865
Average total loss: 2.305450
tensor(-12.8804, device='cuda:0') tensor(1.8643e-06, device='cuda:0') tensor(2.5476e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 0.002831
Average total loss: 2.305416
tensor(-12.8922, device='cuda:0') tensor(1.8500e-06, device='cuda:0') tensor(2.5175e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302585
Average KL loss: 0.002798
Average total loss: 2.305383
tensor(-12.9040, device='cuda:0') tensor(1.8359e-06, device='cuda:0') tensor(2.4881e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 0.002766
Average total loss: 2.305350
tensor(-12.9156, device='cuda:0') tensor(1.8220e-06, device='cuda:0') tensor(2.4594e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.002734
Average total loss: 2.305319
tensor(-12.9271, device='cuda:0') tensor(1.8082e-06, device='cuda:0') tensor(2.4313e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.002703
Average total loss: 2.305288
tensor(-12.9385, device='cuda:0') tensor(1.7947e-06, device='cuda:0') tensor(2.4038e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.002672
Average total loss: 2.305257
tensor(-12.9497, device='cuda:0') tensor(1.7814e-06, device='cuda:0') tensor(2.3769e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.002643
Average total loss: 2.305227
tensor(-12.9609, device='cuda:0') tensor(1.7682e-06, device='cuda:0') tensor(2.3505e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 0.002613
Average total loss: 2.305198
tensor(-12.9719, device='cuda:0') tensor(1.7552e-06, device='cuda:0') tensor(2.3248e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 0.002585
Average total loss: 2.305170
tensor(-12.9828, device='cuda:0') tensor(1.7423e-06, device='cuda:0') tensor(2.2996e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 0.002557
Average total loss: 2.305142
tensor(-12.9936, device='cuda:0') tensor(1.7296e-06, device='cuda:0') tensor(2.2749e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.002530
Average total loss: 2.305115
tensor(-13.0043, device='cuda:0') tensor(1.7171e-06, device='cuda:0') tensor(2.2507e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 0.002503
Average total loss: 2.305088
tensor(-13.0149, device='cuda:0') tensor(1.7047e-06, device='cuda:0') tensor(2.2270e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.002477
Average total loss: 2.305062
tensor(-13.0253, device='cuda:0') tensor(1.6924e-06, device='cuda:0') tensor(2.2038e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 0.002451
Average total loss: 2.305036
tensor(-13.0357, device='cuda:0') tensor(1.6802e-06, device='cuda:0') tensor(2.1810e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.002426
Average total loss: 2.305011
tensor(-13.0460, device='cuda:0') tensor(1.6682e-06, device='cuda:0') tensor(2.1587e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 0.002401
Average total loss: 2.304986
tensor(-13.0562, device='cuda:0') tensor(1.6561e-06, device='cuda:0') tensor(2.1369e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.002377
Average total loss: 2.304962
tensor(-13.0662, device='cuda:0') tensor(1.6442e-06, device='cuda:0') tensor(2.1155e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.002353
Average total loss: 2.304938
tensor(-13.0762, device='cuda:0') tensor(1.6325e-06, device='cuda:0') tensor(2.0944e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.002330
Average total loss: 2.304915
tensor(-13.0861, device='cuda:0') tensor(1.6209e-06, device='cuda:0') tensor(2.0738e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.002307
Average total loss: 2.304892
tensor(-13.0959, device='cuda:0') tensor(1.6094e-06, device='cuda:0') tensor(2.0536e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.002285
Average total loss: 2.304870
tensor(-13.1056, device='cuda:0') tensor(1.5980e-06, device='cuda:0') tensor(2.0338e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.002263
Average total loss: 2.304848
tensor(-13.1152, device='cuda:0') tensor(1.5867e-06, device='cuda:0') tensor(2.0143e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.002241
Average total loss: 2.304826
tensor(-13.1248, device='cuda:0') tensor(1.5754e-06, device='cuda:0') tensor(1.9952e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.002220
Average total loss: 2.304805
tensor(-13.1342, device='cuda:0') tensor(1.5642e-06, device='cuda:0') tensor(1.9765e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.002199
Average total loss: 2.304784
tensor(-13.1436, device='cuda:0') tensor(1.5531e-06, device='cuda:0') tensor(1.9580e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.002179
Average total loss: 2.304764
tensor(-13.1528, device='cuda:0') tensor(1.5421e-06, device='cuda:0') tensor(1.9400e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.002159
Average total loss: 2.304744
tensor(-13.1620, device='cuda:0') tensor(1.5311e-06, device='cuda:0') tensor(1.9222e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.002139
Average total loss: 2.304724
tensor(-13.1712, device='cuda:0') tensor(1.5203e-06, device='cuda:0') tensor(1.9048e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.002120
Average total loss: 2.304705
tensor(-13.1802, device='cuda:0') tensor(1.5095e-06, device='cuda:0') tensor(1.8876e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.002101
Average total loss: 2.304686
tensor(-13.1892, device='cuda:0') tensor(1.4987e-06, device='cuda:0') tensor(1.8708e-10, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 0.002082
Average total loss: 2.304667
tensor(-13.1980, device='cuda:0') tensor(1.4880e-06, device='cuda:0') tensor(1.8542e-10, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 0.002064
Average total loss: 2.304649
tensor(-13.2068, device='cuda:0') tensor(1.4773e-06, device='cuda:0') tensor(1.8380e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.002054
Average total loss: 2.304639
tensor(-13.2077, device='cuda:0') tensor(1.4764e-06, device='cuda:0') tensor(1.8363e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.002052
Average total loss: 2.304637
tensor(-13.2086, device='cuda:0') tensor(1.4754e-06, device='cuda:0') tensor(1.8347e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.002050
Average total loss: 2.304635
tensor(-13.2095, device='cuda:0') tensor(1.4745e-06, device='cuda:0') tensor(1.8331e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304633
tensor(-13.2104, device='cuda:0') tensor(1.4735e-06, device='cuda:0') tensor(1.8314e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.002046
Average total loss: 2.304631
tensor(-13.2113, device='cuda:0') tensor(1.4725e-06, device='cuda:0') tensor(1.8298e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.002045
Average total loss: 2.304630
tensor(-13.2122, device='cuda:0') tensor(1.4715e-06, device='cuda:0') tensor(1.8282e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302585
Average KL loss: 0.002043
Average total loss: 2.304628
tensor(-13.2131, device='cuda:0') tensor(1.4704e-06, device='cuda:0') tensor(1.8266e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 0.002041
Average total loss: 2.304626
tensor(-13.2140, device='cuda:0') tensor(1.4694e-06, device='cuda:0') tensor(1.8250e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.002039
Average total loss: 2.304624
tensor(-13.2148, device='cuda:0') tensor(1.4683e-06, device='cuda:0') tensor(1.8233e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.002037
Average total loss: 2.304622
tensor(-13.2157, device='cuda:0') tensor(1.4672e-06, device='cuda:0') tensor(1.8217e-10, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.002036
Average total loss: 2.304620
tensor(-13.2166, device='cuda:0') tensor(1.4661e-06, device='cuda:0') tensor(1.8201e-10, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.002034
Average total loss: 2.304619
tensor(-13.2175, device='cuda:0') tensor(1.4649e-06, device='cuda:0') tensor(1.8185e-10, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.002033
Average total loss: 2.304618
tensor(-13.2176, device='cuda:0') tensor(1.4648e-06, device='cuda:0') tensor(1.8183e-10, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.002033
Average total loss: 2.304617
tensor(-13.2177, device='cuda:0') tensor(1.4647e-06, device='cuda:0') tensor(1.8181e-10, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.002032
Average total loss: 2.304617
tensor(-13.2178, device='cuda:0') tensor(1.4647e-06, device='cuda:0') tensor(1.8180e-10, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 0.002032
Average total loss: 2.304617
tensor(-13.2179, device='cuda:0') tensor(1.4646e-06, device='cuda:0') tensor(1.8178e-10, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.002032
Average total loss: 2.304617
tensor(-13.2180, device='cuda:0') tensor(1.4645e-06, device='cuda:0') tensor(1.8176e-10, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.002032
Average total loss: 2.304617
tensor(-13.2181, device='cuda:0') tensor(1.4644e-06, device='cuda:0') tensor(1.8175e-10, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.002032
Average total loss: 2.304616
tensor(-13.2182, device='cuda:0') tensor(1.4643e-06, device='cuda:0') tensor(1.8173e-10, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2183, device='cuda:0') tensor(1.4642e-06, device='cuda:0') tensor(1.9562e-10, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302588
Average KL loss: 0.002031
Average total loss: 2.304619
tensor(-13.2183, device='cuda:0') tensor(1.4625e-06, device='cuda:0') tensor(1.8170e-10, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2184, device='cuda:0') tensor(1.4625e-06, device='cuda:0') tensor(1.8168e-10, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4625e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4625e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4624e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4623e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304616
tensor(-13.2185, device='cuda:0') tensor(1.4623e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
 Percentile value: -13.218538284301758
Non-zero model percentage: 64.0%, Non-zero mask percentage: 64.0%

--- Pruning Level [2/24]: ---
conv1.weight         | nonzeros =     327 /    1728             ( 18.92%) | total_pruned =    1401 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3433 /   36864             (  9.31%) | total_pruned =   33431 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6746 /   36864             ( 18.30%) | total_pruned =   30118 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6997 /   36864             ( 18.98%) | total_pruned =   29867 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7346 /   36864             ( 19.93%) | total_pruned =   29518 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20162 /   73728             ( 27.35%) | total_pruned =   53566 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36397 /  147456             ( 24.68%) | total_pruned =  111059 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2484 /    8192             ( 30.32%) | total_pruned =    5708 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13360 /  147456             (  9.06%) | total_pruned =  134096 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      69 /     128             ( 53.91%) | total_pruned =      59 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10332 /  147456             (  7.01%) | total_pruned =  137124 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   78017 /  294912             ( 26.45%) | total_pruned =  216895 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   90477 /  589824             ( 15.34%) | total_pruned =  499347 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5533 /   32768             ( 16.89%) | total_pruned =   27235 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21142 /  589824             (  3.58%) | total_pruned =  568682 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14994 /  589824             (  2.54%) | total_pruned =  574830 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  778420 / 1179648             ( 65.99%) | total_pruned =  401228 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     340 /     512             ( 66.41%) | total_pruned =     172 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1992176 / 2359296             ( 84.44%) | total_pruned =  367120 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     239 /     512             ( 46.68%) | total_pruned =     273 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     414 /     512             ( 80.86%) | total_pruned =      98 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  108037 /  131072             ( 82.43%) | total_pruned =   23035 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     208 /     512             ( 40.62%) | total_pruned =     304 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1972794 / 2359296             ( 83.62%) | total_pruned =  386502 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 7154408, pruned : 4024354, total: 11178762, Compression rate :       1.56x  ( 36.00% pruned)
Train Epoch: 55/100 Loss: 0.016423 Accuracy: 84.30 100.00 % Best test Accuracy: 84.36%
tensor(-13.2185, device='cuda:0') tensor(1.4623e-06, device='cuda:0') tensor(1.8166e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.002022
Average total loss: 2.304607
tensor(-13.2273, device='cuda:0') tensor(1.3820e-06, device='cuda:0') tensor(1.8009e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.002005
Average total loss: 2.304590
tensor(-13.2359, device='cuda:0') tensor(1.3110e-06, device='cuda:0') tensor(1.7853e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001987
Average total loss: 2.304572
tensor(-13.2445, device='cuda:0') tensor(1.2478e-06, device='cuda:0') tensor(1.7701e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001971
Average total loss: 2.304555
tensor(-13.2530, device='cuda:0') tensor(1.1912e-06, device='cuda:0') tensor(1.7551e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001954
Average total loss: 2.304539
tensor(-13.2614, device='cuda:0') tensor(1.1399e-06, device='cuda:0') tensor(1.7404e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001938
Average total loss: 2.304522
tensor(-13.2697, device='cuda:0') tensor(1.0932e-06, device='cuda:0') tensor(1.7259e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001922
Average total loss: 2.304506
tensor(-13.2780, device='cuda:0') tensor(1.0505e-06, device='cuda:0') tensor(1.7117e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001906
Average total loss: 2.304491
tensor(-13.2863, device='cuda:0') tensor(1.0111e-06, device='cuda:0') tensor(1.6977e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001890
Average total loss: 2.304475
tensor(-13.2944, device='cuda:0') tensor(9.7464e-07, device='cuda:0') tensor(1.6839e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001875
Average total loss: 2.304460
tensor(-13.3025, device='cuda:0') tensor(9.4082e-07, device='cuda:0') tensor(1.6703e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001860
Average total loss: 2.304445
tensor(-13.3105, device='cuda:0') tensor(9.0923e-07, device='cuda:0') tensor(1.6569e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001845
Average total loss: 2.304430
tensor(-13.3185, device='cuda:0') tensor(8.7973e-07, device='cuda:0') tensor(1.6438e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001837
Average total loss: 2.304422
tensor(-13.3193, device='cuda:0') tensor(8.7695e-07, device='cuda:0') tensor(1.6425e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304420
tensor(-13.3201, device='cuda:0') tensor(8.7419e-07, device='cuda:0') tensor(1.6412e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-13.3209, device='cuda:0') tensor(8.7144e-07, device='cuda:0') tensor(1.6399e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001832
Average total loss: 2.304417
tensor(-13.3217, device='cuda:0') tensor(8.6869e-07, device='cuda:0') tensor(1.6386e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001831
Average total loss: 2.304416
tensor(-13.3225, device='cuda:0') tensor(8.6596e-07, device='cuda:0') tensor(1.6373e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001830
Average total loss: 2.304414
tensor(-13.3233, device='cuda:0') tensor(8.6324e-07, device='cuda:0') tensor(1.6360e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001828
Average total loss: 2.304413
tensor(-13.3241, device='cuda:0') tensor(8.6054e-07, device='cuda:0') tensor(1.6347e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001827
Average total loss: 2.304412
tensor(-13.3249, device='cuda:0') tensor(8.5786e-07, device='cuda:0') tensor(1.6334e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001825
Average total loss: 2.304410
tensor(-13.3256, device='cuda:0') tensor(8.5520e-07, device='cuda:0') tensor(1.6321e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001824
Average total loss: 2.304409
tensor(-13.3264, device='cuda:0') tensor(8.5256e-07, device='cuda:0') tensor(1.6308e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001822
Average total loss: 2.304407
tensor(-13.3272, device='cuda:0') tensor(8.4994e-07, device='cuda:0') tensor(1.6295e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001822
Average total loss: 2.304406
tensor(-13.3273, device='cuda:0') tensor(8.4973e-07, device='cuda:0') tensor(1.6294e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-13.3274, device='cuda:0') tensor(8.4952e-07, device='cuda:0') tensor(1.6292e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-13.3275, device='cuda:0') tensor(8.4931e-07, device='cuda:0') tensor(1.6290e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-13.3276, device='cuda:0') tensor(8.4911e-07, device='cuda:0') tensor(1.6289e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-13.3277, device='cuda:0') tensor(8.4907e-07, device='cuda:0') tensor(1.6287e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-13.3278, device='cuda:0') tensor(8.4887e-07, device='cuda:0') tensor(1.6286e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-13.3279, device='cuda:0') tensor(8.4867e-07, device='cuda:0') tensor(1.6284e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-13.3280, device='cuda:0') tensor(8.4847e-07, device='cuda:0') tensor(1.6283e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-13.3281, device='cuda:0') tensor(8.4827e-07, device='cuda:0') tensor(1.6281e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-13.3282, device='cuda:0') tensor(8.4807e-07, device='cuda:0') tensor(1.6280e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304405
tensor(-13.3283, device='cuda:0') tensor(8.4787e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4784e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4781e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4778e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4775e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4772e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4770e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4767e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4764e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4761e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001820
Average total loss: 2.304404
tensor(-13.3283, device='cuda:0') tensor(8.4758e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
 Percentile value: -13.328265190124512
Non-zero model percentage: 51.200008392333984%, Non-zero mask percentage: 51.200008392333984%

--- Pruning Level [3/24]: ---
conv1.weight         | nonzeros =     327 /    1728             ( 18.92%) | total_pruned =    1401 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3433 /   36864             (  9.31%) | total_pruned =   33431 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6746 /   36864             ( 18.30%) | total_pruned =   30118 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6997 /   36864             ( 18.98%) | total_pruned =   29867 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7346 /   36864             ( 19.93%) | total_pruned =   29518 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20162 /   73728             ( 27.35%) | total_pruned =   53566 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36397 /  147456             ( 24.68%) | total_pruned =  111059 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2484 /    8192             ( 30.32%) | total_pruned =    5708 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13360 /  147456             (  9.06%) | total_pruned =  134096 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      69 /     128             ( 53.91%) | total_pruned =      59 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10332 /  147456             (  7.01%) | total_pruned =  137124 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   78017 /  294912             ( 26.45%) | total_pruned =  216895 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   90477 /  589824             ( 15.34%) | total_pruned =  499347 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5533 /   32768             ( 16.89%) | total_pruned =   27235 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21142 /  589824             (  3.58%) | total_pruned =  568682 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14994 /  589824             (  2.54%) | total_pruned =  574830 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  107982 / 1179648             (  9.15%) | total_pruned = 1071666 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     327 /     512             ( 63.87%) | total_pruned =     185 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1232271 / 2359296             ( 52.23%) | total_pruned = 1127025 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     239 /     512             ( 46.68%) | total_pruned =     273 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     414 /     512             ( 80.86%) | total_pruned =      98 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  108037 /  131072             ( 82.43%) | total_pruned =   23035 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     208 /     512             ( 40.62%) | total_pruned =     304 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1972794 / 2359296             ( 83.62%) | total_pruned =  386502 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 5723527, pruned : 5455235, total: 11178762, Compression rate :       1.95x  ( 48.80% pruned)
Train Epoch: 68/100 Loss: 0.014594 Accuracy: 83.38 100.00 % Best test Accuracy: 83.80%
tensor(-13.3283, device='cuda:0') tensor(8.4755e-07, device='cuda:0') tensor(1.6278e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001813
Average total loss: 2.304398
tensor(-13.3361, device='cuda:0') tensor(8.2062e-07, device='cuda:0') tensor(1.6151e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001799
Average total loss: 2.304384
tensor(-13.3439, device='cuda:0') tensor(7.9528e-07, device='cuda:0') tensor(1.6026e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001785
Average total loss: 2.304370
tensor(-13.3516, device='cuda:0') tensor(7.7143e-07, device='cuda:0') tensor(1.5903e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001771
Average total loss: 2.304356
tensor(-13.3592, device='cuda:0') tensor(7.4892e-07, device='cuda:0') tensor(1.5782e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001758
Average total loss: 2.304343
tensor(-13.3668, device='cuda:0') tensor(7.2768e-07, device='cuda:0') tensor(1.5663e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001744
Average total loss: 2.304329
tensor(-13.3744, device='cuda:0') tensor(7.0759e-07, device='cuda:0') tensor(1.5545e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001731
Average total loss: 2.304316
tensor(-13.3818, device='cuda:0') tensor(6.8854e-07, device='cuda:0') tensor(1.5429e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001719
Average total loss: 2.304303
tensor(-13.3893, device='cuda:0') tensor(6.7039e-07, device='cuda:0') tensor(1.5315e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001706
Average total loss: 2.304291
tensor(-13.3966, device='cuda:0') tensor(6.5312e-07, device='cuda:0') tensor(1.5203e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001693
Average total loss: 2.304278
tensor(-13.4039, device='cuda:0') tensor(6.3667e-07, device='cuda:0') tensor(1.5092e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001681
Average total loss: 2.304266
tensor(-13.4112, device='cuda:0') tensor(6.2097e-07, device='cuda:0') tensor(1.4982e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001669
Average total loss: 2.304254
tensor(-13.4184, device='cuda:0') tensor(6.0598e-07, device='cuda:0') tensor(1.4875e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001662
Average total loss: 2.304247
tensor(-13.4191, device='cuda:0') tensor(6.0436e-07, device='cuda:0') tensor(1.4864e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-13.4198, device='cuda:0') tensor(6.0277e-07, device='cuda:0') tensor(1.4854e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-13.4205, device='cuda:0') tensor(6.0118e-07, device='cuda:0') tensor(1.4844e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001659
Average total loss: 2.304244
tensor(-13.4212, device='cuda:0') tensor(5.9960e-07, device='cuda:0') tensor(1.4833e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001658
Average total loss: 2.304242
tensor(-13.4219, device='cuda:0') tensor(5.9804e-07, device='cuda:0') tensor(1.4823e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001656
Average total loss: 2.304241
tensor(-13.4226, device='cuda:0') tensor(5.9649e-07, device='cuda:0') tensor(1.4812e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001655
Average total loss: 2.304240
tensor(-13.4233, device='cuda:0') tensor(5.9494e-07, device='cuda:0') tensor(1.4802e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001654
Average total loss: 2.304239
tensor(-13.4240, device='cuda:0') tensor(5.9341e-07, device='cuda:0') tensor(1.4792e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001653
Average total loss: 2.304238
tensor(-13.4247, device='cuda:0') tensor(5.9188e-07, device='cuda:0') tensor(1.4781e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001652
Average total loss: 2.304237
tensor(-13.4254, device='cuda:0') tensor(5.9037e-07, device='cuda:0') tensor(1.4771e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001651
Average total loss: 2.304236
tensor(-13.4261, device='cuda:0') tensor(5.8887e-07, device='cuda:0') tensor(1.4761e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304235
tensor(-13.4262, device='cuda:0') tensor(5.8877e-07, device='cuda:0') tensor(1.4759e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304235
tensor(-13.4263, device='cuda:0') tensor(5.8867e-07, device='cuda:0') tensor(1.4758e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304235
tensor(-13.4264, device='cuda:0') tensor(5.8858e-07, device='cuda:0') tensor(1.4756e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001650
Average total loss: 2.304234
tensor(-13.4265, device='cuda:0') tensor(5.8848e-07, device='cuda:0') tensor(1.4755e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-13.4266, device='cuda:0') tensor(5.8839e-07, device='cuda:0') tensor(1.4754e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-13.4267, device='cuda:0') tensor(5.8829e-07, device='cuda:0') tensor(1.4752e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-13.4268, device='cuda:0') tensor(5.8820e-07, device='cuda:0') tensor(1.4751e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-13.4269, device='cuda:0') tensor(5.8810e-07, device='cuda:0') tensor(1.4749e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-13.4270, device='cuda:0') tensor(5.8801e-07, device='cuda:0') tensor(1.4748e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304233
tensor(-13.4271, device='cuda:0') tensor(5.8791e-07, device='cuda:0') tensor(1.4747e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8782e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8780e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8779e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8778e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8776e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8775e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8774e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8772e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8771e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8770e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001648
Average total loss: 2.304233
tensor(-13.4272, device='cuda:0') tensor(5.8768e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
 Percentile value: -13.427169799804688
Non-zero model percentage: 40.96000671386719%, Non-zero mask percentage: 40.96000671386719%

--- Pruning Level [4/24]: ---
conv1.weight         | nonzeros =     327 /    1728             ( 18.92%) | total_pruned =    1401 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3433 /   36864             (  9.31%) | total_pruned =   33431 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6746 /   36864             ( 18.30%) | total_pruned =   30118 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6997 /   36864             ( 18.98%) | total_pruned =   29867 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7346 /   36864             ( 19.93%) | total_pruned =   29518 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20162 /   73728             ( 27.35%) | total_pruned =   53566 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36397 /  147456             ( 24.68%) | total_pruned =  111059 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2484 /    8192             ( 30.32%) | total_pruned =    5708 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13360 /  147456             (  9.06%) | total_pruned =  134096 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      69 /     128             ( 53.91%) | total_pruned =      59 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10332 /  147456             (  7.01%) | total_pruned =  137124 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   78017 /  294912             ( 26.45%) | total_pruned =  216895 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   90477 /  589824             ( 15.34%) | total_pruned =  499347 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5533 /   32768             ( 16.89%) | total_pruned =   27235 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21142 /  589824             (  3.58%) | total_pruned =  568682 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14994 /  589824             (  2.54%) | total_pruned =  574830 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  107982 / 1179648             (  9.15%) | total_pruned = 1071666 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     327 /     512             ( 63.87%) | total_pruned =     185 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  163968 / 2359296             (  6.95%) | total_pruned = 2195328 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     406 /     512             ( 79.30%) | total_pruned =     106 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   32161 /  131072             ( 24.54%) | total_pruned =   98911 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     208 /     512             ( 40.62%) | total_pruned =     304 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1972794 / 2359296             ( 83.62%) | total_pruned =  386502 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 4578822, pruned : 6599940, total: 11178762, Compression rate :       2.44x  ( 59.04% pruned)
Train Epoch: 61/100 Loss: 0.051602 Accuracy: 82.54 100.00 % Best test Accuracy: 82.92%
tensor(-13.4272, device='cuda:0') tensor(5.8767e-07, device='cuda:0') tensor(1.4745e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001643
Average total loss: 2.304228
tensor(-13.4343, device='cuda:0') tensor(5.7390e-07, device='cuda:0') tensor(1.4641e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001631
Average total loss: 2.304216
tensor(-13.4413, device='cuda:0') tensor(5.6066e-07, device='cuda:0') tensor(1.4538e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001620
Average total loss: 2.304205
tensor(-13.4483, device='cuda:0') tensor(5.4796e-07, device='cuda:0') tensor(1.4437e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001608
Average total loss: 2.304193
tensor(-13.4553, device='cuda:0') tensor(5.3577e-07, device='cuda:0') tensor(1.4336e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001597
Average total loss: 2.304182
tensor(-13.4622, device='cuda:0') tensor(5.2406e-07, device='cuda:0') tensor(1.4238e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001586
Average total loss: 2.304171
tensor(-13.4691, device='cuda:0') tensor(5.1281e-07, device='cuda:0') tensor(1.4140e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001575
Average total loss: 2.304160
tensor(-13.4759, device='cuda:0') tensor(5.0197e-07, device='cuda:0') tensor(1.4044e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001565
Average total loss: 2.304150
tensor(-13.4826, device='cuda:0') tensor(4.9154e-07, device='cuda:0') tensor(1.3950e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001554
Average total loss: 2.304139
tensor(-13.4894, device='cuda:0') tensor(4.8149e-07, device='cuda:0') tensor(1.3856e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001544
Average total loss: 2.304129
tensor(-13.4961, device='cuda:0') tensor(4.7179e-07, device='cuda:0') tensor(1.3764e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001534
Average total loss: 2.304119
tensor(-13.5027, device='cuda:0') tensor(4.6240e-07, device='cuda:0') tensor(1.3673e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001524
Average total loss: 2.304108
tensor(-13.5093, device='cuda:0') tensor(4.5334e-07, device='cuda:0') tensor(1.3583e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001518
Average total loss: 2.304103
tensor(-13.5099, device='cuda:0') tensor(4.5245e-07, device='cuda:0') tensor(1.3574e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-13.5106, device='cuda:0') tensor(4.5157e-07, device='cuda:0') tensor(1.3565e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001516
Average total loss: 2.304101
tensor(-13.5112, device='cuda:0') tensor(4.5069e-07, device='cuda:0') tensor(1.3556e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001515
Average total loss: 2.304100
tensor(-13.5119, device='cuda:0') tensor(4.4982e-07, device='cuda:0') tensor(1.3547e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001514
Average total loss: 2.304099
tensor(-13.5125, device='cuda:0') tensor(4.4896e-07, device='cuda:0') tensor(1.3539e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001513
Average total loss: 2.304098
tensor(-13.5132, device='cuda:0') tensor(4.4810e-07, device='cuda:0') tensor(1.3530e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001512
Average total loss: 2.304097
tensor(-13.5139, device='cuda:0') tensor(4.4725e-07, device='cuda:0') tensor(1.3521e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001511
Average total loss: 2.304096
tensor(-13.5145, device='cuda:0') tensor(4.4640e-07, device='cuda:0') tensor(1.3512e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001510
Average total loss: 2.304095
tensor(-13.5152, device='cuda:0') tensor(4.4556e-07, device='cuda:0') tensor(1.3503e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001509
Average total loss: 2.304094
tensor(-13.5158, device='cuda:0') tensor(4.4473e-07, device='cuda:0') tensor(1.3494e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001508
Average total loss: 2.304093
tensor(-13.5165, device='cuda:0') tensor(4.4389e-07, device='cuda:0') tensor(1.3486e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5165, device='cuda:0') tensor(4.4377e-07, device='cuda:0') tensor(1.3485e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5166, device='cuda:0') tensor(4.4362e-07, device='cuda:0') tensor(1.3484e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5166, device='cuda:0') tensor(4.4349e-07, device='cuda:0') tensor(1.3484e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5167, device='cuda:0') tensor(4.4335e-07, device='cuda:0') tensor(1.3483e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5167, device='cuda:0') tensor(4.4323e-07, device='cuda:0') tensor(1.3482e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5168, device='cuda:0') tensor(4.4309e-07, device='cuda:0') tensor(1.3482e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5168, device='cuda:0') tensor(4.4296e-07, device='cuda:0') tensor(1.3481e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5168, device='cuda:0') tensor(4.4282e-07, device='cuda:0') tensor(1.3480e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5169, device='cuda:0') tensor(4.4269e-07, device='cuda:0') tensor(1.3480e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5169, device='cuda:0') tensor(4.4255e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4242e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4241e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4241e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001507
Average total loss: 2.304092
tensor(-13.5170, device='cuda:0') tensor(4.4241e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
 Percentile value: -13.51699447631836
Non-zero model percentage: 32.76801300048828%, Non-zero mask percentage: 32.76801300048828%

--- Pruning Level [5/24]: ---
conv1.weight         | nonzeros =     327 /    1728             ( 18.92%) | total_pruned =    1401 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3433 /   36864             (  9.31%) | total_pruned =   33431 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6746 /   36864             ( 18.30%) | total_pruned =   30118 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6997 /   36864             ( 18.98%) | total_pruned =   29867 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7346 /   36864             ( 19.93%) | total_pruned =   29518 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20162 /   73728             ( 27.35%) | total_pruned =   53566 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36397 /  147456             ( 24.68%) | total_pruned =  111059 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2484 /    8192             ( 30.32%) | total_pruned =    5708 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13360 /  147456             (  9.06%) | total_pruned =  134096 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      69 /     128             ( 53.91%) | total_pruned =      59 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10332 /  147456             (  7.01%) | total_pruned =  137124 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   78017 /  294912             ( 26.45%) | total_pruned =  216895 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   90477 /  589824             ( 15.34%) | total_pruned =  499347 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5533 /   32768             ( 16.89%) | total_pruned =   27235 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21142 /  589824             (  3.58%) | total_pruned =  568682 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14994 /  589824             (  2.54%) | total_pruned =  574830 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  107982 / 1179648             (  9.15%) | total_pruned = 1071666 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     327 /     512             ( 63.87%) | total_pruned =     185 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  163968 / 2359296             (  6.95%) | total_pruned = 2195328 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     406 /     512             ( 79.30%) | total_pruned =     106 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   11011 /  131072             (  8.40%) | total_pruned =  120061 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     191 /     512             ( 37.30%) | total_pruned =     321 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1078711 / 2359296             ( 45.72%) | total_pruned = 1280585 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3663058, pruned : 7515704, total: 11178762, Compression rate :       3.05x  ( 67.23% pruned)
Train Epoch: 99/100 Loss: 0.034127 Accuracy: 81.94 100.00 % Best test Accuracy: 82.37%
tensor(-13.5170, device='cuda:0') tensor(4.4241e-07, device='cuda:0') tensor(1.3479e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001502
Average total loss: 2.304087
tensor(-13.5235, device='cuda:0') tensor(4.3390e-07, device='cuda:0') tensor(1.3391e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001492
Average total loss: 2.304077
tensor(-13.5299, device='cuda:0') tensor(4.2570e-07, device='cuda:0') tensor(1.3305e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001483
Average total loss: 2.304068
tensor(-13.5364, device='cuda:0') tensor(4.1776e-07, device='cuda:0') tensor(1.3220e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001473
Average total loss: 2.304058
tensor(-13.5427, device='cuda:0') tensor(4.1008e-07, device='cuda:0') tensor(1.3136e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001464
Average total loss: 2.304049
tensor(-13.5491, device='cuda:0') tensor(4.0264e-07, device='cuda:0') tensor(1.3053e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001455
Average total loss: 2.304040
tensor(-13.5554, device='cuda:0') tensor(3.9546e-07, device='cuda:0') tensor(1.2971e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001446
Average total loss: 2.304030
tensor(-13.5616, device='cuda:0') tensor(3.8847e-07, device='cuda:0') tensor(1.2890e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001437
Average total loss: 2.304021
tensor(-13.5679, device='cuda:0') tensor(3.8164e-07, device='cuda:0') tensor(1.2810e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001428
Average total loss: 2.304013
tensor(-13.5741, device='cuda:0') tensor(3.7503e-07, device='cuda:0') tensor(1.2731e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001419
Average total loss: 2.304004
tensor(-13.5802, device='cuda:0') tensor(3.6862e-07, device='cuda:0') tensor(1.2653e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001410
Average total loss: 2.303995
tensor(-13.5863, device='cuda:0') tensor(3.6243e-07, device='cuda:0') tensor(1.2576e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001402
Average total loss: 2.303987
tensor(-13.5924, device='cuda:0') tensor(3.5639e-07, device='cuda:0') tensor(1.2500e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001397
Average total loss: 2.303982
tensor(-13.5930, device='cuda:0') tensor(3.5584e-07, device='cuda:0') tensor(1.2492e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-13.5936, device='cuda:0') tensor(3.5530e-07, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001395
Average total loss: 2.303980
tensor(-13.5942, device='cuda:0') tensor(3.5475e-07, device='cuda:0') tensor(1.2477e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001394
Average total loss: 2.303979
tensor(-13.5948, device='cuda:0') tensor(3.5421e-07, device='cuda:0') tensor(1.2469e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001394
Average total loss: 2.303978
tensor(-13.5954, device='cuda:0') tensor(3.5368e-07, device='cuda:0') tensor(1.2462e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001393
Average total loss: 2.303978
tensor(-13.5960, device='cuda:0') tensor(3.5314e-07, device='cuda:0') tensor(1.2454e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001392
Average total loss: 2.303977
tensor(-13.5966, device='cuda:0') tensor(3.5261e-07, device='cuda:0') tensor(1.2447e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001391
Average total loss: 2.303976
tensor(-13.5972, device='cuda:0') tensor(3.5208e-07, device='cuda:0') tensor(1.2439e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001390
Average total loss: 2.303975
tensor(-13.5979, device='cuda:0') tensor(3.5155e-07, device='cuda:0') tensor(1.2432e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001389
Average total loss: 2.303974
tensor(-13.5985, device='cuda:0') tensor(3.5103e-07, device='cuda:0') tensor(1.2424e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5991, device='cuda:0') tensor(3.5051e-07, device='cuda:0') tensor(1.2416e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5991, device='cuda:0') tensor(3.5046e-07, device='cuda:0') tensor(1.2416e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5992, device='cuda:0') tensor(3.5040e-07, device='cuda:0') tensor(1.2415e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5992, device='cuda:0') tensor(3.5035e-07, device='cuda:0') tensor(1.2415e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5993, device='cuda:0') tensor(3.5029e-07, device='cuda:0') tensor(1.2414e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303973
tensor(-13.5993, device='cuda:0') tensor(3.5024e-07, device='cuda:0') tensor(1.2414e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303972
tensor(-13.5994, device='cuda:0') tensor(3.5018e-07, device='cuda:0') tensor(1.2413e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303972
tensor(-13.5994, device='cuda:0') tensor(3.5013e-07, device='cuda:0') tensor(1.2412e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001388
Average total loss: 2.303972
tensor(-13.5994, device='cuda:0') tensor(3.5007e-07, device='cuda:0') tensor(1.2412e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5995, device='cuda:0') tensor(3.5003e-07, device='cuda:0') tensor(1.2411e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5995, device='cuda:0') tensor(3.4997e-07, device='cuda:0') tensor(1.2411e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001387
Average total loss: 2.303972
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
 Percentile value: -13.599587440490723
Non-zero model percentage: 26.214414596557617%, Non-zero mask percentage: 26.214414596557617%

--- Pruning Level [6/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  675511 / 2359296             ( 28.63%) | total_pruned = 1683785 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 2930447, pruned : 8248315, total: 11178762, Compression rate :       3.81x  ( 73.79% pruned)
Train Epoch: 99/100 Loss: 0.119551 Accuracy: 79.19 99.46 % Best test Accuracy: 81.21%
tensor(-13.5996, device='cuda:0') tensor(3.4992e-07, device='cuda:0') tensor(1.2410e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001383
Average total loss: 2.303968
tensor(-13.6056, device='cuda:0') tensor(3.4413e-07, device='cuda:0') tensor(1.2336e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001375
Average total loss: 2.303960
tensor(-13.6115, device='cuda:0') tensor(3.3856e-07, device='cuda:0') tensor(1.2263e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001367
Average total loss: 2.303952
tensor(-13.6175, device='cuda:0') tensor(3.3316e-07, device='cuda:0') tensor(1.2190e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001359
Average total loss: 2.303944
tensor(-13.6233, device='cuda:0') tensor(3.2786e-07, device='cuda:0') tensor(1.2119e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001351
Average total loss: 2.303936
tensor(-13.6292, device='cuda:0') tensor(3.2270e-07, device='cuda:0') tensor(1.2048e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001343
Average total loss: 2.303928
tensor(-13.6350, device='cuda:0') tensor(3.1769e-07, device='cuda:0') tensor(1.1978e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001335
Average total loss: 2.303920
tensor(-13.6408, device='cuda:0') tensor(3.1285e-07, device='cuda:0') tensor(1.1909e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001328
Average total loss: 2.303912
tensor(-13.6466, device='cuda:0') tensor(3.0804e-07, device='cuda:0') tensor(1.1841e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001320
Average total loss: 2.303905
tensor(-13.6523, device='cuda:0') tensor(3.0339e-07, device='cuda:0') tensor(1.1773e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001312
Average total loss: 2.303897
tensor(-13.6580, device='cuda:0') tensor(2.9889e-07, device='cuda:0') tensor(1.1706e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001305
Average total loss: 2.303890
tensor(-13.6636, device='cuda:0') tensor(2.9443e-07, device='cuda:0') tensor(1.1640e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001298
Average total loss: 2.303883
tensor(-13.6693, device='cuda:0') tensor(2.9010e-07, device='cuda:0') tensor(1.1575e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001294
Average total loss: 2.303878
tensor(-13.6698, device='cuda:0') tensor(2.8969e-07, device='cuda:0') tensor(1.1568e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001293
Average total loss: 2.303878
tensor(-13.6704, device='cuda:0') tensor(2.8929e-07, device='cuda:0') tensor(1.1562e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001292
Average total loss: 2.303877
tensor(-13.6709, device='cuda:0') tensor(2.8889e-07, device='cuda:0') tensor(1.1555e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001291
Average total loss: 2.303876
tensor(-13.6715, device='cuda:0') tensor(2.8850e-07, device='cuda:0') tensor(1.1549e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001291
Average total loss: 2.303876
tensor(-13.6721, device='cuda:0') tensor(2.8810e-07, device='cuda:0') tensor(1.1542e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001290
Average total loss: 2.303875
tensor(-13.6726, device='cuda:0') tensor(2.8771e-07, device='cuda:0') tensor(1.1536e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001289
Average total loss: 2.303874
tensor(-13.6732, device='cuda:0') tensor(2.8732e-07, device='cuda:0') tensor(1.1529e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001289
Average total loss: 2.303873
tensor(-13.6737, device='cuda:0') tensor(2.8693e-07, device='cuda:0') tensor(1.1523e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001288
Average total loss: 2.303873
tensor(-13.6743, device='cuda:0') tensor(2.8656e-07, device='cuda:0') tensor(1.1517e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001287
Average total loss: 2.303872
tensor(-13.6749, device='cuda:0') tensor(2.8620e-07, device='cuda:0') tensor(1.1510e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6754, device='cuda:0') tensor(2.8584e-07, device='cuda:0') tensor(1.1504e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6755, device='cuda:0') tensor(2.8579e-07, device='cuda:0') tensor(1.1503e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6755, device='cuda:0') tensor(2.8576e-07, device='cuda:0') tensor(1.1503e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6756, device='cuda:0') tensor(2.8571e-07, device='cuda:0') tensor(1.1502e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6756, device='cuda:0') tensor(2.8568e-07, device='cuda:0') tensor(1.1501e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303871
tensor(-13.6757, device='cuda:0') tensor(2.8564e-07, device='cuda:0') tensor(1.1501e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303870
tensor(-13.6757, device='cuda:0') tensor(2.8561e-07, device='cuda:0') tensor(1.1500e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303870
tensor(-13.6758, device='cuda:0') tensor(2.8556e-07, device='cuda:0') tensor(1.1500e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001286
Average total loss: 2.303870
tensor(-13.6758, device='cuda:0') tensor(2.8553e-07, device='cuda:0') tensor(1.1499e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6758, device='cuda:0') tensor(2.8548e-07, device='cuda:0') tensor(1.1499e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8545e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001285
Average total loss: 2.303870
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
 Percentile value: -13.675947189331055
Non-zero model percentage: 20.971534729003906%, Non-zero mask percentage: 20.971534729003906%

--- Pruning Level [7/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   89422 / 2359296             (  3.79%) | total_pruned = 2269874 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     254 /     512             ( 49.61%) | total_pruned =     258 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1972670 / 2359296             ( 83.61%) | total_pruned =  386626 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 2344358, pruned : 8834404, total: 11178762, Compression rate :       4.77x  ( 79.03% pruned)
Train Epoch: 99/100 Loss: 0.261580 Accuracy: 77.76 93.88 % Best test Accuracy: 79.81%
tensor(-13.6759, device='cuda:0') tensor(2.8541e-07, device='cuda:0') tensor(1.1498e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001282
Average total loss: 2.303867
tensor(-13.6815, device='cuda:0') tensor(2.8132e-07, device='cuda:0') tensor(1.1434e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001275
Average total loss: 2.303860
tensor(-13.6870, device='cuda:0') tensor(2.7727e-07, device='cuda:0') tensor(1.1371e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001268
Average total loss: 2.303853
tensor(-13.6925, device='cuda:0') tensor(2.7338e-07, device='cuda:0') tensor(1.1309e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001261
Average total loss: 2.303846
tensor(-13.6980, device='cuda:0') tensor(2.6952e-07, device='cuda:0') tensor(1.1247e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001254
Average total loss: 2.303839
tensor(-13.7034, device='cuda:0') tensor(2.6575e-07, device='cuda:0') tensor(1.1186e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001247
Average total loss: 2.303832
tensor(-13.7089, device='cuda:0') tensor(2.6213e-07, device='cuda:0') tensor(1.1125e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001240
Average total loss: 2.303825
tensor(-13.7142, device='cuda:0') tensor(2.5850e-07, device='cuda:0') tensor(1.1066e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001234
Average total loss: 2.303819
tensor(-13.7196, device='cuda:0') tensor(2.5499e-07, device='cuda:0') tensor(1.1007e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001227
Average total loss: 2.303812
tensor(-13.7249, device='cuda:0') tensor(2.5158e-07, device='cuda:0') tensor(1.0948e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001221
Average total loss: 2.303806
tensor(-13.7302, device='cuda:0') tensor(2.4819e-07, device='cuda:0') tensor(1.0890e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001214
Average total loss: 2.303799
tensor(-13.7355, device='cuda:0') tensor(2.4495e-07, device='cuda:0') tensor(1.0833e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001208
Average total loss: 2.303793
tensor(-13.7407, device='cuda:0') tensor(2.4168e-07, device='cuda:0') tensor(1.0777e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001204
Average total loss: 2.303789
tensor(-13.7412, device='cuda:0') tensor(2.4133e-07, device='cuda:0') tensor(1.0771e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001204
Average total loss: 2.303789
tensor(-13.7417, device='cuda:0') tensor(2.4097e-07, device='cuda:0') tensor(1.0766e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001203
Average total loss: 2.303788
tensor(-13.7423, device='cuda:0') tensor(2.4062e-07, device='cuda:0') tensor(1.0760e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001203
Average total loss: 2.303787
tensor(-13.7428, device='cuda:0') tensor(2.4026e-07, device='cuda:0') tensor(1.0754e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001202
Average total loss: 2.303787
tensor(-13.7433, device='cuda:0') tensor(2.3992e-07, device='cuda:0') tensor(1.0749e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001201
Average total loss: 2.303786
tensor(-13.7438, device='cuda:0') tensor(2.3957e-07, device='cuda:0') tensor(1.0743e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001201
Average total loss: 2.303786
tensor(-13.7443, device='cuda:0') tensor(2.3924e-07, device='cuda:0') tensor(1.0738e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001200
Average total loss: 2.303785
tensor(-13.7448, device='cuda:0') tensor(2.3890e-07, device='cuda:0') tensor(1.0732e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001199
Average total loss: 2.303784
tensor(-13.7453, device='cuda:0') tensor(2.3858e-07, device='cuda:0') tensor(1.0727e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001199
Average total loss: 2.303784
tensor(-13.7459, device='cuda:0') tensor(2.3824e-07, device='cuda:0') tensor(1.0721e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-13.7464, device='cuda:0') tensor(2.3793e-07, device='cuda:0') tensor(1.0716e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-13.7464, device='cuda:0') tensor(2.3790e-07, device='cuda:0') tensor(1.0715e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-13.7465, device='cuda:0') tensor(2.3789e-07, device='cuda:0') tensor(1.0715e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-13.7465, device='cuda:0') tensor(2.3786e-07, device='cuda:0') tensor(1.0714e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303783
tensor(-13.7466, device='cuda:0') tensor(2.3785e-07, device='cuda:0') tensor(1.0714e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303782
tensor(-13.7466, device='cuda:0') tensor(2.3782e-07, device='cuda:0') tensor(1.0713e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303782
tensor(-13.7466, device='cuda:0') tensor(2.3781e-07, device='cuda:0') tensor(1.0713e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001198
Average total loss: 2.303782
tensor(-13.7467, device='cuda:0') tensor(2.3778e-07, device='cuda:0') tensor(1.0712e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7467, device='cuda:0') tensor(2.3776e-07, device='cuda:0') tensor(1.0712e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7468, device='cuda:0') tensor(2.3774e-07, device='cuda:0') tensor(1.0711e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7468, device='cuda:0') tensor(2.3772e-07, device='cuda:0') tensor(1.0711e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001197
Average total loss: 2.303782
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
 Percentile value: -13.746889114379883
Non-zero model percentage: 16.777233123779297%, Non-zero mask percentage: 16.777233123779297%

--- Pruning Level [8/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1516433 / 2359296             ( 64.27%) | total_pruned =  842863 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1875487, pruned : 9303275, total: 11178762, Compression rate :       5.96x  ( 83.22% pruned)
Train Epoch: 99/100 Loss: 0.307324 Accuracy: 76.65 93.83 % Best test Accuracy: 78.50%
tensor(-13.7469, device='cuda:0') tensor(2.3770e-07, device='cuda:0') tensor(1.0710e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001194
Average total loss: 2.303779
tensor(-13.7521, device='cuda:0') tensor(2.3466e-07, device='cuda:0') tensor(1.0655e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001188
Average total loss: 2.303773
tensor(-13.7572, device='cuda:0') tensor(2.3161e-07, device='cuda:0') tensor(1.0600e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001182
Average total loss: 2.303767
tensor(-13.7624, device='cuda:0') tensor(2.2871e-07, device='cuda:0') tensor(1.0546e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001176
Average total loss: 2.303761
tensor(-13.7675, device='cuda:0') tensor(2.2575e-07, device='cuda:0') tensor(1.0492e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001170
Average total loss: 2.303755
tensor(-13.7725, device='cuda:0') tensor(2.2297e-07, device='cuda:0') tensor(1.0439e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001164
Average total loss: 2.303749
tensor(-13.7776, device='cuda:0') tensor(2.2014e-07, device='cuda:0') tensor(1.0386e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001158
Average total loss: 2.303743
tensor(-13.7826, device='cuda:0') tensor(2.1743e-07, device='cuda:0') tensor(1.0334e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001152
Average total loss: 2.303737
tensor(-13.7876, device='cuda:0') tensor(2.1474e-07, device='cuda:0') tensor(1.0283e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001147
Average total loss: 2.303732
tensor(-13.7926, device='cuda:0') tensor(2.1213e-07, device='cuda:0') tensor(1.0232e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001141
Average total loss: 2.303726
tensor(-13.7976, device='cuda:0') tensor(2.0956e-07, device='cuda:0') tensor(1.0181e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001135
Average total loss: 2.303720
tensor(-13.8025, device='cuda:0') tensor(2.0702e-07, device='cuda:0') tensor(1.0131e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001130
Average total loss: 2.303715
tensor(-13.8074, device='cuda:0') tensor(2.0456e-07, device='cuda:0') tensor(1.0081e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001127
Average total loss: 2.303712
tensor(-13.8079, device='cuda:0') tensor(2.0414e-07, device='cuda:0') tensor(1.0077e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001126
Average total loss: 2.303711
tensor(-13.8083, device='cuda:0') tensor(2.0373e-07, device='cuda:0') tensor(1.0072e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001126
Average total loss: 2.303711
tensor(-13.8088, device='cuda:0') tensor(2.0332e-07, device='cuda:0') tensor(1.0067e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-13.8093, device='cuda:0') tensor(2.0292e-07, device='cuda:0') tensor(1.0063e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-13.8097, device='cuda:0') tensor(2.0251e-07, device='cuda:0') tensor(1.0058e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001124
Average total loss: 2.303709
tensor(-13.8102, device='cuda:0') tensor(2.0210e-07, device='cuda:0') tensor(1.0053e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001124
Average total loss: 2.303708
tensor(-13.8107, device='cuda:0') tensor(2.0171e-07, device='cuda:0') tensor(1.0049e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001123
Average total loss: 2.303708
tensor(-13.8111, device='cuda:0') tensor(2.0133e-07, device='cuda:0') tensor(1.0044e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001123
Average total loss: 2.303707
tensor(-13.8116, device='cuda:0') tensor(2.0094e-07, device='cuda:0') tensor(1.0039e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001122
Average total loss: 2.303707
tensor(-13.8121, device='cuda:0') tensor(2.0057e-07, device='cuda:0') tensor(1.0034e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8125, device='cuda:0') tensor(2.0019e-07, device='cuda:0') tensor(1.0030e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8126, device='cuda:0') tensor(2.0017e-07, device='cuda:0') tensor(1.0029e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8126, device='cuda:0') tensor(2.0017e-07, device='cuda:0') tensor(1.0029e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8127, device='cuda:0') tensor(2.0015e-07, device='cuda:0') tensor(1.0028e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8127, device='cuda:0') tensor(2.0014e-07, device='cuda:0') tensor(1.0028e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8128, device='cuda:0') tensor(2.0013e-07, device='cuda:0') tensor(1.0027e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8128, device='cuda:0') tensor(2.0012e-07, device='cuda:0') tensor(1.0027e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8129, device='cuda:0') tensor(2.0010e-07, device='cuda:0') tensor(1.0026e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8129, device='cuda:0') tensor(2.0009e-07, device='cuda:0') tensor(1.0026e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8130, device='cuda:0') tensor(2.0008e-07, device='cuda:0') tensor(1.0026e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303706
tensor(-13.8130, device='cuda:0') tensor(2.0007e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001121
Average total loss: 2.303705
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
 Percentile value: -13.813055038452148
Non-zero model percentage: 13.42179012298584%, Non-zero mask percentage: 13.42179012298584%

--- Pruning Level [9/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1141336 / 2359296             ( 48.38%) | total_pruned = 1217960 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1500390, pruned : 9678372, total: 11178762, Compression rate :       7.45x  ( 86.58% pruned)
Train Epoch: 56/100 Loss: 0.502893 Accuracy: 77.28 87.62 % Best test Accuracy: 77.36%
tensor(-13.8130, device='cuda:0') tensor(2.0005e-07, device='cuda:0') tensor(1.0025e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001118
Average total loss: 2.303703
tensor(-13.8179, device='cuda:0') tensor(1.9773e-07, device='cuda:0') tensor(9.9760e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001113
Average total loss: 2.303697
tensor(-13.8227, device='cuda:0') tensor(1.9538e-07, device='cuda:0') tensor(9.9279e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001107
Average total loss: 2.303692
tensor(-13.8276, device='cuda:0') tensor(1.9313e-07, device='cuda:0') tensor(9.8803e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001102
Average total loss: 2.303687
tensor(-13.8323, device='cuda:0') tensor(1.9087e-07, device='cuda:0') tensor(9.8331e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001097
Average total loss: 2.303682
tensor(-13.8371, device='cuda:0') tensor(1.8870e-07, device='cuda:0') tensor(9.7864e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001091
Average total loss: 2.303676
tensor(-13.8418, device='cuda:0') tensor(1.8652e-07, device='cuda:0') tensor(9.7401e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001086
Average total loss: 2.303671
tensor(-13.8466, device='cuda:0') tensor(1.8442e-07, device='cuda:0') tensor(9.6942e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001081
Average total loss: 2.303666
tensor(-13.8513, device='cuda:0') tensor(1.8234e-07, device='cuda:0') tensor(9.6488e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001076
Average total loss: 2.303661
tensor(-13.8559, device='cuda:0') tensor(1.8029e-07, device='cuda:0') tensor(9.6038e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001071
Average total loss: 2.303656
tensor(-13.8606, device='cuda:0') tensor(1.7830e-07, device='cuda:0') tensor(9.5592e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001066
Average total loss: 2.303651
tensor(-13.8652, device='cuda:0') tensor(1.7629e-07, device='cuda:0') tensor(9.5151e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001061
Average total loss: 2.303646
tensor(-13.8698, device='cuda:0') tensor(1.7441e-07, device='cuda:0') tensor(9.4712e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001059
Average total loss: 2.303643
tensor(-13.8703, device='cuda:0') tensor(1.7428e-07, device='cuda:0') tensor(9.4668e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303643
tensor(-13.8708, device='cuda:0') tensor(1.7415e-07, device='cuda:0') tensor(9.4623e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303642
tensor(-13.8712, device='cuda:0') tensor(1.7402e-07, device='cuda:0') tensor(9.4579e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001057
Average total loss: 2.303642
tensor(-13.8717, device='cuda:0') tensor(1.7389e-07, device='cuda:0') tensor(9.4535e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001057
Average total loss: 2.303641
tensor(-13.8722, device='cuda:0') tensor(1.7376e-07, device='cuda:0') tensor(9.4491e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001056
Average total loss: 2.303641
tensor(-13.8726, device='cuda:0') tensor(1.7364e-07, device='cuda:0') tensor(9.4447e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001056
Average total loss: 2.303640
tensor(-13.8731, device='cuda:0') tensor(1.7351e-07, device='cuda:0') tensor(9.4403e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001055
Average total loss: 2.303640
tensor(-13.8736, device='cuda:0') tensor(1.7339e-07, device='cuda:0') tensor(9.4359e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001055
Average total loss: 2.303639
tensor(-13.8740, device='cuda:0') tensor(1.7327e-07, device='cuda:0') tensor(9.4314e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001054
Average total loss: 2.303639
tensor(-13.8745, device='cuda:0') tensor(1.7314e-07, device='cuda:0') tensor(9.4270e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001054
Average total loss: 2.303638
tensor(-13.8750, device='cuda:0') tensor(1.7302e-07, device='cuda:0') tensor(9.4226e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8750, device='cuda:0') tensor(1.7300e-07, device='cuda:0') tensor(9.4222e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8751, device='cuda:0') tensor(1.7300e-07, device='cuda:0') tensor(9.4218e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8751, device='cuda:0') tensor(1.7298e-07, device='cuda:0') tensor(9.4213e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8752, device='cuda:0') tensor(1.7297e-07, device='cuda:0') tensor(9.4209e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8752, device='cuda:0') tensor(1.7296e-07, device='cuda:0') tensor(9.4204e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8753, device='cuda:0') tensor(1.7295e-07, device='cuda:0') tensor(9.4200e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8753, device='cuda:0') tensor(1.7293e-07, device='cuda:0') tensor(9.4196e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8754, device='cuda:0') tensor(1.7293e-07, device='cuda:0') tensor(9.4191e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8754, device='cuda:0') tensor(1.7291e-07, device='cuda:0') tensor(9.4187e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8754, device='cuda:0') tensor(1.7291e-07, device='cuda:0') tensor(9.4182e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001053
Average total loss: 2.303638
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
 Percentile value: -13.875495910644531
Non-zero model percentage: 10.737431526184082%, Non-zero mask percentage: 10.737431526184082%

--- Pruning Level [10/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  841258 / 2359296             ( 35.66%) | total_pruned = 1518038 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1200312, pruned : 9978450, total: 11178762, Compression rate :       9.31x  ( 89.26% pruned)
Train Epoch: 99/100 Loss: 0.277404 Accuracy: 76.38 90.17 % Best test Accuracy: 78.63%
tensor(-13.8755, device='cuda:0') tensor(1.7289e-07, device='cuda:0') tensor(9.4178e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001050
Average total loss: 2.303635
tensor(-13.8801, device='cuda:0') tensor(1.7109e-07, device='cuda:0') tensor(9.3748e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001046
Average total loss: 2.303631
tensor(-13.8846, device='cuda:0') tensor(1.6917e-07, device='cuda:0') tensor(9.3324e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001041
Average total loss: 2.303626
tensor(-13.8891, device='cuda:0') tensor(1.6742e-07, device='cuda:0') tensor(9.2902e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001036
Average total loss: 2.303621
tensor(-13.8936, device='cuda:0') tensor(1.6558e-07, device='cuda:0') tensor(9.2485e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001032
Average total loss: 2.303616
tensor(-13.8981, device='cuda:0') tensor(1.6384e-07, device='cuda:0') tensor(9.2071e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001027
Average total loss: 2.303612
tensor(-13.9026, device='cuda:0') tensor(1.6212e-07, device='cuda:0') tensor(9.1661e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001022
Average total loss: 2.303607
tensor(-13.9070, device='cuda:0') tensor(1.6040e-07, device='cuda:0') tensor(9.1255e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001018
Average total loss: 2.303603
tensor(-13.9115, device='cuda:0') tensor(1.5880e-07, device='cuda:0') tensor(9.0851e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001013
Average total loss: 2.303598
tensor(-13.9159, device='cuda:0') tensor(1.5709e-07, device='cuda:0') tensor(9.0453e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001009
Average total loss: 2.303594
tensor(-13.9202, device='cuda:0') tensor(1.5551e-07, device='cuda:0') tensor(9.0056e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001005
Average total loss: 2.303589
tensor(-13.9246, device='cuda:0') tensor(1.5389e-07, device='cuda:0') tensor(8.9664e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001000
Average total loss: 2.303585
tensor(-13.9290, device='cuda:0') tensor(1.5232e-07, device='cuda:0') tensor(8.9275e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000998
Average total loss: 2.303583
tensor(-13.9294, device='cuda:0') tensor(1.5210e-07, device='cuda:0') tensor(8.9238e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000997
Average total loss: 2.303582
tensor(-13.9298, device='cuda:0') tensor(1.5189e-07, device='cuda:0') tensor(8.9200e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000997
Average total loss: 2.303582
tensor(-13.9302, device='cuda:0') tensor(1.5167e-07, device='cuda:0') tensor(8.9163e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000997
Average total loss: 2.303581
tensor(-13.9306, device='cuda:0') tensor(1.5147e-07, device='cuda:0') tensor(8.9125e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000996
Average total loss: 2.303581
tensor(-13.9311, device='cuda:0') tensor(1.5126e-07, device='cuda:0') tensor(8.9088e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000996
Average total loss: 2.303581
tensor(-13.9315, device='cuda:0') tensor(1.5106e-07, device='cuda:0') tensor(8.9050e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000995
Average total loss: 2.303580
tensor(-13.9319, device='cuda:0') tensor(1.5086e-07, device='cuda:0') tensor(8.9013e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000995
Average total loss: 2.303580
tensor(-13.9323, device='cuda:0') tensor(1.5067e-07, device='cuda:0') tensor(8.8976e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000994
Average total loss: 2.303579
tensor(-13.9327, device='cuda:0') tensor(1.5047e-07, device='cuda:0') tensor(8.8938e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000994
Average total loss: 2.303579
tensor(-13.9332, device='cuda:0') tensor(1.5028e-07, device='cuda:0') tensor(8.8901e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000994
Average total loss: 2.303578
tensor(-13.9336, device='cuda:0') tensor(1.5008e-07, device='cuda:0') tensor(8.8863e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9336, device='cuda:0') tensor(1.5008e-07, device='cuda:0') tensor(8.8859e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9337, device='cuda:0') tensor(1.5006e-07, device='cuda:0') tensor(8.8855e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9337, device='cuda:0') tensor(1.5006e-07, device='cuda:0') tensor(8.8851e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9338, device='cuda:0') tensor(1.5004e-07, device='cuda:0') tensor(8.8847e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9338, device='cuda:0') tensor(1.5004e-07, device='cuda:0') tensor(8.8843e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9339, device='cuda:0') tensor(1.5002e-07, device='cuda:0') tensor(8.8838e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9339, device='cuda:0') tensor(1.5002e-07, device='cuda:0') tensor(8.8834e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9340, device='cuda:0') tensor(1.5000e-07, device='cuda:0') tensor(8.8830e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9340, device='cuda:0') tensor(1.5000e-07, device='cuda:0') tensor(8.8826e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9340, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8822e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000993
Average total loss: 2.303578
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
 Percentile value: -13.934096336364746
Non-zero model percentage: 8.589949607849121%, Non-zero mask percentage: 8.589949607849121%

--- Pruning Level [11/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  601196 / 2359296             ( 25.48%) | total_pruned = 1758100 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 960250, pruned : 10218512, total: 11178762, Compression rate :      11.64x  ( 91.41% pruned)
Train Epoch: 99/100 Loss: 0.587719 Accuracy: 76.55 89.57 % Best test Accuracy: 78.16%
tensor(-13.9341, device='cuda:0') tensor(1.4998e-07, device='cuda:0') tensor(8.8818e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000991
Average total loss: 2.303576
tensor(-13.9384, device='cuda:0') tensor(1.4843e-07, device='cuda:0') tensor(8.8437e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000987
Average total loss: 2.303571
tensor(-13.9427, device='cuda:0') tensor(1.4700e-07, device='cuda:0') tensor(8.8057e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000982
Average total loss: 2.303567
tensor(-13.9470, device='cuda:0') tensor(1.4549e-07, device='cuda:0') tensor(8.7682e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000978
Average total loss: 2.303563
tensor(-13.9512, device='cuda:0') tensor(1.4405e-07, device='cuda:0') tensor(8.7310e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000974
Average total loss: 2.303559
tensor(-13.9555, device='cuda:0') tensor(1.4268e-07, device='cuda:0') tensor(8.6940e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000970
Average total loss: 2.303555
tensor(-13.9597, device='cuda:0') tensor(1.4122e-07, device='cuda:0') tensor(8.6576e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000966
Average total loss: 2.303551
tensor(-13.9639, device='cuda:0') tensor(1.3988e-07, device='cuda:0') tensor(8.6212e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000962
Average total loss: 2.303547
tensor(-13.9680, device='cuda:0') tensor(1.3852e-07, device='cuda:0') tensor(8.5852e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000958
Average total loss: 2.303543
tensor(-13.9722, device='cuda:0') tensor(1.3716e-07, device='cuda:0') tensor(8.5496e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000954
Average total loss: 2.303539
tensor(-13.9764, device='cuda:0') tensor(1.3590e-07, device='cuda:0') tensor(8.5141e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000950
Average total loss: 2.303535
tensor(-13.9805, device='cuda:0') tensor(1.3456e-07, device='cuda:0') tensor(8.4791e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000946
Average total loss: 2.303531
tensor(-13.9846, device='cuda:0') tensor(1.3328e-07, device='cuda:0') tensor(8.4443e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000944
Average total loss: 2.303529
tensor(-13.9850, device='cuda:0') tensor(1.3320e-07, device='cuda:0') tensor(8.4408e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000943
Average total loss: 2.303528
tensor(-13.9854, device='cuda:0') tensor(1.3313e-07, device='cuda:0') tensor(8.4372e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000943
Average total loss: 2.303528
tensor(-13.9859, device='cuda:0') tensor(1.3305e-07, device='cuda:0') tensor(8.4337e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000943
Average total loss: 2.303527
tensor(-13.9863, device='cuda:0') tensor(1.3298e-07, device='cuda:0') tensor(8.4301e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000942
Average total loss: 2.303527
tensor(-13.9867, device='cuda:0') tensor(1.3291e-07, device='cuda:0') tensor(8.4266e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000942
Average total loss: 2.303527
tensor(-13.9871, device='cuda:0') tensor(1.3284e-07, device='cuda:0') tensor(8.4230e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000941
Average total loss: 2.303526
tensor(-13.9875, device='cuda:0') tensor(1.3276e-07, device='cuda:0') tensor(8.4195e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000941
Average total loss: 2.303526
tensor(-13.9880, device='cuda:0') tensor(1.3270e-07, device='cuda:0') tensor(8.4159e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000941
Average total loss: 2.303526
tensor(-13.9884, device='cuda:0') tensor(1.3262e-07, device='cuda:0') tensor(8.4124e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303525
tensor(-13.9888, device='cuda:0') tensor(1.3255e-07, device='cuda:0') tensor(8.4089e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303525
tensor(-13.9892, device='cuda:0') tensor(1.3248e-07, device='cuda:0') tensor(8.4053e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303524
tensor(-13.9893, device='cuda:0') tensor(1.3248e-07, device='cuda:0') tensor(8.4049e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303524
tensor(-13.9893, device='cuda:0') tensor(1.3246e-07, device='cuda:0') tensor(8.4045e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000940
Average total loss: 2.303524
tensor(-13.9894, device='cuda:0') tensor(1.3247e-07, device='cuda:0') tensor(8.4041e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9894, device='cuda:0') tensor(1.3245e-07, device='cuda:0') tensor(8.4038e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9895, device='cuda:0') tensor(1.3246e-07, device='cuda:0') tensor(8.4034e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9895, device='cuda:0') tensor(1.3244e-07, device='cuda:0') tensor(8.4030e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9896, device='cuda:0') tensor(1.3244e-07, device='cuda:0') tensor(8.4026e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9896, device='cuda:0') tensor(1.3243e-07, device='cuda:0') tensor(8.4022e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3243e-07, device='cuda:0') tensor(8.4018e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4014e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303524
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
 Percentile value: -13.989745140075684
Non-zero model percentage: 6.871950626373291%, Non-zero mask percentage: 6.871959686279297%

--- Pruning Level [12/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  409146 / 2359296             ( 17.34%) | total_pruned = 1950150 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     506 /     512             ( 98.83%) | total_pruned =       6 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 768199, pruned : 10410563, total: 11178762, Compression rate :      14.55x  ( 93.13% pruned)
Train Epoch: 99/100 Loss: 0.404683 Accuracy: 76.45 88.17 % Best test Accuracy: 77.70%
tensor(-13.9897, device='cuda:0') tensor(1.3242e-07, device='cuda:0') tensor(8.4010e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000937
Average total loss: 2.303522
tensor(-13.9938, device='cuda:0') tensor(1.3112e-07, device='cuda:0') tensor(8.3669e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000933
Average total loss: 2.303518
tensor(-13.9979, device='cuda:0') tensor(1.2994e-07, device='cuda:0') tensor(8.3330e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000930
Average total loss: 2.303515
tensor(-14.0019, device='cuda:0') tensor(1.2874e-07, device='cuda:0') tensor(8.2993e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000926
Average total loss: 2.303511
tensor(-14.0059, device='cuda:0') tensor(1.2750e-07, device='cuda:0') tensor(8.2660e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000922
Average total loss: 2.303507
tensor(-14.0100, device='cuda:0') tensor(1.2637e-07, device='cuda:0') tensor(8.2329e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000919
Average total loss: 2.303503
tensor(-14.0140, device='cuda:0') tensor(1.2522e-07, device='cuda:0') tensor(8.2000e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000915
Average total loss: 2.303500
tensor(-14.0179, device='cuda:0') tensor(1.2403e-07, device='cuda:0') tensor(8.1675e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000911
Average total loss: 2.303496
tensor(-14.0219, device='cuda:0') tensor(1.2296e-07, device='cuda:0') tensor(8.1351e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000908
Average total loss: 2.303493
tensor(-14.0258, device='cuda:0') tensor(1.2183e-07, device='cuda:0') tensor(8.1031e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000904
Average total loss: 2.303489
tensor(-14.0298, device='cuda:0') tensor(1.2070e-07, device='cuda:0') tensor(8.0713e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000901
Average total loss: 2.303485
tensor(-14.0337, device='cuda:0') tensor(1.1966e-07, device='cuda:0') tensor(8.0397e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000897
Average total loss: 2.303482
tensor(-14.0376, device='cuda:0') tensor(1.1860e-07, device='cuda:0') tensor(8.0084e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000895
Average total loss: 2.303480
tensor(-14.0380, device='cuda:0') tensor(1.1843e-07, device='cuda:0') tensor(8.0054e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000895
Average total loss: 2.303480
tensor(-14.0384, device='cuda:0') tensor(1.1825e-07, device='cuda:0') tensor(8.0024e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-14.0387, device='cuda:0') tensor(1.1808e-07, device='cuda:0') tensor(7.9994e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-14.0391, device='cuda:0') tensor(1.1791e-07, device='cuda:0') tensor(7.9964e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-14.0395, device='cuda:0') tensor(1.1774e-07, device='cuda:0') tensor(7.9934e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303478
tensor(-14.0399, device='cuda:0') tensor(1.1757e-07, device='cuda:0') tensor(7.9904e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303478
tensor(-14.0402, device='cuda:0') tensor(1.1741e-07, device='cuda:0') tensor(7.9875e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000893
Average total loss: 2.303478
tensor(-14.0406, device='cuda:0') tensor(1.1725e-07, device='cuda:0') tensor(7.9845e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303477
tensor(-14.0410, device='cuda:0') tensor(1.1709e-07, device='cuda:0') tensor(7.9815e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303477
tensor(-14.0413, device='cuda:0') tensor(1.1693e-07, device='cuda:0') tensor(7.9785e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303477
tensor(-14.0417, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9755e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303476
tensor(-14.0418, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9752e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000892
Average total loss: 2.303476
tensor(-14.0418, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9748e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0419, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9744e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0419, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9740e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0420, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9737e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0420, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9733e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0420, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9729e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0421, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9725e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0421, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9722e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9718e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000891
Average total loss: 2.303476
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
 Percentile value: -14.04223346710205
Non-zero model percentage: 5.497531414031982%, Non-zero mask percentage: 5.497567176818848%

--- Pruning Level [13/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  255506 / 2359296             ( 10.83%) | total_pruned = 2103790 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     503 /     512             ( 98.24%) | total_pruned =       9 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 614556, pruned : 10564206, total: 11178762, Compression rate :      18.19x  ( 94.50% pruned)
Train Epoch: 99/100 Loss: 0.414289 Accuracy: 76.39 88.28 % Best test Accuracy: 76.90%
tensor(-14.0422, device='cuda:0') tensor(1.1678e-07, device='cuda:0') tensor(7.9714e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-14.0461, device='cuda:0') tensor(1.1580e-07, device='cuda:0') tensor(7.9406e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000886
Average total loss: 2.303471
tensor(-14.0500, device='cuda:0') tensor(1.1478e-07, device='cuda:0') tensor(7.9100e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000883
Average total loss: 2.303467
tensor(-14.0538, device='cuda:0') tensor(1.1374e-07, device='cuda:0') tensor(7.8797e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000879
Average total loss: 2.303464
tensor(-14.0576, device='cuda:0') tensor(1.1278e-07, device='cuda:0') tensor(7.8496e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000876
Average total loss: 2.303461
tensor(-14.0615, device='cuda:0') tensor(1.1183e-07, device='cuda:0') tensor(7.8197e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000873
Average total loss: 2.303457
tensor(-14.0652, device='cuda:0') tensor(1.1082e-07, device='cuda:0') tensor(7.7902e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000869
Average total loss: 2.303454
tensor(-14.0690, device='cuda:0') tensor(1.0988e-07, device='cuda:0') tensor(7.7607e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000866
Average total loss: 2.303451
tensor(-14.0728, device='cuda:0') tensor(1.0902e-07, device='cuda:0') tensor(7.7314e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000863
Average total loss: 2.303448
tensor(-14.0765, device='cuda:0') tensor(1.0801e-07, device='cuda:0') tensor(7.7025e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000859
Average total loss: 2.303444
tensor(-14.0803, device='cuda:0') tensor(1.0710e-07, device='cuda:0') tensor(7.6738e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000856
Average total loss: 2.303441
tensor(-14.0840, device='cuda:0') tensor(1.0625e-07, device='cuda:0') tensor(7.6452e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000853
Average total loss: 2.303438
tensor(-14.0877, device='cuda:0') tensor(1.0533e-07, device='cuda:0') tensor(7.6169e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303436
tensor(-14.0881, device='cuda:0') tensor(1.0528e-07, device='cuda:0') tensor(7.6140e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303436
tensor(-14.0885, device='cuda:0') tensor(1.0522e-07, device='cuda:0') tensor(7.6112e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000851
Average total loss: 2.303436
tensor(-14.0888, device='cuda:0') tensor(1.0517e-07, device='cuda:0') tensor(7.6083e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303435
tensor(-14.0892, device='cuda:0') tensor(1.0511e-07, device='cuda:0') tensor(7.6055e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303435
tensor(-14.0896, device='cuda:0') tensor(1.0506e-07, device='cuda:0') tensor(7.6027e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000850
Average total loss: 2.303435
tensor(-14.0900, device='cuda:0') tensor(1.0501e-07, device='cuda:0') tensor(7.5998e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-14.0903, device='cuda:0') tensor(1.0496e-07, device='cuda:0') tensor(7.5970e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-14.0907, device='cuda:0') tensor(1.0492e-07, device='cuda:0') tensor(7.5941e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000849
Average total loss: 2.303434
tensor(-14.0911, device='cuda:0') tensor(1.0487e-07, device='cuda:0') tensor(7.5913e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0915, device='cuda:0') tensor(1.0482e-07, device='cuda:0') tensor(7.5885e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0918, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5856e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0919, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5853e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0919, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5849e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0920, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5846e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0920, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5842e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0921, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5838e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0921, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5835e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303433
tensor(-14.0922, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5831e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0922, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5828e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0923, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5824e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0923, device='cuda:0') tensor(1.0478e-07, device='cuda:0') tensor(7.5821e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000848
Average total loss: 2.303432
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
 Percentile value: -14.092354774475098
Non-zero model percentage: 4.398009300231934%, Non-zero mask percentage: 4.398054122924805%

--- Pruning Level [14/24]: ---
conv1.weight         | nonzeros =     320 /    1728             ( 18.52%) | total_pruned =    1408 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2360 /   36864             (  6.40%) | total_pruned =   34504 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4273 /   36864             ( 11.59%) | total_pruned =   32591 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4341 /   36864             ( 11.78%) | total_pruned =   32523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4329 /   36864             ( 11.74%) | total_pruned =   32535 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12017 /   73728             ( 16.30%) | total_pruned =   61711 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   19859 /  147456             ( 13.47%) | total_pruned =  127597 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1808 /    8192             ( 22.07%) | total_pruned =    6384 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4604 /  147456             (  3.12%) | total_pruned =  142852 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3603 /  147456             (  2.44%) | total_pruned =  143853 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42706 /  294912             ( 14.48%) | total_pruned =  252206 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  132594 / 2359296             (  5.62%) | total_pruned = 2226702 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     308 /     512             ( 60.16%) | total_pruned =     204 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4362 /    5120             ( 85.20%) | total_pruned =     758 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 491643, pruned : 10687119, total: 11178762, Compression rate :      22.74x  ( 95.60% pruned)
Train Epoch: 99/100 Loss: 0.462602 Accuracy: 75.11 85.94 % Best test Accuracy: 76.80%
tensor(-14.0924, device='cuda:0') tensor(1.0477e-07, device='cuda:0') tensor(7.5817e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000846
Average total loss: 2.303431
tensor(-14.0960, device='cuda:0') tensor(1.0396e-07, device='cuda:0') tensor(7.5538e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000843
Average total loss: 2.303428
tensor(-14.0997, device='cuda:0') tensor(1.0310e-07, device='cuda:0') tensor(7.5261e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000840
Average total loss: 2.303425
tensor(-14.1034, device='cuda:0') tensor(1.0220e-07, device='cuda:0') tensor(7.4987e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000837
Average total loss: 2.303422
tensor(-14.1070, device='cuda:0') tensor(1.0138e-07, device='cuda:0') tensor(7.4714e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000834
Average total loss: 2.303419
tensor(-14.1107, device='cuda:0') tensor(1.0063e-07, device='cuda:0') tensor(7.4442e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000831
Average total loss: 2.303416
tensor(-14.1143, device='cuda:0') tensor(9.9740e-08, device='cuda:0') tensor(7.4175e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000828
Average total loss: 2.303413
tensor(-14.1179, device='cuda:0') tensor(9.8919e-08, device='cuda:0') tensor(7.3908e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000825
Average total loss: 2.303410
tensor(-14.1215, device='cuda:0') tensor(9.8156e-08, device='cuda:0') tensor(7.3643e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000822
Average total loss: 2.303407
tensor(-14.1250, device='cuda:0') tensor(9.7400e-08, device='cuda:0') tensor(7.3379e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000819
Average total loss: 2.303404
tensor(-14.1286, device='cuda:0') tensor(9.6563e-08, device='cuda:0') tensor(7.3119e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000816
Average total loss: 2.303401
tensor(-14.1321, device='cuda:0') tensor(9.5792e-08, device='cuda:0') tensor(7.2860e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000813
Average total loss: 2.303398
tensor(-14.1357, device='cuda:0') tensor(9.5089e-08, device='cuda:0') tensor(7.2602e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-14.1361, device='cuda:0') tensor(9.5052e-08, device='cuda:0') tensor(7.2574e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-14.1364, device='cuda:0') tensor(9.5015e-08, device='cuda:0') tensor(7.2547e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-14.1368, device='cuda:0') tensor(9.4980e-08, device='cuda:0') tensor(7.2520e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303395
tensor(-14.1372, device='cuda:0') tensor(9.4947e-08, device='cuda:0') tensor(7.2493e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303395
tensor(-14.1376, device='cuda:0') tensor(9.4914e-08, device='cuda:0') tensor(7.2466e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303395
tensor(-14.1379, device='cuda:0') tensor(9.4881e-08, device='cuda:0') tensor(7.2439e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000810
Average total loss: 2.303394
tensor(-14.1383, device='cuda:0') tensor(9.4848e-08, device='cuda:0') tensor(7.2412e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-14.1387, device='cuda:0') tensor(9.4815e-08, device='cuda:0') tensor(7.2385e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-14.1391, device='cuda:0') tensor(9.4782e-08, device='cuda:0') tensor(7.2358e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000809
Average total loss: 2.303394
tensor(-14.1394, device='cuda:0') tensor(9.4750e-08, device='cuda:0') tensor(7.2331e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1398, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2304e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1399, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2300e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1399, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2297e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1399, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2294e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1400, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2290e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1400, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2287e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1401, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2283e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1401, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2280e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1402, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2277e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1402, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2273e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4717e-08, device='cuda:0') tensor(7.2270e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
 Percentile value: -14.14031982421875
Non-zero model percentage: 3.5184037685394287%, Non-zero mask percentage: 3.5184483528137207%

--- Pruning Level [15/24]: ---
conv1.weight         | nonzeros =     315 /    1728             ( 18.23%) | total_pruned =    1413 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1765 /   36864             (  4.79%) | total_pruned =   35099 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3028 /   36864             (  8.21%) | total_pruned =   33836 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2978 /   36864             (  8.08%) | total_pruned =   33886 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2906 /   36864             (  7.88%) | total_pruned =   33958 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    7861 /   73728             ( 10.66%) | total_pruned =   65867 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12151 /  147456             (  8.24%) | total_pruned =  135305 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1360 /    8192             ( 16.60%) | total_pruned =    6832 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2147 /  147456             (  1.46%) | total_pruned =  145309 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1757 /  147456             (  1.19%) | total_pruned =  145699 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   28716 /  294912             (  9.74%) | total_pruned =  266196 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     191 /     256             ( 74.61%) | total_pruned =      65 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   44664 /  589824             (  7.57%) | total_pruned =  545160 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2993 /   32768             (  9.13%) | total_pruned =   29775 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5686 /  589824             (  0.96%) | total_pruned =  584138 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4164 /  589824             (  0.71%) | total_pruned =  585660 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47868 / 1179648             (  4.06%) | total_pruned = 1131780 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     324 /     512             ( 63.28%) | total_pruned =     188 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   61118 / 2359296             (  2.59%) | total_pruned = 2298178 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     402 /     512             ( 78.52%) | total_pruned =     110 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4705 /  131072             (  3.59%) | total_pruned =  126367 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   70061 / 2359296             (  2.97%) | total_pruned = 2289235 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     303 /     512             ( 59.18%) | total_pruned =     209 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4350 /    5120             ( 84.96%) | total_pruned =     770 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 393314, pruned : 10785448, total: 11178762, Compression rate :      28.42x  ( 96.48% pruned)
Train Epoch: 99/100 Loss: 0.521720 Accuracy: 75.79 84.50 % Best test Accuracy: 76.37%
tensor(-14.1403, device='cuda:0') tensor(9.4721e-08, device='cuda:0') tensor(7.2266e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000806
Average total loss: 2.303391
tensor(-14.1438, device='cuda:0') tensor(9.3950e-08, device='cuda:0') tensor(7.2014e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000804
Average total loss: 2.303389
tensor(-14.1473, device='cuda:0') tensor(9.3249e-08, device='cuda:0') tensor(7.1762e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000801
Average total loss: 2.303386
tensor(-14.1508, device='cuda:0') tensor(9.2595e-08, device='cuda:0') tensor(7.1511e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000798
Average total loss: 2.303383
tensor(-14.1543, device='cuda:0') tensor(9.1795e-08, device='cuda:0') tensor(7.1264e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000795
Average total loss: 2.303380
tensor(-14.1577, device='cuda:0') tensor(9.1068e-08, device='cuda:0') tensor(7.1018e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000793
Average total loss: 2.303377
tensor(-14.1612, device='cuda:0') tensor(9.0402e-08, device='cuda:0') tensor(7.0773e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000790
Average total loss: 2.303375
tensor(-14.1647, device='cuda:0') tensor(8.9771e-08, device='cuda:0') tensor(7.0529e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000787
Average total loss: 2.303372
tensor(-14.1681, device='cuda:0') tensor(8.9013e-08, device='cuda:0') tensor(7.0288e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000784
Average total loss: 2.303369
tensor(-14.1715, device='cuda:0') tensor(8.8315e-08, device='cuda:0') tensor(7.0049e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000782
Average total loss: 2.303367
tensor(-14.1749, device='cuda:0') tensor(8.7689e-08, device='cuda:0') tensor(6.9811e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000779
Average total loss: 2.303364
tensor(-14.1783, device='cuda:0') tensor(8.7087e-08, device='cuda:0') tensor(6.9573e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000776
Average total loss: 2.303361
tensor(-14.1817, device='cuda:0') tensor(8.6356e-08, device='cuda:0') tensor(6.9339e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000775
Average total loss: 2.303360
tensor(-14.1820, device='cuda:0') tensor(8.6285e-08, device='cuda:0') tensor(6.9317e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000775
Average total loss: 2.303360
tensor(-14.1823, device='cuda:0') tensor(8.6222e-08, device='cuda:0') tensor(6.9294e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-14.1826, device='cuda:0') tensor(8.6152e-08, device='cuda:0') tensor(6.9271e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-14.1830, device='cuda:0') tensor(8.6090e-08, device='cuda:0') tensor(6.9249e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-14.1833, device='cuda:0') tensor(8.6022e-08, device='cuda:0') tensor(6.9226e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000774
Average total loss: 2.303359
tensor(-14.1836, device='cuda:0') tensor(8.5961e-08, device='cuda:0') tensor(6.9203e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-14.1840, device='cuda:0') tensor(8.5894e-08, device='cuda:0') tensor(6.9181e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-14.1843, device='cuda:0') tensor(8.5835e-08, device='cuda:0') tensor(6.9158e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-14.1846, device='cuda:0') tensor(8.5769e-08, device='cuda:0') tensor(6.9135e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000773
Average total loss: 2.303358
tensor(-14.1849, device='cuda:0') tensor(8.5711e-08, device='cuda:0') tensor(6.9113e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1853, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9090e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1853, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9087e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1854, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9084e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1854, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9081e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1855, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9077e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1855, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9074e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1855, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9071e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1856, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9068e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1856, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9064e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1857, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9061e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1857, device='cuda:0') tensor(8.5645e-08, device='cuda:0') tensor(6.9058e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000772
Average total loss: 2.303357
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
 Percentile value: -14.185781478881836
Non-zero model percentage: 2.8147213459014893%, Non-zero mask percentage: 2.8147659301757812%

--- Pruning Level [16/24]: ---
conv1.weight         | nonzeros =     315 /    1728             ( 18.23%) | total_pruned =    1413 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1765 /   36864             (  4.79%) | total_pruned =   35099 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3028 /   36864             (  8.21%) | total_pruned =   33836 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2978 /   36864             (  8.08%) | total_pruned =   33886 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2906 /   36864             (  7.88%) | total_pruned =   33958 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    7861 /   73728             ( 10.66%) | total_pruned =   65867 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12151 /  147456             (  8.24%) | total_pruned =  135305 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1360 /    8192             ( 16.60%) | total_pruned =    6832 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2147 /  147456             (  1.46%) | total_pruned =  145309 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1757 /  147456             (  1.19%) | total_pruned =  145699 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   26115 /  294912             (  8.86%) | total_pruned =  268797 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     189 /     256             ( 73.83%) | total_pruned =      67 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   27019 /  589824             (  4.58%) | total_pruned =  562805 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     139 /     256             ( 54.30%) | total_pruned =     117 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1951 /   32768             (  5.95%) | total_pruned =   30817 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2177 /  589824             (  0.37%) | total_pruned =  587647 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1681 /  589824             (  0.29%) | total_pruned =  588143 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   27244 / 1179648             (  2.31%) | total_pruned = 1152404 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     322 /     512             ( 62.89%) | total_pruned =     190 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   30550 / 2359296             (  1.29%) | total_pruned = 2328746 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4568 /  131072             (  3.49%) | total_pruned =  126504 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   77329 / 2359296             (  3.28%) | total_pruned = 2281967 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   70061 / 2359296             (  2.97%) | total_pruned = 2289235 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     303 /     512             ( 59.18%) | total_pruned =     209 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4350 /    5120             ( 84.96%) | total_pruned =     770 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 314651, pruned : 10864111, total: 11178762, Compression rate :      35.53x  ( 97.19% pruned)
Train Epoch: 99/100 Loss: 0.543545 Accuracy: 74.08 81.27 % Best test Accuracy: 74.29%
tensor(-14.1858, device='cuda:0') tensor(8.5649e-08, device='cuda:0') tensor(6.9055e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000771
Average total loss: 2.303356
tensor(-14.1891, device='cuda:0') tensor(8.5052e-08, device='cuda:0') tensor(6.8823e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000768
Average total loss: 2.303353
tensor(-14.1925, device='cuda:0') tensor(8.4478e-08, device='cuda:0') tensor(6.8592e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000766
Average total loss: 2.303350
tensor(-14.1958, device='cuda:0') tensor(8.3776e-08, device='cuda:0') tensor(6.8365e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000763
Average total loss: 2.303348
tensor(-14.1991, device='cuda:0') tensor(8.3146e-08, device='cuda:0') tensor(6.8138e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000760
Average total loss: 2.303345
tensor(-14.2025, device='cuda:0') tensor(8.2559e-08, device='cuda:0') tensor(6.7912e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000758
Average total loss: 2.303343
tensor(-14.2058, device='cuda:0') tensor(8.2026e-08, device='cuda:0') tensor(6.7688e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000755
Average total loss: 2.303340
tensor(-14.2091, device='cuda:0') tensor(8.1371e-08, device='cuda:0') tensor(6.7466e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000753
Average total loss: 2.303338
tensor(-14.2123, device='cuda:0') tensor(8.0748e-08, device='cuda:0') tensor(6.7246e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000751
Average total loss: 2.303335
tensor(-14.2156, device='cuda:0') tensor(8.0180e-08, device='cuda:0') tensor(6.7026e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000748
Average total loss: 2.303333
tensor(-14.2189, device='cuda:0') tensor(7.9654e-08, device='cuda:0') tensor(6.6807e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000746
Average total loss: 2.303331
tensor(-14.2221, device='cuda:0') tensor(7.9083e-08, device='cuda:0') tensor(6.6590e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000743
Average total loss: 2.303328
tensor(-14.2253, device='cuda:0') tensor(7.8460e-08, device='cuda:0') tensor(6.6376e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000742
Average total loss: 2.303327
tensor(-14.2257, device='cuda:0') tensor(7.8433e-08, device='cuda:0') tensor(6.6354e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000742
Average total loss: 2.303327
tensor(-14.2260, device='cuda:0') tensor(7.8399e-08, device='cuda:0') tensor(6.6333e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-14.2263, device='cuda:0') tensor(7.8373e-08, device='cuda:0') tensor(6.6311e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-14.2267, device='cuda:0') tensor(7.8338e-08, device='cuda:0') tensor(6.6289e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-14.2270, device='cuda:0') tensor(7.8312e-08, device='cuda:0') tensor(6.6268e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000741
Average total loss: 2.303326
tensor(-14.2273, device='cuda:0') tensor(7.8278e-08, device='cuda:0') tensor(6.6246e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-14.2276, device='cuda:0') tensor(7.8252e-08, device='cuda:0') tensor(6.6224e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-14.2280, device='cuda:0') tensor(7.8217e-08, device='cuda:0') tensor(6.6203e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-14.2283, device='cuda:0') tensor(7.8191e-08, device='cuda:0') tensor(6.6181e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000740
Average total loss: 2.303325
tensor(-14.2286, device='cuda:0') tensor(7.8157e-08, device='cuda:0') tensor(6.6159e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2289, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6138e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2290, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6135e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2290, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6131e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2291, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6128e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2291, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6125e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2292, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6122e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2292, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6119e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2293, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6116e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2293, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6113e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2294, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6110e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2294, device='cuda:0') tensor(7.8130e-08, device='cuda:0') tensor(6.6107e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000739
Average total loss: 2.303324
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
 Percentile value: -14.229455947875977
Non-zero model percentage: 2.251769781112671%, Non-zero mask percentage: 2.251814603805542%

--- Pruning Level [17/24]: ---
conv1.weight         | nonzeros =     315 /    1728             ( 18.23%) | total_pruned =    1413 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1765 /   36864             (  4.79%) | total_pruned =   35099 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3028 /   36864             (  8.21%) | total_pruned =   33836 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2978 /   36864             (  8.08%) | total_pruned =   33886 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2906 /   36864             (  7.88%) | total_pruned =   33958 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    7861 /   73728             ( 10.66%) | total_pruned =   65867 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12151 /  147456             (  8.24%) | total_pruned =  135305 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1360 /    8192             ( 16.60%) | total_pruned =    6832 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2147 /  147456             (  1.46%) | total_pruned =  145309 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1757 /  147456             (  1.19%) | total_pruned =  145699 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      93 /     128             ( 72.66%) | total_pruned =      35 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   26115 /  294912             (  8.86%) | total_pruned =  268797 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     189 /     256             ( 73.83%) | total_pruned =      67 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   27019 /  589824             (  4.58%) | total_pruned =  562805 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     139 /     256             ( 54.30%) | total_pruned =     117 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1951 /   32768             (  5.95%) | total_pruned =   30817 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2177 /  589824             (  0.37%) | total_pruned =  587647 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1681 /  589824             (  0.29%) | total_pruned =  588143 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   27244 / 1179648             (  2.31%) | total_pruned = 1152404 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     322 /     512             ( 62.89%) | total_pruned =     190 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   30550 / 2359296             (  1.29%) | total_pruned = 2328746 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2678 /  131072             (  2.04%) | total_pruned =  128394 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     152 /     512             ( 29.69%) | total_pruned =     360 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     387 /     512             ( 75.59%) | total_pruned =     125 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   41672 / 2359296             (  1.77%) | total_pruned = 2317624 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   44720 / 2359296             (  1.90%) | total_pruned = 2314576 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     303 /     512             ( 59.18%) | total_pruned =     209 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4350 /    5120             ( 84.96%) | total_pruned =     770 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 251720, pruned : 10927042, total: 11178762, Compression rate :      44.41x  ( 97.75% pruned)
Train Epoch: 99/100 Loss: 0.584518 Accuracy: 72.91 79.79 % Best test Accuracy: 73.45%
tensor(-14.2295, device='cuda:0') tensor(7.8126e-08, device='cuda:0') tensor(6.6104e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000738
Average total loss: 2.303323
tensor(-14.2327, device='cuda:0') tensor(7.7620e-08, device='cuda:0') tensor(6.5891e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000735
Average total loss: 2.303320
tensor(-14.2359, device='cuda:0') tensor(7.7115e-08, device='cuda:0') tensor(6.5679e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000733
Average total loss: 2.303318
tensor(-14.2391, device='cuda:0') tensor(7.6501e-08, device='cuda:0') tensor(6.5471e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000731
Average total loss: 2.303316
tensor(-14.2422, device='cuda:0') tensor(7.5947e-08, device='cuda:0') tensor(6.5263e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000728
Average total loss: 2.303313
tensor(-14.2454, device='cuda:0') tensor(7.5437e-08, device='cuda:0') tensor(6.5056e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000726
Average total loss: 2.303311
tensor(-14.2486, device='cuda:0') tensor(7.4969e-08, device='cuda:0') tensor(6.4850e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000724
Average total loss: 2.303309
tensor(-14.2518, device='cuda:0') tensor(7.4436e-08, device='cuda:0') tensor(6.4646e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000722
Average total loss: 2.303306
tensor(-14.2549, device='cuda:0') tensor(7.3867e-08, device='cuda:0') tensor(6.4443e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000719
Average total loss: 2.303304
tensor(-14.2580, device='cuda:0') tensor(7.3357e-08, device='cuda:0') tensor(6.4242e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000717
Average total loss: 2.303302
tensor(-14.2612, device='cuda:0') tensor(7.2881e-08, device='cuda:0') tensor(6.4041e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000715
Average total loss: 2.303300
tensor(-14.2643, device='cuda:0') tensor(7.2447e-08, device='cuda:0') tensor(6.3841e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000713
Average total loss: 2.303297
tensor(-14.2674, device='cuda:0') tensor(7.1908e-08, device='cuda:0') tensor(6.3644e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-14.2677, device='cuda:0') tensor(7.1882e-08, device='cuda:0') tensor(6.3623e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-14.2680, device='cuda:0') tensor(7.1865e-08, device='cuda:0') tensor(6.3602e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-14.2684, device='cuda:0') tensor(7.1839e-08, device='cuda:0') tensor(6.3581e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000711
Average total loss: 2.303296
tensor(-14.2687, device='cuda:0') tensor(7.1822e-08, device='cuda:0') tensor(6.3561e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2690, device='cuda:0') tensor(7.1796e-08, device='cuda:0') tensor(6.3540e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2693, device='cuda:0') tensor(7.1779e-08, device='cuda:0') tensor(6.3519e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2697, device='cuda:0') tensor(7.1753e-08, device='cuda:0') tensor(6.3498e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2700, device='cuda:0') tensor(7.1735e-08, device='cuda:0') tensor(6.3477e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2703, device='cuda:0') tensor(7.1710e-08, device='cuda:0') tensor(6.3457e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2707, device='cuda:0') tensor(7.1692e-08, device='cuda:0') tensor(6.3436e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2710, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3415e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2710, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3412e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2711, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3409e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2711, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3406e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2712, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3403e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2712, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3400e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2713, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3397e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303294
tensor(-14.2713, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3394e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2714, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3391e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2714, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3388e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2714, device='cuda:0') tensor(7.1667e-08, device='cuda:0') tensor(6.3386e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000709
Average total loss: 2.303293
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
 Percentile value: -14.271491050720215
Non-zero model percentage: 1.801406979560852%, Non-zero mask percentage: 1.8014516830444336%

--- Pruning Level [18/24]: ---
conv1.weight         | nonzeros =     295 /    1728             ( 17.07%) | total_pruned =    1433 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     953 /   36864             (  2.59%) | total_pruned =   35911 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1462 /   36864             (  3.97%) | total_pruned =   35402 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1349 /   36864             (  3.66%) | total_pruned =   35515 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1341 /   36864             (  3.64%) | total_pruned =   35523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2981 /   73728             (  4.04%) | total_pruned =   70747 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4297 /  147456             (  2.91%) | total_pruned =  143159 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     792 /    8192             (  9.67%) | total_pruned =    7400 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     527 /  147456             (  0.36%) | total_pruned =  146929 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     424 /  147456             (  0.29%) | total_pruned =  147032 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8882 /  294912             (  3.01%) | total_pruned =  286030 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     186 /     256             ( 72.66%) | total_pruned =      70 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   23637 /  589824             (  4.01%) | total_pruned =  566187 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     139 /     256             ( 54.30%) | total_pruned =     117 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     176 /     256             ( 68.75%) | total_pruned =      80 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1951 /   32768             (  5.95%) | total_pruned =   30817 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2177 /  589824             (  0.37%) | total_pruned =  587647 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1681 /  589824             (  0.29%) | total_pruned =  588143 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   27244 / 1179648             (  2.31%) | total_pruned = 1152404 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     322 /     512             ( 62.89%) | total_pruned =     190 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   30550 / 2359296             (  1.29%) | total_pruned = 2328746 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2678 /  131072             (  2.04%) | total_pruned =  128394 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     152 /     512             ( 29.69%) | total_pruned =     360 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     387 /     512             ( 75.59%) | total_pruned =     125 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   41672 / 2359296             (  1.77%) | total_pruned = 2317624 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   36931 / 2359296             (  1.57%) | total_pruned = 2322365 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4347 /    5120             ( 84.90%) | total_pruned =     773 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 201375, pruned : 10977387, total: 11178762, Compression rate :      55.51x  ( 98.20% pruned)
Train Epoch: 99/100 Loss: 0.837247 Accuracy: 66.28 69.93 % Best test Accuracy: 67.40%
tensor(-14.2715, device='cuda:0') tensor(7.1671e-08, device='cuda:0') tensor(6.3383e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000707
Average total loss: 2.303292
tensor(-14.2746, device='cuda:0') tensor(7.1199e-08, device='cuda:0') tensor(6.3187e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000705
Average total loss: 2.303290
tensor(-14.2777, device='cuda:0') tensor(7.0761e-08, device='cuda:0') tensor(6.2993e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000703
Average total loss: 2.303288
tensor(-14.2807, device='cuda:0') tensor(7.0336e-08, device='cuda:0') tensor(6.2799e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000701
Average total loss: 2.303286
tensor(-14.2838, device='cuda:0') tensor(6.9789e-08, device='cuda:0') tensor(6.2609e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000699
Average total loss: 2.303284
tensor(-14.2868, device='cuda:0') tensor(6.9304e-08, device='cuda:0') tensor(6.2419e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000697
Average total loss: 2.303282
tensor(-14.2899, device='cuda:0') tensor(6.8848e-08, device='cuda:0') tensor(6.2229e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000695
Average total loss: 2.303280
tensor(-14.2929, device='cuda:0') tensor(6.8439e-08, device='cuda:0') tensor(6.2041e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000693
Average total loss: 2.303277
tensor(-14.2959, device='cuda:0') tensor(6.8044e-08, device='cuda:0') tensor(6.1853e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000690
Average total loss: 2.303275
tensor(-14.2989, device='cuda:0') tensor(6.7521e-08, device='cuda:0') tensor(6.1668e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000688
Average total loss: 2.303273
tensor(-14.3019, device='cuda:0') tensor(6.7051e-08, device='cuda:0') tensor(6.1484e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000686
Average total loss: 2.303271
tensor(-14.3049, device='cuda:0') tensor(6.6616e-08, device='cuda:0') tensor(6.1300e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000684
Average total loss: 2.303269
tensor(-14.3079, device='cuda:0') tensor(6.6217e-08, device='cuda:0') tensor(6.1117e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303268
tensor(-14.3082, device='cuda:0') tensor(6.6126e-08, device='cuda:0') tensor(6.1100e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303268
tensor(-14.3084, device='cuda:0') tensor(6.6037e-08, device='cuda:0') tensor(6.1083e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303268
tensor(-14.3087, device='cuda:0') tensor(6.5949e-08, device='cuda:0') tensor(6.1066e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000683
Average total loss: 2.303267
tensor(-14.3090, device='cuda:0') tensor(6.5863e-08, device='cuda:0') tensor(6.1049e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-14.3093, device='cuda:0') tensor(6.5778e-08, device='cuda:0') tensor(6.1032e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-14.3096, device='cuda:0') tensor(6.5695e-08, device='cuda:0') tensor(6.1014e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-14.3098, device='cuda:0') tensor(6.5613e-08, device='cuda:0') tensor(6.0997e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303267
tensor(-14.3101, device='cuda:0') tensor(6.5532e-08, device='cuda:0') tensor(6.0980e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000682
Average total loss: 2.303266
tensor(-14.3104, device='cuda:0') tensor(6.5452e-08, device='cuda:0') tensor(6.0963e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3107, device='cuda:0') tensor(6.5374e-08, device='cuda:0') tensor(6.0946e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3110, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0929e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3110, device='cuda:0') tensor(6.5299e-08, device='cuda:0') tensor(6.0926e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3111, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0923e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3111, device='cuda:0') tensor(6.5299e-08, device='cuda:0') tensor(6.0920e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3112, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0918e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3112, device='cuda:0') tensor(6.5299e-08, device='cuda:0') tensor(6.0915e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3112, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0912e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3113, device='cuda:0') tensor(6.5299e-08, device='cuda:0') tensor(6.0909e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3113, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0906e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3114, device='cuda:0') tensor(6.5299e-08, device='cuda:0') tensor(6.0903e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3114, device='cuda:0') tensor(6.5296e-08, device='cuda:0') tensor(6.0901e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000681
Average total loss: 2.303266
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
 Percentile value: -14.311484336853027
Non-zero model percentage: 1.4411165714263916%, Non-zero mask percentage: 1.4411613941192627%

--- Pruning Level [19/24]: ---
conv1.weight         | nonzeros =     295 /    1728             ( 17.07%) | total_pruned =    1433 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     953 /   36864             (  2.59%) | total_pruned =   35911 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1462 /   36864             (  3.97%) | total_pruned =   35402 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1349 /   36864             (  3.66%) | total_pruned =   35515 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1341 /   36864             (  3.64%) | total_pruned =   35523 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2981 /   73728             (  4.04%) | total_pruned =   70747 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4297 /  147456             (  2.91%) | total_pruned =  143159 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     792 /    8192             (  9.67%) | total_pruned =    7400 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     527 /  147456             (  0.36%) | total_pruned =  146929 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     424 /  147456             (  0.29%) | total_pruned =  147032 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8882 /  294912             (  3.01%) | total_pruned =  286030 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     186 /     256             ( 72.66%) | total_pruned =      70 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   10016 /  589824             (  1.70%) | total_pruned =  579808 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     866 /   32768             (  2.64%) | total_pruned =   31902 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     328 /  589824             (  0.06%) | total_pruned =  589496 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     298 /  589824             (  0.05%) | total_pruned =  589526 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      63 /     256             ( 24.61%) | total_pruned =     193 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    9323 / 1179648             (  0.79%) | total_pruned = 1170325 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     227 /     512             ( 44.34%) | total_pruned =     285 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     316 /     512             ( 61.72%) | total_pruned =     196 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   26292 / 2359296             (  1.11%) | total_pruned = 2333004 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2678 /  131072             (  2.04%) | total_pruned =  128394 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     152 /     512             ( 29.69%) | total_pruned =     360 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     387 /     512             ( 75.59%) | total_pruned =     125 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   41672 / 2359296             (  1.77%) | total_pruned = 2317624 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   36931 / 2359296             (  1.57%) | total_pruned = 2322365 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     502 /     512             ( 98.05%) | total_pruned =      10 | shape = torch.Size([512])
linear.weight        | nonzeros =    4347 /    5120             ( 84.90%) | total_pruned =     773 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 161099, pruned : 11017663, total: 11178762, Compression rate :      69.39x  ( 98.56% pruned)
Train Epoch: 99/100 Loss: 0.867452 Accuracy: 65.91 69.25 % Best test Accuracy: 66.04%
tensor(-14.3115, device='cuda:0') tensor(6.5300e-08, device='cuda:0') tensor(6.0898e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000680
Average total loss: 2.303265
tensor(-14.3144, device='cuda:0') tensor(6.4799e-08, device='cuda:0') tensor(6.0719e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000678
Average total loss: 2.303263
tensor(-14.3174, device='cuda:0') tensor(6.4350e-08, device='cuda:0') tensor(6.0540e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000676
Average total loss: 2.303261
tensor(-14.3203, device='cuda:0') tensor(6.3934e-08, device='cuda:0') tensor(6.0362e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000674
Average total loss: 2.303259
tensor(-14.3233, device='cuda:0') tensor(6.3555e-08, device='cuda:0') tensor(6.0185e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000672
Average total loss: 2.303257
tensor(-14.3262, device='cuda:0') tensor(6.3205e-08, device='cuda:0') tensor(6.0008e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000670
Average total loss: 2.303255
tensor(-14.3291, device='cuda:0') tensor(6.2786e-08, device='cuda:0') tensor(5.9833e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000668
Average total loss: 2.303253
tensor(-14.3320, device='cuda:0') tensor(6.2336e-08, device='cuda:0') tensor(5.9660e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000666
Average total loss: 2.303251
tensor(-14.3349, device='cuda:0') tensor(6.1927e-08, device='cuda:0') tensor(5.9487e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000664
Average total loss: 2.303249
tensor(-14.3378, device='cuda:0') tensor(6.1553e-08, device='cuda:0') tensor(5.9315e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000662
Average total loss: 2.303247
tensor(-14.3407, device='cuda:0') tensor(6.1208e-08, device='cuda:0') tensor(5.9144e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000660
Average total loss: 2.303245
tensor(-14.3436, device='cuda:0') tensor(6.0891e-08, device='cuda:0') tensor(5.8973e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000658
Average total loss: 2.303243
tensor(-14.3465, device='cuda:0') tensor(6.0433e-08, device='cuda:0') tensor(5.8805e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-14.3467, device='cuda:0') tensor(6.0411e-08, device='cuda:0') tensor(5.8788e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-14.3470, device='cuda:0') tensor(6.0390e-08, device='cuda:0') tensor(5.8772e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-14.3473, device='cuda:0') tensor(6.0369e-08, device='cuda:0') tensor(5.8755e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303242
tensor(-14.3476, device='cuda:0') tensor(6.0347e-08, device='cuda:0') tensor(5.8739e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000657
Average total loss: 2.303241
tensor(-14.3479, device='cuda:0') tensor(6.0326e-08, device='cuda:0') tensor(5.8722e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-14.3481, device='cuda:0') tensor(6.0305e-08, device='cuda:0') tensor(5.8706e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-14.3484, device='cuda:0') tensor(6.0284e-08, device='cuda:0') tensor(5.8689e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-14.3487, device='cuda:0') tensor(6.0263e-08, device='cuda:0') tensor(5.8673e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-14.3490, device='cuda:0') tensor(6.0242e-08, device='cuda:0') tensor(5.8656e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000656
Average total loss: 2.303241
tensor(-14.3493, device='cuda:0') tensor(6.0221e-08, device='cuda:0') tensor(5.8640e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3495, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8624e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3496, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8621e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3496, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8618e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3497, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8615e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3497, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8613e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3498, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8610e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3498, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8607e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3499, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8604e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3499, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8602e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3500, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8599e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3500, device='cuda:0') tensor(6.0200e-08, device='cuda:0') tensor(5.8596e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000655
Average total loss: 2.303240
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
 Percentile value: -14.350057601928711
Non-zero model percentage: 1.1528825759887695%, Non-zero mask percentage: 1.1529362201690674%

--- Pruning Level [20/24]: ---
conv1.weight         | nonzeros =     294 /    1728             ( 17.01%) | total_pruned =    1434 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     920 /   36864             (  2.50%) | total_pruned =   35944 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1410 /   36864             (  3.82%) | total_pruned =   35454 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1292 /   36864             (  3.50%) | total_pruned =   35572 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1282 /   36864             (  3.48%) | total_pruned =   35582 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2845 /   73728             (  3.86%) | total_pruned =   70883 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4064 /  147456             (  2.76%) | total_pruned =  143392 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     772 /    8192             (  9.42%) | total_pruned =    7420 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     490 /  147456             (  0.33%) | total_pruned =  146966 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     395 /  147456             (  0.27%) | total_pruned =  147061 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8347 /  294912             (  2.83%) | total_pruned =  286565 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     185 /     256             ( 72.27%) | total_pruned =      71 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    9516 /  589824             (  1.61%) | total_pruned =  580308 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     833 /   32768             (  2.54%) | total_pruned =   31935 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     301 /  589824             (  0.05%) | total_pruned =  589523 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      89 /     256             ( 34.77%) | total_pruned =     167 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     271 /  589824             (  0.05%) | total_pruned =  589553 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8797 / 1179648             (  0.75%) | total_pruned = 1170851 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     316 /     512             ( 61.72%) | total_pruned =     196 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    7708 / 2359296             (  0.33%) | total_pruned = 2351588 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     200 /     512             ( 39.06%) | total_pruned =     312 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     865 /  131072             (  0.66%) | total_pruned =  130207 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     106 /     512             ( 20.70%) | total_pruned =     406 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   32273 / 2359296             (  1.37%) | total_pruned = 2327023 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   36931 / 2359296             (  1.57%) | total_pruned = 2322365 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     501 /     512             ( 97.85%) | total_pruned =      11 | shape = torch.Size([512])
linear.weight        | nonzeros =    4347 /    5120             ( 84.90%) | total_pruned =     773 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 128878, pruned : 11049884, total: 11178762, Compression rate :      86.74x  ( 98.85% pruned)
Train Epoch: 99/100 Loss: 1.160396 Accuracy: 65.02 67.48 % Best test Accuracy: 65.15%
tensor(-14.3501, device='cuda:0') tensor(6.0197e-08, device='cuda:0') tensor(5.8593e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000654
Average total loss: 2.303239
tensor(-14.3529, device='cuda:0') tensor(5.9832e-08, device='cuda:0') tensor(5.8427e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000652
Average total loss: 2.303237
tensor(-14.3558, device='cuda:0') tensor(5.9489e-08, device='cuda:0') tensor(5.8260e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000650
Average total loss: 2.303235
tensor(-14.3586, device='cuda:0') tensor(5.9178e-08, device='cuda:0') tensor(5.8094e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000649
Average total loss: 2.303233
tensor(-14.3614, device='cuda:0') tensor(5.8808e-08, device='cuda:0') tensor(5.7930e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000647
Average total loss: 2.303232
tensor(-14.3642, device='cuda:0') tensor(5.8392e-08, device='cuda:0') tensor(5.7768e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000645
Average total loss: 2.303230
tensor(-14.3670, device='cuda:0') tensor(5.8013e-08, device='cuda:0') tensor(5.7606e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000643
Average total loss: 2.303228
tensor(-14.3698, device='cuda:0') tensor(5.7667e-08, device='cuda:0') tensor(5.7445e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000641
Average total loss: 2.303226
tensor(-14.3727, device='cuda:0') tensor(5.7349e-08, device='cuda:0') tensor(5.7284e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000639
Average total loss: 2.303224
tensor(-14.3755, device='cuda:0') tensor(5.7054e-08, device='cuda:0') tensor(5.7124e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000638
Average total loss: 2.303223
tensor(-14.3782, device='cuda:0') tensor(5.6709e-08, device='cuda:0') tensor(5.6965e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000636
Average total loss: 2.303221
tensor(-14.3810, device='cuda:0') tensor(5.6311e-08, device='cuda:0') tensor(5.6808e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000634
Average total loss: 2.303219
tensor(-14.3838, device='cuda:0') tensor(5.5954e-08, device='cuda:0') tensor(5.6652e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-14.3840, device='cuda:0') tensor(5.5935e-08, device='cuda:0') tensor(5.6636e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-14.3843, device='cuda:0') tensor(5.5917e-08, device='cuda:0') tensor(5.6620e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-14.3846, device='cuda:0') tensor(5.5898e-08, device='cuda:0') tensor(5.6604e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303218
tensor(-14.3849, device='cuda:0') tensor(5.5879e-08, device='cuda:0') tensor(5.6588e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000633
Average total loss: 2.303217
tensor(-14.3852, device='cuda:0') tensor(5.5860e-08, device='cuda:0') tensor(5.6573e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.3854, device='cuda:0') tensor(5.5841e-08, device='cuda:0') tensor(5.6557e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.3857, device='cuda:0') tensor(5.5823e-08, device='cuda:0') tensor(5.6541e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.3860, device='cuda:0') tensor(5.5804e-08, device='cuda:0') tensor(5.6525e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.3863, device='cuda:0') tensor(5.5785e-08, device='cuda:0') tensor(5.6509e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303216
tensor(-14.3866, device='cuda:0') tensor(5.5766e-08, device='cuda:0') tensor(5.6493e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3868, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6477e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3869, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6475e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3869, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6472e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3870, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6470e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3870, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6467e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3871, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6464e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3871, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6462e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3872, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6459e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3872, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6456e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3873, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6454e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3873, device='cuda:0') tensor(5.5747e-08, device='cuda:0') tensor(5.6451e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000631
Average total loss: 2.303216
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
 Percentile value: -14.38735294342041
Non-zero model percentage: 0.9223024249076843%, Non-zero mask percentage: 0.9223561882972717%

--- Pruning Level [21/24]: ---
conv1.weight         | nonzeros =     294 /    1728             ( 17.01%) | total_pruned =    1434 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     920 /   36864             (  2.50%) | total_pruned =   35944 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1410 /   36864             (  3.82%) | total_pruned =   35454 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1292 /   36864             (  3.50%) | total_pruned =   35572 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1282 /   36864             (  3.48%) | total_pruned =   35582 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2845 /   73728             (  3.86%) | total_pruned =   70883 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4064 /  147456             (  2.76%) | total_pruned =  143392 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     772 /    8192             (  9.42%) | total_pruned =    7420 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     490 /  147456             (  0.33%) | total_pruned =  146966 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     395 /  147456             (  0.27%) | total_pruned =  147061 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8347 /  294912             (  2.83%) | total_pruned =  286565 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     185 /     256             ( 72.27%) | total_pruned =      71 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    9516 /  589824             (  1.61%) | total_pruned =  580308 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     833 /   32768             (  2.54%) | total_pruned =   31935 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     301 /  589824             (  0.05%) | total_pruned =  589523 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      89 /     256             ( 34.77%) | total_pruned =     167 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     271 /  589824             (  0.05%) | total_pruned =  589553 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8797 / 1179648             (  0.75%) | total_pruned = 1170851 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     316 /     512             ( 61.72%) | total_pruned =     196 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    7708 / 2359296             (  0.33%) | total_pruned = 2351588 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     200 /     512             ( 39.06%) | total_pruned =     312 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     865 /  131072             (  0.66%) | total_pruned =  130207 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     106 /     512             ( 20.70%) | total_pruned =     406 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   11938 / 2359296             (  0.51%) | total_pruned = 2347358 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     210 /     512             ( 41.02%) | total_pruned =     302 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     218 /     512             ( 42.58%) | total_pruned =     294 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   31550 / 2359296             (  1.34%) | total_pruned = 2327746 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     501 /     512             ( 97.85%) | total_pruned =      11 | shape = torch.Size([512])
linear.weight        | nonzeros =    4347 /    5120             ( 84.90%) | total_pruned =     773 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 103102, pruned : 11075660, total: 11178762, Compression rate :     108.42x  ( 99.08% pruned)
Train Epoch: 99/100 Loss: 1.176726 Accuracy: 63.73 65.21 % Best test Accuracy: 63.86%
tensor(-14.3874, device='cuda:0') tensor(5.5743e-08, device='cuda:0') tensor(5.6448e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000630
Average total loss: 2.303215
tensor(-14.3901, device='cuda:0') tensor(5.5452e-08, device='cuda:0') tensor(5.6293e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000628
Average total loss: 2.303213
tensor(-14.3929, device='cuda:0') tensor(5.5173e-08, device='cuda:0') tensor(5.6138e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000627
Average total loss: 2.303212
tensor(-14.3956, device='cuda:0') tensor(5.4823e-08, device='cuda:0') tensor(5.5985e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000625
Average total loss: 2.303210
tensor(-14.3983, device='cuda:0') tensor(5.4450e-08, device='cuda:0') tensor(5.5833e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000623
Average total loss: 2.303208
tensor(-14.4010, device='cuda:0') tensor(5.4111e-08, device='cuda:0') tensor(5.5682e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000622
Average total loss: 2.303207
tensor(-14.4037, device='cuda:0') tensor(5.3797e-08, device='cuda:0') tensor(5.5532e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000620
Average total loss: 2.303205
tensor(-14.4064, device='cuda:0') tensor(5.3511e-08, device='cuda:0') tensor(5.5381e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000618
Average total loss: 2.303203
tensor(-14.4091, device='cuda:0') tensor(5.3246e-08, device='cuda:0') tensor(5.5231e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000617
Average total loss: 2.303201
tensor(-14.4119, device='cuda:0') tensor(5.2965e-08, device='cuda:0') tensor(5.5082e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000615
Average total loss: 2.303200
tensor(-14.4145, device='cuda:0') tensor(5.2600e-08, device='cuda:0') tensor(5.4936e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000613
Average total loss: 2.303198
tensor(-14.4172, device='cuda:0') tensor(5.2262e-08, device='cuda:0') tensor(5.4790e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000612
Average total loss: 2.303197
tensor(-14.4198, device='cuda:0') tensor(5.1958e-08, device='cuda:0') tensor(5.4644e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000611
Average total loss: 2.303196
tensor(-14.4201, device='cuda:0') tensor(5.1947e-08, device='cuda:0') tensor(5.4629e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000611
Average total loss: 2.303195
tensor(-14.4204, device='cuda:0') tensor(5.1936e-08, device='cuda:0') tensor(5.4613e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-14.4207, device='cuda:0') tensor(5.1925e-08, device='cuda:0') tensor(5.4598e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-14.4210, device='cuda:0') tensor(5.1914e-08, device='cuda:0') tensor(5.4583e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-14.4212, device='cuda:0') tensor(5.1903e-08, device='cuda:0') tensor(5.4568e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-14.4215, device='cuda:0') tensor(5.1892e-08, device='cuda:0') tensor(5.4552e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303195
tensor(-14.4218, device='cuda:0') tensor(5.1881e-08, device='cuda:0') tensor(5.4537e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000610
Average total loss: 2.303194
tensor(-14.4221, device='cuda:0') tensor(5.1870e-08, device='cuda:0') tensor(5.4522e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4224, device='cuda:0') tensor(5.1859e-08, device='cuda:0') tensor(5.4506e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4226, device='cuda:0') tensor(5.1848e-08, device='cuda:0') tensor(5.4491e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4229, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4476e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4230, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4473e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4230, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4471e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4231, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4468e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4231, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4466e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4232, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4463e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4232, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4461e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4232, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4458e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4233, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4455e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4233, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4453e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1837e-08, device='cuda:0') tensor(5.4450e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000609
Average total loss: 2.303194
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
 Percentile value: -14.42343521118164
Non-zero model percentage: 0.7378365993499756%, Non-zero mask percentage: 0.7378903031349182%

--- Pruning Level [22/24]: ---
conv1.weight         | nonzeros =     294 /    1728             ( 17.01%) | total_pruned =    1434 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
bn1.bias             | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     920 /   36864             (  2.50%) | total_pruned =   35944 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1410 /   36864             (  3.82%) | total_pruned =   35454 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1292 /   36864             (  3.50%) | total_pruned =   35572 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1282 /   36864             (  3.48%) | total_pruned =   35582 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2845 /   73728             (  3.86%) | total_pruned =   70883 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4064 /  147456             (  2.76%) | total_pruned =  143392 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      48 /     128             ( 37.50%) | total_pruned =      80 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     772 /    8192             (  9.42%) | total_pruned =    7420 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     490 /  147456             (  0.33%) | total_pruned =  146966 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     395 /  147456             (  0.27%) | total_pruned =  147061 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      91 /     128             ( 71.09%) | total_pruned =      37 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8347 /  294912             (  2.83%) | total_pruned =  286565 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     185 /     256             ( 72.27%) | total_pruned =      71 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    9516 /  589824             (  1.61%) | total_pruned =  580308 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     833 /   32768             (  2.54%) | total_pruned =   31935 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     301 /  589824             (  0.05%) | total_pruned =  589523 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      72 /     256             ( 28.12%) | total_pruned =     184 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      89 /     256             ( 34.77%) | total_pruned =     167 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     271 /  589824             (  0.05%) | total_pruned =  589553 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8797 / 1179648             (  0.75%) | total_pruned = 1170851 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     316 /     512             ( 61.72%) | total_pruned =     196 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    7708 / 2359296             (  0.33%) | total_pruned = 2351588 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     200 /     512             ( 39.06%) | total_pruned =     312 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     865 /  131072             (  0.66%) | total_pruned =  130207 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     106 /     512             ( 20.70%) | total_pruned =     406 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   11938 / 2359296             (  0.51%) | total_pruned = 2347358 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     210 /     512             ( 41.02%) | total_pruned =     302 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     218 /     512             ( 42.58%) | total_pruned =     294 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   10929 / 2359296             (  0.46%) | total_pruned = 2348367 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     501 /     512             ( 97.85%) | total_pruned =      11 | shape = torch.Size([512])
linear.weight        | nonzeros =    4347 /    5120             ( 84.90%) | total_pruned =     773 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 82481, pruned : 11096281, total: 11178762, Compression rate :     135.53x  ( 99.26% pruned)
Train Epoch: 99/100 Loss: 1.225325 Accuracy: 63.05 64.89 % Best test Accuracy: 63.31%
tensor(-14.4234, device='cuda:0') tensor(5.1833e-08, device='cuda:0') tensor(5.4448e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000608
Average total loss: 2.303193
tensor(-14.4261, device='cuda:0') tensor(5.1582e-08, device='cuda:0') tensor(5.4303e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000606
Average total loss: 2.303191
tensor(-14.4288, device='cuda:0') tensor(5.1343e-08, device='cuda:0') tensor(5.4159e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000605
Average total loss: 2.303190
tensor(-14.4314, device='cuda:0') tensor(5.0989e-08, device='cuda:0') tensor(5.4017e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000603
Average total loss: 2.303188
tensor(-14.4340, device='cuda:0') tensor(5.0661e-08, device='cuda:0') tensor(5.3876e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000601
Average total loss: 2.303186
tensor(-14.4366, device='cuda:0') tensor(5.0359e-08, device='cuda:0') tensor(5.3735e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000600
Average total loss: 2.303185
tensor(-14.4392, device='cuda:0') tensor(5.0082e-08, device='cuda:0') tensor(5.3594e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000598
Average total loss: 2.303183
tensor(-14.4419, device='cuda:0') tensor(4.9826e-08, device='cuda:0') tensor(5.3454e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000597
Average total loss: 2.303182
tensor(-14.4445, device='cuda:0') tensor(4.9589e-08, device='cuda:0') tensor(5.3315e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000595
Average total loss: 2.303180
tensor(-14.4471, device='cuda:0') tensor(4.9360e-08, device='cuda:0') tensor(5.3175e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000594
Average total loss: 2.303179
tensor(-14.4496, device='cuda:0') tensor(4.9023e-08, device='cuda:0') tensor(5.3039e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000592
Average total loss: 2.303177
tensor(-14.4522, device='cuda:0') tensor(4.8710e-08, device='cuda:0') tensor(5.2903e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000591
Average total loss: 2.303176
tensor(-14.4548, device='cuda:0') tensor(4.8429e-08, device='cuda:0') tensor(5.2767e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000590
Average total loss: 2.303175
tensor(-14.4551, device='cuda:0') tensor(4.8421e-08, device='cuda:0') tensor(5.2752e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000590
Average total loss: 2.303174
tensor(-14.4554, device='cuda:0') tensor(4.8412e-08, device='cuda:0') tensor(5.2737e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4556, device='cuda:0') tensor(4.8327e-08, device='cuda:0') tensor(5.2724e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4558, device='cuda:0') tensor(4.8162e-08, device='cuda:0') tensor(5.2711e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4561, device='cuda:0') tensor(4.7998e-08, device='cuda:0') tensor(5.2699e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4563, device='cuda:0') tensor(4.7843e-08, device='cuda:0') tensor(5.2687e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4565, device='cuda:0') tensor(4.7684e-08, device='cuda:0') tensor(5.2674e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4568, device='cuda:0') tensor(4.7540e-08, device='cuda:0') tensor(5.2662e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000589
Average total loss: 2.303174
tensor(-14.4570, device='cuda:0') tensor(4.7386e-08, device='cuda:0') tensor(5.2650e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4572, device='cuda:0') tensor(4.7248e-08, device='cuda:0') tensor(5.2638e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4575, device='cuda:0') tensor(4.7100e-08, device='cuda:0') tensor(5.2625e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4575, device='cuda:0') tensor(4.7106e-08, device='cuda:0') tensor(5.2623e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4576, device='cuda:0') tensor(4.7100e-08, device='cuda:0') tensor(5.2620e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4576, device='cuda:0') tensor(4.7106e-08, device='cuda:0') tensor(5.2618e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4577, device='cuda:0') tensor(4.7100e-08, device='cuda:0') tensor(5.2615e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4577, device='cuda:0') tensor(4.7106e-08, device='cuda:0') tensor(5.2613e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000588
Average total loss: 2.303173
tensor(-14.4578, device='cuda:0') tensor(4.7100e-08, device='cuda:0') tensor(5.2610e-11, device='cuda:0')
