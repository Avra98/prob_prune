Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/24]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/100 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.4999e-06, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302984
Average KL loss: 17.598898
Average total loss: 19.901882
tensor(-3.5300, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.7670e-07, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302735
Average KL loss: 1.928126
Average total loss: 4.230861
tensor(-4.5024, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.0861e-07, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302676
Average KL loss: 0.944790
Average total loss: 3.247466
tensor(-5.0321, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(6.4599e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302653
Average KL loss: 0.599491
Average total loss: 2.902144
tensor(-5.4216, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.3834e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302673
Average KL loss: 0.423229
Average total loss: 2.725902
tensor(-5.7320, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.2280e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302593
Average KL loss: 0.318702
Average total loss: 2.621295
tensor(-5.9907, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.4898e-08, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302605
Average KL loss: 0.250731
Average total loss: 2.553336
tensor(-6.2129, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.9996e-08, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302624
Average KL loss: 0.203578
Average total loss: 2.506202
tensor(-6.4080, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.6487e-08, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302589
Average KL loss: 0.169304
Average total loss: 2.471893
tensor(-6.5820, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.3849e-08, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302606
Average KL loss: 0.143489
Average total loss: 2.446095
tensor(-6.7392, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.1815e-08, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302595
Average KL loss: 0.123466
Average total loss: 2.426061
tensor(-6.8827, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.0264e-08, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302605
Average KL loss: 0.107569
Average total loss: 2.410173
tensor(-7.0149, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(8.9713e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302593
Average KL loss: 0.094708
Average total loss: 2.397301
tensor(-7.1375, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(7.9599e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302571
Average KL loss: 0.084130
Average total loss: 2.386701
tensor(-7.2518, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(7.0981e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302605
Average KL loss: 0.075310
Average total loss: 2.377916
tensor(-7.3590, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(6.3761e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302562
Average KL loss: 0.067862
Average total loss: 2.370424
tensor(-7.4600, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.7590e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302587
Average KL loss: 0.061509
Average total loss: 2.364096
tensor(-7.5556, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.2554e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302595
Average KL loss: 0.056038
Average total loss: 2.358634
tensor(-7.6463, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.7866e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302576
Average KL loss: 0.051290
Average total loss: 2.353865
tensor(-7.7326, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.3890e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302588
Average KL loss: 0.047139
Average total loss: 2.349727
tensor(-7.8151, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.0521e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302591
Average KL loss: 0.043484
Average total loss: 2.346076
tensor(-7.8940, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.7350e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302609
Average KL loss: 0.040248
Average total loss: 2.342857
tensor(-7.9697, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.4627e-09, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302586
Average KL loss: 0.037367
Average total loss: 2.339952
tensor(-8.0425, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.2191e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302590
Average KL loss: 0.034789
Average total loss: 2.337379
tensor(-8.1127, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.0019e-09, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302576
Average KL loss: 0.032472
Average total loss: 2.335048
tensor(-8.1803, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.8036e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302588
Average KL loss: 0.030382
Average total loss: 2.332970
tensor(-8.2458, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.6278e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302579
Average KL loss: 0.028486
Average total loss: 2.331066
tensor(-8.3091, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.4666e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302599
Average KL loss: 0.026764
Average total loss: 2.329363
tensor(-8.3705, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.3197e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.025192
Average total loss: 2.327777
tensor(-8.4302, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.1684e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302584
Average KL loss: 0.023754
Average total loss: 2.326338
tensor(-8.4881, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(2.0626e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302590
Average KL loss: 0.022434
Average total loss: 2.325024
tensor(-8.5445, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.9495e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302583
Average KL loss: 0.021220
Average total loss: 2.323803
tensor(-8.5994, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8454e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302581
Average KL loss: 0.020100
Average total loss: 2.322681
tensor(-8.6529, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.7492e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.019064
Average total loss: 2.321650
tensor(-8.7051, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.6606e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302584
Average KL loss: 0.018105
Average total loss: 2.320689
tensor(-8.7561, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.5777e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302583
Average KL loss: 0.017214
Average total loss: 2.319797
tensor(-8.8060, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.5009e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302576
Average KL loss: 0.016385
Average total loss: 2.318962
tensor(-8.8548, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.4295e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.015613
Average total loss: 2.318198
tensor(-8.9026, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.3507e-09, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.014891
Average total loss: 2.317477
tensor(-8.9494, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.3005e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302579
Average KL loss: 0.014217
Average total loss: 2.316796
tensor(-8.9953, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.2428e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302586
Average KL loss: 0.013585
Average total loss: 2.316171
tensor(-9.0403, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.1875e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.012993
Average total loss: 2.315578
tensor(-9.0845, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.1362e-09, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302586
Average KL loss: 0.012436
Average total loss: 2.315022
tensor(-9.1279, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.0880e-09, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302590
Average KL loss: 0.011912
Average total loss: 2.314502
tensor(-9.1705, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.0425e-09, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302584
Average KL loss: 0.011419
Average total loss: 2.314003
tensor(-9.2124, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.9974e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.010954
Average total loss: 2.313538
tensor(-9.2537, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.5937e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302582
Average KL loss: 0.010515
Average total loss: 2.313096
tensor(-9.2942, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(9.2459e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302584
Average KL loss: 0.010100
Average total loss: 2.312684
tensor(-9.3342, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.8628e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302580
Average KL loss: 0.009707
Average total loss: 2.312287
tensor(-9.3736, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.5099e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.009335
Average total loss: 2.311920
tensor(-9.4123, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(8.1865e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.008982
Average total loss: 2.311567
tensor(-9.4506, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.8793e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.008648
Average total loss: 2.311233
tensor(-9.4883, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.5878e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302584
Average KL loss: 0.008330
Average total loss: 2.310914
tensor(-9.5254, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.3048e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.008028
Average total loss: 2.310613
tensor(-9.5621, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(7.0474e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.007740
Average total loss: 2.310326
tensor(-9.5984, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.7967e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302589
Average KL loss: 0.007467
Average total loss: 2.310056
tensor(-9.6341, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.5664e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302587
Average KL loss: 0.007206
Average total loss: 2.309793
tensor(-9.6695, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.3301e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302590
Average KL loss: 0.006957
Average total loss: 2.309547
tensor(-9.7044, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(6.1130e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302586
Average KL loss: 0.006720
Average total loss: 2.309306
tensor(-9.7389, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.9182e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302590
Average KL loss: 0.006493
Average total loss: 2.309083
tensor(-9.7730, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.7070e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.006277
Average total loss: 2.308861
tensor(-9.8067, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.5185e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.006069
Average total loss: 2.308654
tensor(-9.8401, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.3374e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302588
Average KL loss: 0.005871
Average total loss: 2.308459
tensor(-9.8731, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.1642e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302586
Average KL loss: 0.005682
Average total loss: 2.308267
tensor(-9.9057, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.9982e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.005500
Average total loss: 2.308085
tensor(-9.9380, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.8393e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302586
Average KL loss: 0.005326
Average total loss: 2.307912
tensor(-9.9700, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.6869e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.005159
Average total loss: 2.307744
tensor(-10.0017, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.5407e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.004999
Average total loss: 2.307584
tensor(-10.0331, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.4005e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302580
Average KL loss: 0.004845
Average total loss: 2.307425
tensor(-10.0642, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.2658e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302581
Average KL loss: 0.004698
Average total loss: 2.307279
tensor(-10.0949, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.1365e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302584
Average KL loss: 0.004556
Average total loss: 2.307140
tensor(-10.1255, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.0122e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.004419
Average total loss: 2.307004
tensor(-10.1557, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.8927e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.004288
Average total loss: 2.306873
tensor(-10.1856, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.7778e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.004162
Average total loss: 2.306747
tensor(-10.2154, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.6673e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.004041
Average total loss: 2.306626
tensor(-10.2448, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.5608e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302584
Average KL loss: 0.003924
Average total loss: 2.306508
tensor(-10.2740, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.4585e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302583
Average KL loss: 0.003812
Average total loss: 2.306394
tensor(-10.3030, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.3597e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.003703
Average total loss: 2.306288
tensor(-10.3317, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.2645e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302583
Average KL loss: 0.003599
Average total loss: 2.306182
tensor(-10.3602, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.1730e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302587
Average KL loss: 0.003498
Average total loss: 2.306086
tensor(-10.3885, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(3.0843e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.003401
Average total loss: 2.305986
tensor(-10.4165, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.9990e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.003307
Average total loss: 2.305892
tensor(-10.4443, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.9166e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.003217
Average total loss: 2.305802
tensor(-10.4720, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.8372e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302583
Average KL loss: 0.003129
Average total loss: 2.305713
tensor(-10.4994, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.7604e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.003045
Average total loss: 2.305630
tensor(-10.5266, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.6864e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302584
Average KL loss: 0.002964
Average total loss: 2.305548
tensor(-10.5536, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.6148e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302583
Average KL loss: 0.002885
Average total loss: 2.305468
tensor(-10.5804, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.5457e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302584
Average KL loss: 0.002809
Average total loss: 2.305393
tensor(-10.6070, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.4793e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302590
Average KL loss: 0.002735
Average total loss: 2.305325
tensor(-10.6335, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.4141e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302590
Average KL loss: 0.002664
Average total loss: 2.305254
tensor(-10.6597, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.3515e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.002595
Average total loss: 2.305180
tensor(-10.6858, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.2910e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.002529
Average total loss: 2.305114
tensor(-10.7117, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.2325e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302584
Average KL loss: 0.002465
Average total loss: 2.305049
tensor(-10.7374, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.1760e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302583
Average KL loss: 0.002402
Average total loss: 2.304985
tensor(-10.7629, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.1211e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302586
Average KL loss: 0.002342
Average total loss: 2.304928
tensor(-10.7883, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.0680e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302584
Average KL loss: 0.002284
Average total loss: 2.304868
tensor(-10.8134, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(2.0577e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302584
Average KL loss: 0.002227
Average total loss: 2.304811
tensor(-10.8385, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.9667e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302588
Average KL loss: 0.002172
Average total loss: 2.304760
tensor(-10.8633, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.9183e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302589
Average KL loss: 0.002119
Average total loss: 2.304708
tensor(-10.8880, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.8714e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304653
tensor(-10.9125, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.8261e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302581
Average KL loss: 0.002017
Average total loss: 2.304598
tensor(-10.9369, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7827e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302590
Average KL loss: 0.001969
Average total loss: 2.304559
tensor(-10.9611, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7396e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.001922
Average total loss: 2.304507
tensor(-10.9852, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6982e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302586
Average KL loss: 0.001876
Average total loss: 2.304462
tensor(-11.0091, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6580e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302589
Average KL loss: 0.001832
Average total loss: 2.304421
tensor(-11.0329, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.6190e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.001789
Average total loss: 2.304374
tensor(-11.0565, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5812e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.001747
Average total loss: 2.304332
tensor(-11.0799, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5446e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.001707
Average total loss: 2.304292
tensor(-11.1032, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.5090e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-11.1264, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4744e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.001630
Average total loss: 2.304215
tensor(-11.1494, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4409e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.001593
Average total loss: 2.304178
tensor(-11.1722, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.4083e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.001557
Average total loss: 2.304142
tensor(-11.1950, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3766e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-11.2175, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3459e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.001488
Average total loss: 2.304073
tensor(-11.2400, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.3162e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.001455
Average total loss: 2.304040
tensor(-11.2623, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2870e-10, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302582
Average KL loss: 0.001423
Average total loss: 2.304005
tensor(-11.2844, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2588e-10, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302582
Average KL loss: 0.001392
Average total loss: 2.303975
tensor(-11.3064, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2314e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.001362
Average total loss: 2.303947
tensor(-11.3283, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.2047e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.001332
Average total loss: 2.303917
tensor(-11.3501, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1788e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.001304
Average total loss: 2.303889
tensor(-11.3717, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1536e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302586
Average KL loss: 0.001276
Average total loss: 2.303862
tensor(-11.3931, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1291e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.001249
Average total loss: 2.303834
tensor(-11.4145, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.1052e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.001223
Average total loss: 2.303808
tensor(-11.4356, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0821e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302588
Average KL loss: 0.001197
Average total loss: 2.303785
tensor(-11.4567, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0595e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302584
Average KL loss: 0.001172
Average total loss: 2.303757
tensor(-11.4776, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.0376e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.001148
Average total loss: 2.303733
tensor(-11.4984, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.0162e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.001125
Average total loss: 2.303710
tensor(-11.5191, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.9540e-11, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.001102
Average total loss: 2.303686
tensor(-11.5396, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.7516e-11, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.001079
Average total loss: 2.303664
tensor(-11.5600, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.5546e-11, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.001058
Average total loss: 2.303642
tensor(-11.5803, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.3628e-11, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.001036
Average total loss: 2.303621
tensor(-11.6004, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(9.1760e-11, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.001016
Average total loss: 2.303601
tensor(-11.6204, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.9942e-11, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302580
Average KL loss: 0.000996
Average total loss: 2.303576
tensor(-11.6403, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.8174e-11, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302589
Average KL loss: 0.000976
Average total loss: 2.303565
tensor(-11.6601, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.6446e-11, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.000957
Average total loss: 2.303542
tensor(-11.6797, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.4765e-11, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.000939
Average total loss: 2.303523
tensor(-11.6992, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.3128e-11, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.000921
Average total loss: 2.303505
tensor(-11.7185, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1533e-11, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.000911
Average total loss: 2.303496
tensor(-11.7205, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1376e-11, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302584
Average KL loss: 0.000909
Average total loss: 2.303493
tensor(-11.7224, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1217e-11, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.000907
Average total loss: 2.303492
tensor(-11.7244, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.1057e-11, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.000905
Average total loss: 2.303490
tensor(-11.7264, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0895e-11, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.000903
Average total loss: 2.303488
tensor(-11.7284, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0732e-11, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.000902
Average total loss: 2.303486
tensor(-11.7304, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0568e-11, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.000900
Average total loss: 2.303485
tensor(-11.7325, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0402e-11, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.000898
Average total loss: 2.303483
tensor(-11.7346, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0236e-11, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.000896
Average total loss: 2.303481
tensor(-11.7367, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(8.0067e-11, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.000894
Average total loss: 2.303479
tensor(-11.7388, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9899e-11, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302586
Average KL loss: 0.000892
Average total loss: 2.303479
tensor(-11.7409, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9727e-11, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.000890
Average total loss: 2.303475
tensor(-11.7431, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9556e-11, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302584
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7433, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9538e-11, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7435, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9519e-11, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7438, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9500e-11, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.000889
Average total loss: 2.303474
tensor(-11.7440, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9482e-11, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7442, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9463e-11, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7445, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9445e-11, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7447, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9426e-11, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7449, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9407e-11, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.000888
Average total loss: 2.303473
tensor(-11.7452, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9389e-11, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7454, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9370e-11, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7456, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9352e-11, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7457, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9348e-11, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7457, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9344e-11, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7458, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9341e-11, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7458, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9337e-11, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7459, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9333e-11, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7459, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9329e-11, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7460, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9326e-11, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7460, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9322e-11, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7461, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9318e-11, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7461, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9315e-11, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302582
Average KL loss: 0.000887
Average total loss: 2.303468
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.000887
Average total loss: 2.303472
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
 Percentile value: -11.746682167053223
Non-zero model percentage: 80.0%, Non-zero mask percentage: 80.0%

--- Pruning Level [1/24]: ---
conv1.weight         | nonzeros =    1487 /    1728             ( 86.05%) | total_pruned =     241 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
bn1.bias             | nonzeros =      55 /      64             ( 85.94%) | total_pruned =       9 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   31779 /   36864             ( 86.21%) | total_pruned =    5085 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      35 /      64             ( 54.69%) | total_pruned =      29 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   25960 /   36864             ( 70.42%) | total_pruned =   10904 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   25791 /   36864             ( 69.96%) | total_pruned =   11073 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      42 /      64             ( 65.62%) | total_pruned =      22 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   24145 /   36864             ( 65.50%) | total_pruned =   12719 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   42231 /   73728             ( 57.28%) | total_pruned =   31497 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   90646 /  147456             ( 61.47%) | total_pruned =   56810 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    4782 /    8192             ( 58.37%) | total_pruned =    3410 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  110291 /  147456             ( 74.80%) | total_pruned =   37165 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  117216 /  147456             ( 79.49%) | total_pruned =   30240 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  182728 /  294912             ( 61.96%) | total_pruned =  112184 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     167 /     256             ( 65.23%) | total_pruned =      89 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     174 /     256             ( 67.97%) | total_pruned =      82 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  452856 /  589824             ( 76.78%) | total_pruned =  136968 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   24188 /   32768             ( 73.82%) | total_pruned =    8580 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     135 /     256             ( 52.73%) | total_pruned =     121 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     154 /     256             ( 60.16%) | total_pruned =     102 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  517295 /  589824             ( 87.70%) | total_pruned =   72529 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     141 /     256             ( 55.08%) | total_pruned =     115 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  535703 /  589824             ( 90.82%) | total_pruned =   54121 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  983632 / 1179648             ( 83.38%) | total_pruned =  196016 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     280 /     512             ( 54.69%) | total_pruned =     232 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     343 /     512             ( 66.99%) | total_pruned =     169 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1958892 / 2359296             ( 83.03%) | total_pruned =  400404 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     304 /     512             ( 59.38%) | total_pruned =     208 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  106987 /  131072             ( 81.62%) | total_pruned =   24085 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     272 /     512             ( 53.12%) | total_pruned =     240 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1822284 / 2359296             ( 77.24%) | total_pruned =  537012 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     294 /     512             ( 57.42%) | total_pruned =     218 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1868368 / 2359296             ( 79.19%) | total_pruned =  490928 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8943010, pruned : 2235752, total: 11178762, Compression rate :       1.25x  ( 20.00% pruned)
Train Epoch: 63/100 Loss: 0.026984 Accuracy: 88.40 100.00 % Best test Accuracy: 88.43%
tensor(-11.7462, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(7.9311e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000870
Average total loss: 2.303455
tensor(-11.7842, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(7.6299e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000838
Average total loss: 2.303422
tensor(-11.8208, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(7.3534e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000808
Average total loss: 2.303393
tensor(-11.8562, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(7.0970e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000780
Average total loss: 2.303365
tensor(-11.8904, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(6.8582e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000754
Average total loss: 2.303339
tensor(-11.9234, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.6350e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000730
Average total loss: 2.303315
tensor(-11.9554, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.4261e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000708
Average total loss: 2.303292
tensor(-11.9863, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(6.2299e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000686
Average total loss: 2.303271
tensor(-12.0164, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(6.0454e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000666
Average total loss: 2.303251
tensor(-12.0455, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.8715e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000647
Average total loss: 2.303232
tensor(-12.0739, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.7074e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000629
Average total loss: 2.303214
tensor(-12.1014, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.5523e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000613
Average total loss: 2.303197
tensor(-12.1282, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.4053e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000597
Average total loss: 2.303181
tensor(-12.1543, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(5.2660e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000581
Average total loss: 2.303166
tensor(-12.1798, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(5.1336e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000567
Average total loss: 2.303152
tensor(-12.2046, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(5.0078e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000553
Average total loss: 2.303138
tensor(-12.2288, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.8880e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000540
Average total loss: 2.303125
tensor(-12.2524, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.7738e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000528
Average total loss: 2.303113
tensor(-12.2755, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.6648e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000516
Average total loss: 2.303101
tensor(-12.2981, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(4.5607e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000504
Average total loss: 2.303089
tensor(-12.3202, device='cuda:0') tensor(9.5788e-05, device='cuda:0') tensor(4.4611e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000493
Average total loss: 2.303078
tensor(-12.3418, device='cuda:0') tensor(9.0407e-05, device='cuda:0') tensor(4.3658e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000483
Average total loss: 2.303068
tensor(-12.3629, device='cuda:0') tensor(8.5509e-05, device='cuda:0') tensor(4.2744e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000477
Average total loss: 2.303062
tensor(-12.3650, device='cuda:0') tensor(8.5049e-05, device='cuda:0') tensor(4.2655e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000476
Average total loss: 2.303061
tensor(-12.3671, device='cuda:0') tensor(8.4579e-05, device='cuda:0') tensor(4.2567e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000475
Average total loss: 2.303060
tensor(-12.3691, device='cuda:0') tensor(8.4114e-05, device='cuda:0') tensor(4.2480e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000474
Average total loss: 2.303059
tensor(-12.3712, device='cuda:0') tensor(8.3654e-05, device='cuda:0') tensor(4.2392e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000473
Average total loss: 2.303058
tensor(-12.3732, device='cuda:0') tensor(8.3200e-05, device='cuda:0') tensor(4.2305e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000472
Average total loss: 2.303057
tensor(-12.3753, device='cuda:0') tensor(8.2750e-05, device='cuda:0') tensor(4.2218e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000471
Average total loss: 2.303056
tensor(-12.3773, device='cuda:0') tensor(8.2306e-05, device='cuda:0') tensor(4.2131e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000471
Average total loss: 2.303055
tensor(-12.3794, device='cuda:0') tensor(8.1866e-05, device='cuda:0') tensor(4.2045e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000470
Average total loss: 2.303054
tensor(-12.3815, device='cuda:0') tensor(8.1431e-05, device='cuda:0') tensor(4.1958e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000469
Average total loss: 2.303053
tensor(-12.3835, device='cuda:0') tensor(8.1001e-05, device='cuda:0') tensor(4.1872e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000468
Average total loss: 2.303052
tensor(-12.3856, device='cuda:0') tensor(8.0575e-05, device='cuda:0') tensor(4.1786e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3858, device='cuda:0') tensor(8.0526e-05, device='cuda:0') tensor(4.1778e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3859, device='cuda:0') tensor(8.0477e-05, device='cuda:0') tensor(4.1771e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3861, device='cuda:0') tensor(8.0428e-05, device='cuda:0') tensor(4.1763e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3863, device='cuda:0') tensor(8.0379e-05, device='cuda:0') tensor(4.1755e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303052
tensor(-12.3865, device='cuda:0') tensor(8.0330e-05, device='cuda:0') tensor(4.1747e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303051
tensor(-12.3867, device='cuda:0') tensor(8.0281e-05, device='cuda:0') tensor(4.1739e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000467
Average total loss: 2.303051
tensor(-12.3869, device='cuda:0') tensor(8.0232e-05, device='cuda:0') tensor(4.1732e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3871, device='cuda:0') tensor(8.0183e-05, device='cuda:0') tensor(4.1724e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3873, device='cuda:0') tensor(8.0134e-05, device='cuda:0') tensor(4.1716e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3874, device='cuda:0') tensor(8.0085e-05, device='cuda:0') tensor(4.1708e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0036e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0021e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(8.0006e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9991e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9976e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9962e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9947e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9932e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9917e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9902e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9887e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.000466
Average total loss: 2.303051
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
 Percentile value: -12.387809753417969
Non-zero model percentage: 64.0%, Non-zero mask percentage: 64.0%

--- Pruning Level [2/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  922280 / 1179648             ( 78.18%) | total_pruned =  257368 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     280 /     512             ( 54.69%) | total_pruned =     232 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     343 /     512             ( 66.99%) | total_pruned =     169 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1958892 / 2359296             ( 83.03%) | total_pruned =  400404 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     304 /     512             ( 59.38%) | total_pruned =     208 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  106987 /  131072             ( 81.62%) | total_pruned =   24085 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     272 /     512             ( 53.12%) | total_pruned =     240 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1803016 / 2359296             ( 76.42%) | total_pruned =  556280 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     294 /     512             ( 57.42%) | total_pruned =     218 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1797721 / 2359296             ( 76.20%) | total_pruned =  561575 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 7154408, pruned : 4024354, total: 11178762, Compression rate :       1.56x  ( 36.00% pruned)
Train Epoch: 57/100 Loss: 0.015501 Accuracy: 86.01 100.00 % Best test Accuracy: 86.21%
tensor(-12.3876, device='cuda:0') tensor(7.9872e-05, device='cuda:0') tensor(4.1700e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000462
Average total loss: 2.303046
tensor(-12.4078, device='cuda:0') tensor(7.5702e-05, device='cuda:0') tensor(4.0866e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000452
Average total loss: 2.303037
tensor(-12.4276, device='cuda:0') tensor(7.1848e-05, device='cuda:0') tensor(4.0064e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000444
Average total loss: 2.303029
tensor(-12.4471, device='cuda:0') tensor(6.8287e-05, device='cuda:0') tensor(3.9293e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000435
Average total loss: 2.303020
tensor(-12.4661, device='cuda:0') tensor(6.4992e-05, device='cuda:0') tensor(3.8551e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000427
Average total loss: 2.303012
tensor(-12.4849, device='cuda:0') tensor(6.1935e-05, device='cuda:0') tensor(3.7836e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000419
Average total loss: 2.303004
tensor(-12.5032, device='cuda:0') tensor(5.9095e-05, device='cuda:0') tensor(3.7148e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000412
Average total loss: 2.302996
tensor(-12.5213, device='cuda:0') tensor(5.6450e-05, device='cuda:0') tensor(3.6484e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000404
Average total loss: 2.302989
tensor(-12.5390, device='cuda:0') tensor(5.3981e-05, device='cuda:0') tensor(3.5843e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000397
Average total loss: 2.302982
tensor(-12.5564, device='cuda:0') tensor(5.1675e-05, device='cuda:0') tensor(3.5225e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000390
Average total loss: 2.302975
tensor(-12.5735, device='cuda:0') tensor(4.9516e-05, device='cuda:0') tensor(3.4627e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000384
Average total loss: 2.302969
tensor(-12.5903, device='cuda:0') tensor(4.7492e-05, device='cuda:0') tensor(3.4050e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000378
Average total loss: 2.302962
tensor(-12.6068, device='cuda:0') tensor(4.5592e-05, device='cuda:0') tensor(3.3491e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000374
Average total loss: 2.302959
tensor(-12.6085, device='cuda:0') tensor(4.5410e-05, device='cuda:0') tensor(3.3437e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000373
Average total loss: 2.302958
tensor(-12.6101, device='cuda:0') tensor(4.5230e-05, device='cuda:0') tensor(3.3382e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000373
Average total loss: 2.302958
tensor(-12.6117, device='cuda:0') tensor(4.5052e-05, device='cuda:0') tensor(3.3327e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000372
Average total loss: 2.302957
tensor(-12.6134, device='cuda:0') tensor(4.4876e-05, device='cuda:0') tensor(3.3273e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000372
Average total loss: 2.302957
tensor(-12.6150, device='cuda:0') tensor(4.4702e-05, device='cuda:0') tensor(3.3218e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000371
Average total loss: 2.302956
tensor(-12.6166, device='cuda:0') tensor(4.4529e-05, device='cuda:0') tensor(3.3164e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000370
Average total loss: 2.302955
tensor(-12.6183, device='cuda:0') tensor(4.4358e-05, device='cuda:0') tensor(3.3110e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000370
Average total loss: 2.302955
tensor(-12.6199, device='cuda:0') tensor(4.4188e-05, device='cuda:0') tensor(3.3056e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000369
Average total loss: 2.302954
tensor(-12.6215, device='cuda:0') tensor(4.4017e-05, device='cuda:0') tensor(3.3002e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000369
Average total loss: 2.302954
tensor(-12.6231, device='cuda:0') tensor(4.3837e-05, device='cuda:0') tensor(3.2950e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6247, device='cuda:0') tensor(4.3660e-05, device='cuda:0') tensor(3.2897e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6249, device='cuda:0') tensor(4.3637e-05, device='cuda:0') tensor(3.2893e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6250, device='cuda:0') tensor(4.3613e-05, device='cuda:0') tensor(3.2888e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6251, device='cuda:0') tensor(4.3590e-05, device='cuda:0') tensor(3.2884e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302953
tensor(-12.6253, device='cuda:0') tensor(4.3567e-05, device='cuda:0') tensor(3.2879e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000368
Average total loss: 2.302952
tensor(-12.6254, device='cuda:0') tensor(4.3544e-05, device='cuda:0') tensor(3.2874e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6256, device='cuda:0') tensor(4.3521e-05, device='cuda:0') tensor(3.2870e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6257, device='cuda:0') tensor(4.3498e-05, device='cuda:0') tensor(3.2865e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6258, device='cuda:0') tensor(4.3475e-05, device='cuda:0') tensor(3.2861e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6260, device='cuda:0') tensor(4.3452e-05, device='cuda:0') tensor(3.2856e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6261, device='cuda:0') tensor(4.3429e-05, device='cuda:0') tensor(3.2851e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3406e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3400e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3395e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3390e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3385e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3379e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3374e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3369e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3364e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3358e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3353e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000367
Average total loss: 2.302952
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
 Percentile value: -12.6264009475708
Non-zero model percentage: 51.200008392333984%, Non-zero mask percentage: 51.200008392333984%

--- Pruning Level [3/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  185125 / 1179648             ( 15.69%) | total_pruned =  994523 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1265710 / 2359296             ( 53.65%) | total_pruned = 1093586 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     304 /     512             ( 59.38%) | total_pruned =     208 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  106987 /  131072             ( 81.62%) | total_pruned =   24085 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     272 /     512             ( 53.12%) | total_pruned =     240 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1803016 / 2359296             ( 76.42%) | total_pruned =  556280 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     294 /     512             ( 57.42%) | total_pruned =     218 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1797721 / 2359296             ( 76.20%) | total_pruned =  561575 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 5723527, pruned : 5455235, total: 11178762, Compression rate :       1.95x  ( 48.80% pruned)
Train Epoch: 71/100 Loss: 0.019458 Accuracy: 86.21 100.00 % Best test Accuracy: 86.37%
tensor(-12.6263, device='cuda:0') tensor(4.3348e-05, device='cuda:0') tensor(3.2847e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000364
Average total loss: 2.302949
tensor(-12.6422, device='cuda:0') tensor(4.1691e-05, device='cuda:0') tensor(3.2327e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000359
Average total loss: 2.302943
tensor(-12.6579, device='cuda:0') tensor(4.0126e-05, device='cuda:0') tensor(3.1823e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000353
Average total loss: 2.302938
tensor(-12.6734, device='cuda:0') tensor(3.8649e-05, device='cuda:0') tensor(3.1334e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000348
Average total loss: 2.302933
tensor(-12.6886, device='cuda:0') tensor(3.7252e-05, device='cuda:0') tensor(3.0861e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000342
Average total loss: 2.302927
tensor(-12.7036, device='cuda:0') tensor(3.5931e-05, device='cuda:0') tensor(3.0401e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000337
Average total loss: 2.302922
tensor(-12.7184, device='cuda:0') tensor(3.4680e-05, device='cuda:0') tensor(2.9955e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000332
Average total loss: 2.302917
tensor(-12.7330, device='cuda:0') tensor(3.3494e-05, device='cuda:0') tensor(2.9522e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-12.7473, device='cuda:0') tensor(3.2369e-05, device='cuda:0') tensor(2.9101e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000323
Average total loss: 2.302908
tensor(-12.7615, device='cuda:0') tensor(3.1301e-05, device='cuda:0') tensor(2.8692e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000319
Average total loss: 2.302903
tensor(-12.7755, device='cuda:0') tensor(3.0285e-05, device='cuda:0') tensor(2.8294e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000314
Average total loss: 2.302899
tensor(-12.7892, device='cuda:0') tensor(2.9318e-05, device='cuda:0') tensor(2.7907e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000310
Average total loss: 2.302895
tensor(-12.8028, device='cuda:0') tensor(2.8398e-05, device='cuda:0') tensor(2.7531e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000308
Average total loss: 2.302892
tensor(-12.8042, device='cuda:0') tensor(2.8311e-05, device='cuda:0') tensor(2.7494e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000307
Average total loss: 2.302892
tensor(-12.8055, device='cuda:0') tensor(2.8225e-05, device='cuda:0') tensor(2.7456e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000307
Average total loss: 2.302892
tensor(-12.8069, device='cuda:0') tensor(2.8140e-05, device='cuda:0') tensor(2.7419e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000306
Average total loss: 2.302891
tensor(-12.8082, device='cuda:0') tensor(2.8056e-05, device='cuda:0') tensor(2.7382e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000306
Average total loss: 2.302891
tensor(-12.8096, device='cuda:0') tensor(2.7972e-05, device='cuda:0') tensor(2.7345e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8109, device='cuda:0') tensor(2.7889e-05, device='cuda:0') tensor(2.7308e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8123, device='cuda:0') tensor(2.7807e-05, device='cuda:0') tensor(2.7271e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-12.8136, device='cuda:0') tensor(2.7719e-05, device='cuda:0') tensor(2.7235e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000304
Average total loss: 2.302889
tensor(-12.8149, device='cuda:0') tensor(2.7628e-05, device='cuda:0') tensor(2.7199e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000304
Average total loss: 2.302889
tensor(-12.8162, device='cuda:0') tensor(2.7538e-05, device='cuda:0') tensor(2.7163e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8175, device='cuda:0') tensor(2.7449e-05, device='cuda:0') tensor(2.7128e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8177, device='cuda:0') tensor(2.7442e-05, device='cuda:0') tensor(2.7124e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8178, device='cuda:0') tensor(2.7436e-05, device='cuda:0') tensor(2.7120e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8180, device='cuda:0') tensor(2.7430e-05, device='cuda:0') tensor(2.7117e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8181, device='cuda:0') tensor(2.7423e-05, device='cuda:0') tensor(2.7113e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8182, device='cuda:0') tensor(2.7417e-05, device='cuda:0') tensor(2.7109e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8184, device='cuda:0') tensor(2.7410e-05, device='cuda:0') tensor(2.7105e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8185, device='cuda:0') tensor(2.7404e-05, device='cuda:0') tensor(2.7101e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8187, device='cuda:0') tensor(2.7398e-05, device='cuda:0') tensor(2.7098e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8188, device='cuda:0') tensor(2.7391e-05, device='cuda:0') tensor(2.7094e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8189, device='cuda:0') tensor(2.7385e-05, device='cuda:0') tensor(2.7088e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000303
Average total loss: 2.302888
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
 Percentile value: -12.819193840026855
Non-zero model percentage: 40.96000671386719%, Non-zero mask percentage: 40.96000671386719%

--- Pruning Level [4/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  185125 / 1179648             ( 15.69%) | total_pruned =  994523 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  381190 / 2359296             ( 16.16%) | total_pruned = 1978106 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     295 /     512             ( 57.62%) | total_pruned =     217 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22384 /  131072             ( 17.08%) | total_pruned =  108688 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 1628524 / 2359296             ( 69.03%) | total_pruned =  730772 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     294 /     512             ( 57.42%) | total_pruned =     218 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1797721 / 2359296             ( 76.20%) | total_pruned =  561575 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 4578822, pruned : 6599940, total: 11178762, Compression rate :       2.44x  ( 59.04% pruned)
Train Epoch: 70/100 Loss: 0.027472 Accuracy: 85.12 100.00 % Best test Accuracy: 85.49%
tensor(-12.8191, device='cuda:0') tensor(2.7379e-05, device='cuda:0') tensor(2.7086e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000301
Average total loss: 2.302886
tensor(-12.8323, device='cuda:0') tensor(2.6549e-05, device='cuda:0') tensor(2.6732e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000297
Average total loss: 2.302882
tensor(-12.8453, device='cuda:0') tensor(2.5756e-05, device='cuda:0') tensor(2.6386e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000293
Average total loss: 2.302878
tensor(-12.8581, device='cuda:0') tensor(2.4998e-05, device='cuda:0') tensor(2.6049e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000289
Average total loss: 2.302874
tensor(-12.8708, device='cuda:0') tensor(2.4273e-05, device='cuda:0') tensor(2.5721e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000286
Average total loss: 2.302871
tensor(-12.8833, device='cuda:0') tensor(2.3580e-05, device='cuda:0') tensor(2.5401e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000282
Average total loss: 2.302867
tensor(-12.8957, device='cuda:0') tensor(2.2916e-05, device='cuda:0') tensor(2.5089e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000279
Average total loss: 2.302864
tensor(-12.9079, device='cuda:0') tensor(2.2280e-05, device='cuda:0') tensor(2.4784e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-12.9200, device='cuda:0') tensor(2.1671e-05, device='cuda:0') tensor(2.4487e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000272
Average total loss: 2.302857
tensor(-12.9319, device='cuda:0') tensor(2.1087e-05, device='cuda:0') tensor(2.4197e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000269
Average total loss: 2.302854
tensor(-12.9437, device='cuda:0') tensor(2.0526e-05, device='cuda:0') tensor(2.3913e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000266
Average total loss: 2.302851
tensor(-12.9553, device='cuda:0') tensor(1.9987e-05, device='cuda:0') tensor(2.3636e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000263
Average total loss: 2.302848
tensor(-12.9668, device='cuda:0') tensor(1.9469e-05, device='cuda:0') tensor(2.3366e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000261
Average total loss: 2.302846
tensor(-12.9680, device='cuda:0') tensor(1.9415e-05, device='cuda:0') tensor(2.3340e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000261
Average total loss: 2.302846
tensor(-12.9691, device='cuda:0') tensor(1.9361e-05, device='cuda:0') tensor(2.3313e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9702, device='cuda:0') tensor(1.9307e-05, device='cuda:0') tensor(2.3287e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9713, device='cuda:0') tensor(1.9255e-05, device='cuda:0') tensor(2.3261e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302845
tensor(-12.9724, device='cuda:0') tensor(1.9202e-05, device='cuda:0') tensor(2.3235e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000260
Average total loss: 2.302844
tensor(-12.9736, device='cuda:0') tensor(1.9150e-05, device='cuda:0') tensor(2.3209e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9747, device='cuda:0') tensor(1.9091e-05, device='cuda:0') tensor(2.3183e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9758, device='cuda:0') tensor(1.9040e-05, device='cuda:0') tensor(2.3157e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-12.9769, device='cuda:0') tensor(1.8989e-05, device='cuda:0') tensor(2.3131e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9781, device='cuda:0') tensor(1.8939e-05, device='cuda:0') tensor(2.3105e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9792, device='cuda:0') tensor(1.8889e-05, device='cuda:0') tensor(2.3079e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9793, device='cuda:0') tensor(1.8881e-05, device='cuda:0') tensor(2.3077e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9794, device='cuda:0') tensor(1.8873e-05, device='cuda:0') tensor(2.3075e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9795, device='cuda:0') tensor(1.8865e-05, device='cuda:0') tensor(2.3073e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9795, device='cuda:0') tensor(1.8857e-05, device='cuda:0') tensor(2.3071e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9796, device='cuda:0') tensor(1.8849e-05, device='cuda:0') tensor(2.3068e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9797, device='cuda:0') tensor(1.8841e-05, device='cuda:0') tensor(2.3066e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9798, device='cuda:0') tensor(1.8833e-05, device='cuda:0') tensor(2.3064e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9799, device='cuda:0') tensor(1.8825e-05, device='cuda:0') tensor(2.3062e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9800, device='cuda:0') tensor(1.8818e-05, device='cuda:0') tensor(2.3060e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9801, device='cuda:0') tensor(1.8810e-05, device='cuda:0') tensor(2.3058e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000258
Average total loss: 2.302843
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
 Percentile value: -12.980291366577148
Non-zero model percentage: 32.76801300048828%, Non-zero mask percentage: 32.76801300048828%

--- Pruning Level [5/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  185125 / 1179648             ( 15.69%) | total_pruned =  994523 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  381190 / 2359296             ( 16.16%) | total_pruned = 1978106 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     295 /     512             ( 57.62%) | total_pruned =     217 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22384 /  131072             ( 17.08%) | total_pruned =  108688 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  712760 / 2359296             ( 30.21%) | total_pruned = 1646536 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     294 /     512             ( 57.42%) | total_pruned =     218 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1797721 / 2359296             ( 76.20%) | total_pruned =  561575 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 3663058, pruned : 7515704, total: 11178762, Compression rate :       3.05x  ( 67.23% pruned)
Train Epoch: 60/100 Loss: 0.021227 Accuracy: 84.00 100.00 % Best test Accuracy: 84.45%
tensor(-12.9802, device='cuda:0') tensor(1.8802e-05, device='cuda:0') tensor(2.3055e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000256
Average total loss: 2.302841
tensor(-12.9914, device='cuda:0') tensor(1.8329e-05, device='cuda:0') tensor(2.2798e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000253
Average total loss: 2.302838
tensor(-13.0025, device='cuda:0') tensor(1.7873e-05, device='cuda:0') tensor(2.2546e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000251
Average total loss: 2.302836
tensor(-13.0135, device='cuda:0') tensor(1.7434e-05, device='cuda:0') tensor(2.2300e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000248
Average total loss: 2.302833
tensor(-13.0244, device='cuda:0') tensor(1.7011e-05, device='cuda:0') tensor(2.2059e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000245
Average total loss: 2.302830
tensor(-13.0351, device='cuda:0') tensor(1.6603e-05, device='cuda:0') tensor(2.1823e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000243
Average total loss: 2.302828
tensor(-13.0458, device='cuda:0') tensor(1.6211e-05, device='cuda:0') tensor(2.1592e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000240
Average total loss: 2.302825
tensor(-13.0563, device='cuda:0') tensor(1.5831e-05, device='cuda:0') tensor(2.1366e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-13.0667, device='cuda:0') tensor(1.5465e-05, device='cuda:0') tensor(2.1145e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000235
Average total loss: 2.302820
tensor(-13.0770, device='cuda:0') tensor(1.5112e-05, device='cuda:0') tensor(2.0928e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000233
Average total loss: 2.302818
tensor(-13.0872, device='cuda:0') tensor(1.4771e-05, device='cuda:0') tensor(2.0716e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000230
Average total loss: 2.302815
tensor(-13.0973, device='cuda:0') tensor(1.4441e-05, device='cuda:0') tensor(2.0508e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000228
Average total loss: 2.302813
tensor(-13.1073, device='cuda:0') tensor(1.4123e-05, device='cuda:0') tensor(2.0304e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000227
Average total loss: 2.302812
tensor(-13.1083, device='cuda:0') tensor(1.4090e-05, device='cuda:0') tensor(2.0284e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000227
Average total loss: 2.302812
tensor(-13.1093, device='cuda:0') tensor(1.4057e-05, device='cuda:0') tensor(2.0264e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1102, device='cuda:0') tensor(1.4025e-05, device='cuda:0') tensor(2.0244e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1112, device='cuda:0') tensor(1.3993e-05, device='cuda:0') tensor(2.0224e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1122, device='cuda:0') tensor(1.3961e-05, device='cuda:0') tensor(2.0204e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302811
tensor(-13.1132, device='cuda:0') tensor(1.3930e-05, device='cuda:0') tensor(2.0184e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302810
tensor(-13.1142, device='cuda:0') tensor(1.3899e-05, device='cuda:0') tensor(2.0165e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1152, device='cuda:0') tensor(1.3868e-05, device='cuda:0') tensor(2.0145e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1161, device='cuda:0') tensor(1.3837e-05, device='cuda:0') tensor(2.0125e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1171, device='cuda:0') tensor(1.3807e-05, device='cuda:0') tensor(2.0105e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302810
tensor(-13.1181, device='cuda:0') tensor(1.3777e-05, device='cuda:0') tensor(2.0086e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302809
tensor(-13.1182, device='cuda:0') tensor(1.3773e-05, device='cuda:0') tensor(2.0084e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000225
Average total loss: 2.302809
tensor(-13.1183, device='cuda:0') tensor(1.3769e-05, device='cuda:0') tensor(2.0082e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1184, device='cuda:0') tensor(1.3765e-05, device='cuda:0') tensor(2.0080e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1185, device='cuda:0') tensor(1.3761e-05, device='cuda:0') tensor(2.0078e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1186, device='cuda:0') tensor(1.3757e-05, device='cuda:0') tensor(2.0076e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1187, device='cuda:0') tensor(1.3753e-05, device='cuda:0') tensor(2.0074e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1188, device='cuda:0') tensor(1.3749e-05, device='cuda:0') tensor(2.0072e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1188, device='cuda:0') tensor(1.3745e-05, device='cuda:0') tensor(2.0071e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1189, device='cuda:0') tensor(1.3741e-05, device='cuda:0') tensor(2.0069e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1190, device='cuda:0') tensor(1.3737e-05, device='cuda:0') tensor(2.0067e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000224
Average total loss: 2.302809
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
 Percentile value: -13.1192045211792
Non-zero model percentage: 26.214414596557617%, Non-zero mask percentage: 26.214414596557617%

--- Pruning Level [6/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  185125 / 1179648             ( 15.69%) | total_pruned =  994523 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  381190 / 2359296             ( 16.16%) | total_pruned = 1978106 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     295 /     512             ( 57.62%) | total_pruned =     217 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22384 /  131072             ( 17.08%) | total_pruned =  108688 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  529621 / 2359296             ( 22.45%) | total_pruned = 1829675 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     288 /     512             ( 56.25%) | total_pruned =     224 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1248767 / 2359296             ( 52.93%) | total_pruned = 1110529 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 2930447, pruned : 8248315, total: 11178762, Compression rate :       3.81x  ( 73.79% pruned)
Train Epoch: 49/100 Loss: 0.028622 Accuracy: 82.36 99.99 % Best test Accuracy: 83.58%
tensor(-13.1191, device='cuda:0') tensor(1.3733e-05, device='cuda:0') tensor(2.0065e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000223
Average total loss: 2.302808
tensor(-13.1289, device='cuda:0') tensor(1.3438e-05, device='cuda:0') tensor(1.9870e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000221
Average total loss: 2.302806
tensor(-13.1386, device='cuda:0') tensor(1.3152e-05, device='cuda:0') tensor(1.9678e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000219
Average total loss: 2.302804
tensor(-13.1482, device='cuda:0') tensor(1.2875e-05, device='cuda:0') tensor(1.9490e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000217
Average total loss: 2.302802
tensor(-13.1577, device='cuda:0') tensor(1.2606e-05, device='cuda:0') tensor(1.9306e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000215
Average total loss: 2.302800
tensor(-13.1671, device='cuda:0') tensor(1.2346e-05, device='cuda:0') tensor(1.9125e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000213
Average total loss: 2.302798
tensor(-13.1764, device='cuda:0') tensor(1.2094e-05, device='cuda:0') tensor(1.8947e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000211
Average total loss: 2.302796
tensor(-13.1857, device='cuda:0') tensor(1.1849e-05, device='cuda:0') tensor(1.8773e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302794
tensor(-13.1948, device='cuda:0') tensor(1.1612e-05, device='cuda:0') tensor(1.8602e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000207
Average total loss: 2.302792
tensor(-13.2039, device='cuda:0') tensor(1.1383e-05, device='cuda:0') tensor(1.8434e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000205
Average total loss: 2.302790
tensor(-13.2129, device='cuda:0') tensor(1.1160e-05, device='cuda:0') tensor(1.8269e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000203
Average total loss: 2.302788
tensor(-13.2218, device='cuda:0') tensor(1.1046e-05, device='cuda:0') tensor(1.8107e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000202
Average total loss: 2.302786
tensor(-13.2306, device='cuda:0') tensor(1.0926e-05, device='cuda:0') tensor(1.7948e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000201
Average total loss: 2.302785
tensor(-13.2315, device='cuda:0') tensor(1.0907e-05, device='cuda:0') tensor(1.7932e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302785
tensor(-13.2324, device='cuda:0') tensor(1.0888e-05, device='cuda:0') tensor(1.7916e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302785
tensor(-13.2333, device='cuda:0') tensor(1.0869e-05, device='cuda:0') tensor(1.7900e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302785
tensor(-13.2342, device='cuda:0') tensor(1.0850e-05, device='cuda:0') tensor(1.7884e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302785
tensor(-13.2351, device='cuda:0') tensor(1.0831e-05, device='cuda:0') tensor(1.7868e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302785
tensor(-13.2360, device='cuda:0') tensor(1.0813e-05, device='cuda:0') tensor(1.7852e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302784
tensor(-13.2368, device='cuda:0') tensor(1.0794e-05, device='cuda:0') tensor(1.7837e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302784
tensor(-13.2377, device='cuda:0') tensor(1.0776e-05, device='cuda:0') tensor(1.7821e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302784
tensor(-13.2386, device='cuda:0') tensor(1.0758e-05, device='cuda:0') tensor(1.7805e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302784
tensor(-13.2395, device='cuda:0') tensor(1.0740e-05, device='cuda:0') tensor(1.7789e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302784
tensor(-13.2404, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7773e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2405, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7772e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2406, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7770e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2407, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7768e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2408, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7767e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2409, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7765e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2410, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7763e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2411, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7762e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2411, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7760e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2412, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7758e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000199
Average total loss: 2.302783
tensor(-13.2413, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7757e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
 Percentile value: -13.241496086120605
Non-zero model percentage: 20.971534729003906%, Non-zero mask percentage: 20.971534729003906%

--- Pruning Level [7/24]: ---
conv1.weight         | nonzeros =     398 /    1728             ( 23.03%) | total_pruned =    1330 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4690 /   36864             ( 12.72%) | total_pruned =   32174 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10339 /   36864             ( 28.05%) | total_pruned =   26525 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10643 /   36864             ( 28.87%) | total_pruned =   26221 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   12175 /   36864             ( 33.03%) | total_pruned =   24689 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29605 /   73728             ( 40.15%) | total_pruned =   44123 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   54431 /  147456             ( 36.91%) | total_pruned =   93025 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3195 /    8192             ( 39.00%) | total_pruned =    4997 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   34828 /  147456             ( 23.62%) | total_pruned =  112628 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   28144 /  147456             ( 19.09%) | total_pruned =  119312 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  107980 /  294912             ( 36.61%) | total_pruned =  186932 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     172 /     256             ( 67.19%) | total_pruned =      84 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  128673 /  589824             ( 21.82%) | total_pruned =  461151 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    8168 /   32768             ( 24.93%) | total_pruned =   24600 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   68600 /  589824             ( 11.63%) | total_pruned =  521224 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     132 /     256             ( 51.56%) | total_pruned =     124 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   50288 /  589824             (  8.53%) | total_pruned =  539536 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  185125 / 1179648             ( 15.69%) | total_pruned =  994523 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     266 /     512             ( 51.95%) | total_pruned =     246 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  381190 / 2359296             ( 16.16%) | total_pruned = 1978106 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     295 /     512             ( 57.62%) | total_pruned =     217 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     368 /     512             ( 71.88%) | total_pruned =     144 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22384 /  131072             ( 17.08%) | total_pruned =  108688 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     383 /     512             ( 74.80%) | total_pruned =     129 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  529621 / 2359296             ( 22.45%) | total_pruned = 1829675 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     288 /     512             ( 56.25%) | total_pruned =     224 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  662678 / 2359296             ( 28.09%) | total_pruned = 1696618 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 2344358, pruned : 8834404, total: 11178762, Compression rate :       4.77x  ( 79.03% pruned)
Train Epoch: 49/100 Loss: 0.066366 Accuracy: 83.02 99.99 % Best test Accuracy: 84.02%
tensor(-13.2414, device='cuda:0') tensor(1.0722e-05, device='cuda:0') tensor(1.7755e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-13.2501, device='cuda:0') tensor(1.0517e-05, device='cuda:0') tensor(1.7602e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000196
Average total loss: 2.302781
tensor(-13.2587, device='cuda:0') tensor(1.0317e-05, device='cuda:0') tensor(1.7452e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000194
Average total loss: 2.302779
tensor(-13.2672, device='cuda:0') tensor(1.0124e-05, device='cuda:0') tensor(1.7304e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000193
Average total loss: 2.302778
tensor(-13.2756, device='cuda:0') tensor(9.9354e-06, device='cuda:0') tensor(1.7158e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000191
Average total loss: 2.302776
tensor(-13.2840, device='cuda:0') tensor(9.7527e-06, device='cuda:0') tensor(1.7015e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000189
Average total loss: 2.302774
tensor(-13.2923, device='cuda:0') tensor(9.5746e-06, device='cuda:0') tensor(1.6874e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000188
Average total loss: 2.302773
tensor(-13.3005, device='cuda:0') tensor(9.4022e-06, device='cuda:0') tensor(1.6736e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-13.3087, device='cuda:0') tensor(9.2337e-06, device='cuda:0') tensor(1.6600e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-13.3168, device='cuda:0') tensor(9.0704e-06, device='cuda:0') tensor(1.6466e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000183
Average total loss: 2.302768
tensor(-13.3248, device='cuda:0') tensor(8.9110e-06, device='cuda:0') tensor(1.6334e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000182
Average total loss: 2.302767
tensor(-13.3328, device='cuda:0') tensor(8.7561e-06, device='cuda:0') tensor(1.6205e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000180
Average total loss: 2.302765
tensor(-13.3407, device='cuda:0') tensor(8.6057e-06, device='cuda:0') tensor(1.6077e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000180
Average total loss: 2.302765
tensor(-13.3415, device='cuda:0') tensor(8.5919e-06, device='cuda:0') tensor(1.6064e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000180
Average total loss: 2.302764
tensor(-13.3423, device='cuda:0') tensor(8.5783e-06, device='cuda:0') tensor(1.6051e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3431, device='cuda:0') tensor(8.5648e-06, device='cuda:0') tensor(1.6039e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3439, device='cuda:0') tensor(8.5514e-06, device='cuda:0') tensor(1.6026e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3447, device='cuda:0') tensor(8.5381e-06, device='cuda:0') tensor(1.6013e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3455, device='cuda:0') tensor(8.5249e-06, device='cuda:0') tensor(1.6000e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3463, device='cuda:0') tensor(8.5117e-06, device='cuda:0') tensor(1.5988e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-13.3471, device='cuda:0') tensor(8.4988e-06, device='cuda:0') tensor(1.5975e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302763
tensor(-13.3479, device='cuda:0') tensor(8.4858e-06, device='cuda:0') tensor(1.5962e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3487, device='cuda:0') tensor(8.4731e-06, device='cuda:0') tensor(1.5950e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3495, device='cuda:0') tensor(8.4603e-06, device='cuda:0') tensor(1.5937e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3495, device='cuda:0') tensor(8.4603e-06, device='cuda:0') tensor(1.5935e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3496, device='cuda:0') tensor(8.4603e-06, device='cuda:0') tensor(1.5934e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3497, device='cuda:0') tensor(8.4602e-06, device='cuda:0') tensor(1.5932e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3498, device='cuda:0') tensor(8.4602e-06, device='cuda:0') tensor(1.5931e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3499, device='cuda:0') tensor(8.4601e-06, device='cuda:0') tensor(1.5930e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3500, device='cuda:0') tensor(8.4601e-06, device='cuda:0') tensor(1.5928e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3501, device='cuda:0') tensor(8.4601e-06, device='cuda:0') tensor(1.5927e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3502, device='cuda:0') tensor(8.4600e-06, device='cuda:0') tensor(1.5925e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3503, device='cuda:0') tensor(8.4600e-06, device='cuda:0') tensor(1.5924e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3504, device='cuda:0') tensor(8.4600e-06, device='cuda:0') tensor(1.5922e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4599e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000178
Average total loss: 2.302763
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
 Percentile value: -13.350543022155762
Non-zero model percentage: 16.777233123779297%, Non-zero mask percentage: 16.777233123779297%

--- Pruning Level [8/24]: ---
conv1.weight         | nonzeros =     390 /    1728             ( 22.57%) | total_pruned =    1338 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4171 /   36864             ( 11.31%) | total_pruned =   32693 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    8658 /   36864             ( 23.49%) | total_pruned =   28206 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    8845 /   36864             ( 23.99%) | total_pruned =   28019 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    9964 /   36864             ( 27.03%) | total_pruned =   26900 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   25740 /   73728             ( 34.91%) | total_pruned =   47988 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   45673 /  147456             ( 30.97%) | total_pruned =  101783 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2828 /    8192             ( 34.52%) | total_pruned =    5364 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   25217 /  147456             ( 17.10%) | total_pruned =  122239 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   19495 /  147456             ( 13.22%) | total_pruned =  127961 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   90672 /  294912             ( 30.75%) | total_pruned =  204240 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   98729 /  589824             ( 16.74%) | total_pruned =  491095 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     178 /     256             ( 69.53%) | total_pruned =      78 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    6300 /   32768             ( 19.23%) | total_pruned =   26468 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   44880 /  589824             (  7.61%) | total_pruned =  544944 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     123 /     256             ( 48.05%) | total_pruned =     133 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   32188 /  589824             (  5.46%) | total_pruned =  557636 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     163 /     256             ( 63.67%) | total_pruned =      93 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  137128 / 1179648             ( 11.62%) | total_pruned = 1042520 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     265 /     512             ( 51.76%) | total_pruned =     247 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     322 /     512             ( 62.89%) | total_pruned =     190 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  280390 / 2359296             ( 11.88%) | total_pruned = 2078906 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     291 /     512             ( 56.84%) | total_pruned =     221 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     364 /     512             ( 71.09%) | total_pruned =     148 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   16427 /  131072             ( 12.53%) | total_pruned =  114645 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     239 /     512             ( 46.68%) | total_pruned =     273 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     380 /     512             ( 74.22%) | total_pruned =     132 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  480832 / 2359296             ( 20.38%) | total_pruned = 1878464 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     288 /     512             ( 56.25%) | total_pruned =     224 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  526332 / 2359296             ( 22.31%) | total_pruned = 1832964 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4819 /    5120             ( 94.12%) | total_pruned =     301 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 1875487, pruned : 9303275, total: 11178762, Compression rate :       5.96x  ( 83.22% pruned)
Train Epoch: 89/100 Loss: 0.045797 Accuracy: 82.05 99.98 % Best test Accuracy: 83.41%
tensor(-13.3505, device='cuda:0') tensor(8.4598e-06, device='cuda:0') tensor(1.5921e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000177
Average total loss: 2.302762
tensor(-13.3582, device='cuda:0') tensor(8.3172e-06, device='cuda:0') tensor(1.5797e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000176
Average total loss: 2.302761
tensor(-13.3660, device='cuda:0') tensor(8.1778e-06, device='cuda:0') tensor(1.5676e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000175
Average total loss: 2.302759
tensor(-13.3736, device='cuda:0') tensor(8.0418e-06, device='cuda:0') tensor(1.5557e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000173
Average total loss: 2.302758
tensor(-13.3812, device='cuda:0') tensor(7.9095e-06, device='cuda:0') tensor(1.5439e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000172
Average total loss: 2.302757
tensor(-13.3887, device='cuda:0') tensor(7.7807e-06, device='cuda:0') tensor(1.5323e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000171
Average total loss: 2.302756
tensor(-13.3962, device='cuda:0') tensor(7.6549e-06, device='cuda:0') tensor(1.5209e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000169
Average total loss: 2.302754
tensor(-13.4036, device='cuda:0') tensor(7.5318e-06, device='cuda:0') tensor(1.5096e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000168
Average total loss: 2.302753
tensor(-13.4110, device='cuda:0') tensor(7.4118e-06, device='cuda:0') tensor(1.4986e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000167
Average total loss: 2.302752
tensor(-13.4183, device='cuda:0') tensor(7.2948e-06, device='cuda:0') tensor(1.4876e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000166
Average total loss: 2.302751
tensor(-13.4256, device='cuda:0') tensor(7.1806e-06, device='cuda:0') tensor(1.4769e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000165
Average total loss: 2.302749
tensor(-13.4328, device='cuda:0') tensor(7.0693e-06, device='cuda:0') tensor(1.4663e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000163
Average total loss: 2.302748
tensor(-13.4400, device='cuda:0') tensor(6.9605e-06, device='cuda:0') tensor(1.4558e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000163
Average total loss: 2.302748
tensor(-13.4407, device='cuda:0') tensor(6.9486e-06, device='cuda:0') tensor(1.4548e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000163
Average total loss: 2.302747
tensor(-13.4414, device='cuda:0') tensor(6.9368e-06, device='cuda:0') tensor(1.4538e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4421, device='cuda:0') tensor(6.9251e-06, device='cuda:0') tensor(1.4527e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4428, device='cuda:0') tensor(6.9135e-06, device='cuda:0') tensor(1.4517e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4435, device='cuda:0') tensor(6.9019e-06, device='cuda:0') tensor(1.4507e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4442, device='cuda:0') tensor(6.8904e-06, device='cuda:0') tensor(1.4497e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4449, device='cuda:0') tensor(6.8791e-06, device='cuda:0') tensor(1.4487e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4456, device='cuda:0') tensor(6.8678e-06, device='cuda:0') tensor(1.4477e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4463, device='cuda:0') tensor(6.8567e-06, device='cuda:0') tensor(1.4466e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302747
tensor(-13.4470, device='cuda:0') tensor(6.8456e-06, device='cuda:0') tensor(1.4456e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000162
Average total loss: 2.302746
tensor(-13.4477, device='cuda:0') tensor(6.8347e-06, device='cuda:0') tensor(1.4446e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4478, device='cuda:0') tensor(6.8347e-06, device='cuda:0') tensor(1.4445e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4479, device='cuda:0') tensor(6.8346e-06, device='cuda:0') tensor(1.4443e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4480, device='cuda:0') tensor(6.8346e-06, device='cuda:0') tensor(1.4442e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4480, device='cuda:0') tensor(6.8346e-06, device='cuda:0') tensor(1.4441e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4481, device='cuda:0') tensor(6.8346e-06, device='cuda:0') tensor(1.4439e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4482, device='cuda:0') tensor(6.8345e-06, device='cuda:0') tensor(1.4438e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4483, device='cuda:0') tensor(6.8345e-06, device='cuda:0') tensor(1.4437e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4484, device='cuda:0') tensor(6.8345e-06, device='cuda:0') tensor(1.4435e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4485, device='cuda:0') tensor(6.8345e-06, device='cuda:0') tensor(1.4434e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4486, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4433e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
 Percentile value: -13.448751449584961
Non-zero model percentage: 13.42179012298584%, Non-zero mask percentage: 13.42179012298584%

--- Pruning Level [9/24]: ---
conv1.weight         | nonzeros =     383 /    1728             ( 22.16%) | total_pruned =    1345 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3859 /   36864             ( 10.47%) | total_pruned =   33005 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    7763 /   36864             ( 21.06%) | total_pruned =   29101 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7966 /   36864             ( 21.61%) | total_pruned =   28898 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    8837 /   36864             ( 23.97%) | total_pruned =   28027 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   23399 /   73728             ( 31.74%) | total_pruned =   50329 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      81 /     128             ( 63.28%) | total_pruned =      47 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   40823 /  147456             ( 27.68%) | total_pruned =  106633 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2588 /    8192             ( 31.59%) | total_pruned =    5604 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      82 /     128             ( 64.06%) | total_pruned =      46 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   20564 /  147456             ( 13.95%) | total_pruned =  126892 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   15672 /  147456             ( 10.63%) | total_pruned =  131784 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   81142 /  294912             ( 27.51%) | total_pruned =  213770 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   84449 /  589824             ( 14.32%) | total_pruned =  505375 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     177 /     256             ( 69.14%) | total_pruned =      79 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    5397 /   32768             ( 16.47%) | total_pruned =   27371 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     124 /     256             ( 48.44%) | total_pruned =     132 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   34976 /  589824             (  5.93%) | total_pruned =  554848 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     122 /     256             ( 47.66%) | total_pruned =     134 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   25090 /  589824             (  4.25%) | total_pruned =  564734 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     112 /     256             ( 43.75%) | total_pruned =     144 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  115220 / 1179648             (  9.77%) | total_pruned = 1064428 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     263 /     512             ( 51.37%) | total_pruned =     249 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     320 /     512             ( 62.50%) | total_pruned =     192 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  249957 / 2359296             ( 10.59%) | total_pruned = 2109339 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     291 /     512             ( 56.84%) | total_pruned =     221 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     363 /     512             ( 70.90%) | total_pruned =     149 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   15538 /  131072             ( 11.85%) | total_pruned =  115534 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     237 /     512             ( 46.29%) | total_pruned =     275 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     379 /     512             ( 74.02%) | total_pruned =     133 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  388930 / 2359296             ( 16.49%) | total_pruned = 1970366 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     309 /     512             ( 60.35%) | total_pruned =     203 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     285 /     512             ( 55.66%) | total_pruned =     227 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  357237 / 2359296             ( 15.14%) | total_pruned = 2002059 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4818 /    5120             ( 94.10%) | total_pruned =     302 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 1500390, pruned : 9678372, total: 11178762, Compression rate :       7.45x  ( 86.58% pruned)
Train Epoch: 92/100 Loss: 0.035040 Accuracy: 80.79 99.98 % Best test Accuracy: 83.11%
tensor(-13.4487, device='cuda:0') tensor(6.8344e-06, device='cuda:0') tensor(1.4431e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000161
Average total loss: 2.302746
tensor(-13.4557, device='cuda:0') tensor(6.7311e-06, device='cuda:0') tensor(1.4330e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000160
Average total loss: 2.302745
tensor(-13.4627, device='cuda:0') tensor(6.6300e-06, device='cuda:0') tensor(1.4230e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000159
Average total loss: 2.302743
tensor(-13.4697, device='cuda:0') tensor(6.5311e-06, device='cuda:0') tensor(1.4132e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000157
Average total loss: 2.302742
tensor(-13.4766, device='cuda:0') tensor(6.4344e-06, device='cuda:0') tensor(1.4034e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000156
Average total loss: 2.302741
tensor(-13.4834, device='cuda:0') tensor(6.3399e-06, device='cuda:0') tensor(1.3938e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000155
Average total loss: 2.302740
tensor(-13.4902, device='cuda:0') tensor(6.2475e-06, device='cuda:0') tensor(1.3844e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000154
Average total loss: 2.302739
tensor(-13.4970, device='cuda:0') tensor(6.1571e-06, device='cuda:0') tensor(1.3751e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000153
Average total loss: 2.302738
tensor(-13.5037, device='cuda:0') tensor(6.0687e-06, device='cuda:0') tensor(1.3659e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000152
Average total loss: 2.302737
tensor(-13.5104, device='cuda:0') tensor(5.9822e-06, device='cuda:0') tensor(1.3568e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000151
Average total loss: 2.302736
tensor(-13.5170, device='cuda:0') tensor(5.8976e-06, device='cuda:0') tensor(1.3478e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000150
Average total loss: 2.302735
tensor(-13.5236, device='cuda:0') tensor(5.8149e-06, device='cuda:0') tensor(1.3390e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000149
Average total loss: 2.302734
tensor(-13.5301, device='cuda:0') tensor(5.7339e-06, device='cuda:0') tensor(1.3303e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000149
Average total loss: 2.302734
tensor(-13.5308, device='cuda:0') tensor(5.7263e-06, device='cuda:0') tensor(1.3294e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000149
Average total loss: 2.302733
tensor(-13.5315, device='cuda:0') tensor(5.7187e-06, device='cuda:0') tensor(1.3285e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5321, device='cuda:0') tensor(5.7112e-06, device='cuda:0') tensor(1.3276e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5328, device='cuda:0') tensor(5.7037e-06, device='cuda:0') tensor(1.3268e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5334, device='cuda:0') tensor(5.6963e-06, device='cuda:0') tensor(1.3259e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5341, device='cuda:0') tensor(5.6890e-06, device='cuda:0') tensor(1.3250e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5347, device='cuda:0') tensor(5.6817e-06, device='cuda:0') tensor(1.3242e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5354, device='cuda:0') tensor(5.6745e-06, device='cuda:0') tensor(1.3233e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5360, device='cuda:0') tensor(5.6674e-06, device='cuda:0') tensor(1.3224e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5367, device='cuda:0') tensor(5.6604e-06, device='cuda:0') tensor(1.3216e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302733
tensor(-13.5373, device='cuda:0') tensor(5.6534e-06, device='cuda:0') tensor(1.3207e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5374, device='cuda:0') tensor(5.6502e-06, device='cuda:0') tensor(1.3206e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5374, device='cuda:0') tensor(5.6470e-06, device='cuda:0') tensor(1.3206e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5375, device='cuda:0') tensor(5.6438e-06, device='cuda:0') tensor(1.3205e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5375, device='cuda:0') tensor(5.6405e-06, device='cuda:0') tensor(1.3205e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5376, device='cuda:0') tensor(5.6373e-06, device='cuda:0') tensor(1.3204e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5376, device='cuda:0') tensor(5.6341e-06, device='cuda:0') tensor(1.3203e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5377, device='cuda:0') tensor(5.6309e-06, device='cuda:0') tensor(1.3203e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5377, device='cuda:0') tensor(5.6277e-06, device='cuda:0') tensor(1.3202e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5378, device='cuda:0') tensor(5.6244e-06, device='cuda:0') tensor(1.3202e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5378, device='cuda:0') tensor(5.6212e-06, device='cuda:0') tensor(1.3201e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000148
Average total loss: 2.302732
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
 Percentile value: -13.537899017333984
Non-zero model percentage: 10.737431526184082%, Non-zero mask percentage: 10.737431526184082%

--- Pruning Level [10/24]: ---
conv1.weight         | nonzeros =     372 /    1728             ( 21.53%) | total_pruned =    1356 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3286 /   36864             (  8.91%) | total_pruned =   33578 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6191 /   36864             ( 16.79%) | total_pruned =   30673 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6278 /   36864             ( 17.03%) | total_pruned =   30586 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    6823 /   36864             ( 18.51%) | total_pruned =   30041 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   18780 /   73728             ( 25.47%) | total_pruned =   54948 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   31693 /  147456             ( 21.49%) | total_pruned =  115763 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2189 /    8192             ( 26.72%) | total_pruned =    6003 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   17300 /  147456             ( 11.73%) | total_pruned =  130156 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   12997 /  147456             (  8.81%) | total_pruned =  134459 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   73669 /  294912             ( 24.98%) | total_pruned =  221243 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     170 /     256             ( 66.41%) | total_pruned =      86 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   74448 /  589824             ( 12.62%) | total_pruned =  515376 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     175 /     256             ( 68.36%) | total_pruned =      81 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4795 /   32768             ( 14.63%) | total_pruned =   27973 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     123 /     256             ( 48.05%) | total_pruned =     133 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   28635 /  589824             (  4.85%) | total_pruned =  561189 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     101 /     256             ( 39.45%) | total_pruned =     155 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     120 /     256             ( 46.88%) | total_pruned =     136 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   20498 /  589824             (  3.48%) | total_pruned =  569326 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  100118 / 1179648             (  8.49%) | total_pruned = 1079530 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     261 /     512             ( 50.98%) | total_pruned =     251 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     318 /     512             ( 62.11%) | total_pruned =     194 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  201848 / 2359296             (  8.56%) | total_pruned = 2157448 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     288 /     512             ( 56.25%) | total_pruned =     224 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     357 /     512             ( 69.73%) | total_pruned =     155 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   11840 /  131072             (  9.03%) | total_pruned =  119232 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     232 /     512             ( 45.31%) | total_pruned =     280 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     376 /     512             ( 73.44%) | total_pruned =     136 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  303257 / 2359296             ( 12.85%) | total_pruned = 2056039 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     307 /     512             ( 59.96%) | total_pruned =     205 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  264750 / 2359296             ( 11.22%) | total_pruned = 2094546 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4817 /    5120             ( 94.08%) | total_pruned =     303 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 1200312, pruned : 9978450, total: 11178762, Compression rate :       9.31x  ( 89.26% pruned)
Train Epoch: 99/100 Loss: 0.129418 Accuracy: 79.78 99.88 % Best test Accuracy: 81.76%
tensor(-13.5379, device='cuda:0') tensor(5.6180e-06, device='cuda:0') tensor(1.3200e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000147
Average total loss: 2.302732
tensor(-13.5443, device='cuda:0') tensor(5.5414e-06, device='cuda:0') tensor(1.3115e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000146
Average total loss: 2.302731
tensor(-13.5507, device='cuda:0') tensor(5.4655e-06, device='cuda:0') tensor(1.3032e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000145
Average total loss: 2.302730
tensor(-13.5571, device='cuda:0') tensor(5.3912e-06, device='cuda:0') tensor(1.2949e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000144
Average total loss: 2.302729
tensor(-13.5634, device='cuda:0') tensor(5.3185e-06, device='cuda:0') tensor(1.2867e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000143
Average total loss: 2.302728
tensor(-13.5697, device='cuda:0') tensor(5.2474e-06, device='cuda:0') tensor(1.2787e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000143
Average total loss: 2.302727
tensor(-13.5759, device='cuda:0') tensor(5.1778e-06, device='cuda:0') tensor(1.2707e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000142
Average total loss: 2.302727
tensor(-13.5821, device='cuda:0') tensor(5.1098e-06, device='cuda:0') tensor(1.2629e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000141
Average total loss: 2.302726
tensor(-13.5883, device='cuda:0') tensor(5.0430e-06, device='cuda:0') tensor(1.2551e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000140
Average total loss: 2.302725
tensor(-13.5944, device='cuda:0') tensor(4.9771e-06, device='cuda:0') tensor(1.2474e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000139
Average total loss: 2.302724
tensor(-13.6005, device='cuda:0') tensor(4.9125e-06, device='cuda:0') tensor(1.2399e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000138
Average total loss: 2.302723
tensor(-13.6066, device='cuda:0') tensor(4.8495e-06, device='cuda:0') tensor(1.2324e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6126, device='cuda:0') tensor(4.7880e-06, device='cuda:0') tensor(1.2250e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6132, device='cuda:0') tensor(4.7826e-06, device='cuda:0') tensor(1.2242e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6138, device='cuda:0') tensor(4.7774e-06, device='cuda:0') tensor(1.2235e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6144, device='cuda:0') tensor(4.7722e-06, device='cuda:0') tensor(1.2227e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6150, device='cuda:0') tensor(4.7670e-06, device='cuda:0') tensor(1.2220e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000137
Average total loss: 2.302722
tensor(-13.6156, device='cuda:0') tensor(4.7618e-06, device='cuda:0') tensor(1.2212e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6162, device='cuda:0') tensor(4.7567e-06, device='cuda:0') tensor(1.2205e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6169, device='cuda:0') tensor(4.7516e-06, device='cuda:0') tensor(1.2198e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6175, device='cuda:0') tensor(4.7465e-06, device='cuda:0') tensor(1.2190e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6181, device='cuda:0') tensor(4.7416e-06, device='cuda:0') tensor(1.2183e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6187, device='cuda:0') tensor(4.7366e-06, device='cuda:0') tensor(1.2175e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6193, device='cuda:0') tensor(4.7317e-06, device='cuda:0') tensor(1.2168e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6193, device='cuda:0') tensor(4.7313e-06, device='cuda:0') tensor(1.2167e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6194, device='cuda:0') tensor(4.7310e-06, device='cuda:0') tensor(1.2167e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6194, device='cuda:0') tensor(4.7306e-06, device='cuda:0') tensor(1.2166e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6195, device='cuda:0') tensor(4.7303e-06, device='cuda:0') tensor(1.2166e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6195, device='cuda:0') tensor(4.7299e-06, device='cuda:0') tensor(1.2165e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6196, device='cuda:0') tensor(4.7296e-06, device='cuda:0') tensor(1.2165e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6196, device='cuda:0') tensor(4.7293e-06, device='cuda:0') tensor(1.2164e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6197, device='cuda:0') tensor(4.7289e-06, device='cuda:0') tensor(1.2163e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6197, device='cuda:0') tensor(4.7286e-06, device='cuda:0') tensor(1.2163e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7283e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302721
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
 Percentile value: -13.619832038879395
Non-zero model percentage: 8.589949607849121%, Non-zero mask percentage: 8.589949607849121%

--- Pruning Level [11/24]: ---
conv1.weight         | nonzeros =     367 /    1728             ( 21.24%) | total_pruned =    1361 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3080 /   36864             (  8.36%) | total_pruned =   33784 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    5703 /   36864             ( 15.47%) | total_pruned =   31161 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    5757 /   36864             ( 15.62%) | total_pruned =   31107 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    6222 /   36864             ( 16.88%) | total_pruned =   30642 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      41 /      64             ( 64.06%) | total_pruned =      23 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   17202 /   73728             ( 23.33%) | total_pruned =   56526 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   29661 /  147456             ( 20.12%) | total_pruned =  117795 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2159 /    8192             ( 26.35%) | total_pruned =    6033 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   13076 /  147456             (  8.87%) | total_pruned =  134380 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    9747 /  147456             (  6.61%) | total_pruned =  137709 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   62522 /  294912             ( 21.20%) | total_pruned =  232390 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     168 /     256             ( 65.62%) | total_pruned =      88 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   60964 /  589824             ( 10.34%) | total_pruned =  528860 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     142 /     256             ( 55.47%) | total_pruned =     114 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     173 /     256             ( 67.58%) | total_pruned =      83 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    3955 /   32768             ( 12.07%) | total_pruned =   28813 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     121 /     256             ( 47.27%) | total_pruned =     135 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     148 /     256             ( 57.81%) | total_pruned =     108 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   20752 /  589824             (  3.52%) | total_pruned =  569072 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     115 /     256             ( 44.92%) | total_pruned =     141 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14931 /  589824             (  2.53%) | total_pruned =  574893 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   80022 / 1179648             (  6.78%) | total_pruned = 1099626 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     311 /     512             ( 60.74%) | total_pruned =     201 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  158644 / 2359296             (  6.72%) | total_pruned = 2200652 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     284 /     512             ( 55.47%) | total_pruned =     228 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     356 /     512             ( 69.53%) | total_pruned =     156 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    9445 /  131072             (  7.21%) | total_pruned =  121627 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     225 /     512             ( 43.95%) | total_pruned =     287 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     373 /     512             ( 72.85%) | total_pruned =     139 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  242047 / 2359296             ( 10.26%) | total_pruned = 2117249 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     307 /     512             ( 59.96%) | total_pruned =     205 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     275 /     512             ( 53.71%) | total_pruned =     237 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  203502 / 2359296             (  8.63%) | total_pruned = 2155794 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4817 /    5120             ( 94.08%) | total_pruned =     303 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 960250, pruned : 10218512, total: 11178762, Compression rate :      11.64x  ( 91.41% pruned)
Train Epoch: 99/100 Loss: 0.113338 Accuracy: 78.51 99.09 % Best test Accuracy: 81.50%
tensor(-13.6198, device='cuda:0') tensor(4.7280e-06, device='cuda:0') tensor(1.2162e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000136
Average total loss: 2.302720
tensor(-13.6257, device='cuda:0') tensor(4.6682e-06, device='cuda:0') tensor(1.2090e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000135
Average total loss: 2.302720
tensor(-13.6316, device='cuda:0') tensor(4.6097e-06, device='cuda:0') tensor(1.2019e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000134
Average total loss: 2.302719
tensor(-13.6375, device='cuda:0') tensor(4.5527e-06, device='cuda:0') tensor(1.1948e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000133
Average total loss: 2.302718
tensor(-13.6434, device='cuda:0') tensor(4.4968e-06, device='cuda:0') tensor(1.1879e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000132
Average total loss: 2.302717
tensor(-13.6492, device='cuda:0') tensor(4.4412e-06, device='cuda:0') tensor(1.1810e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000132
Average total loss: 2.302717
tensor(-13.6549, device='cuda:0') tensor(4.3869e-06, device='cuda:0') tensor(1.1742e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000131
Average total loss: 2.302716
tensor(-13.6607, device='cuda:0') tensor(4.3342e-06, device='cuda:0') tensor(1.1675e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000130
Average total loss: 2.302715
tensor(-13.6664, device='cuda:0') tensor(4.2819e-06, device='cuda:0') tensor(1.1608e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000129
Average total loss: 2.302714
tensor(-13.6720, device='cuda:0') tensor(4.2303e-06, device='cuda:0') tensor(1.1543e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000129
Average total loss: 2.302714
tensor(-13.6777, device='cuda:0') tensor(4.1803e-06, device='cuda:0') tensor(1.1478e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000128
Average total loss: 2.302713
tensor(-13.6833, device='cuda:0') tensor(4.1309e-06, device='cuda:0') tensor(1.1414e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302712
tensor(-13.6889, device='cuda:0') tensor(4.0820e-06, device='cuda:0') tensor(1.1350e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302712
tensor(-13.6894, device='cuda:0') tensor(4.0775e-06, device='cuda:0') tensor(1.1344e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302712
tensor(-13.6900, device='cuda:0') tensor(4.0732e-06, device='cuda:0') tensor(1.1338e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302712
tensor(-13.6905, device='cuda:0') tensor(4.0688e-06, device='cuda:0') tensor(1.1331e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302711
tensor(-13.6911, device='cuda:0') tensor(4.0646e-06, device='cuda:0') tensor(1.1325e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000127
Average total loss: 2.302711
tensor(-13.6917, device='cuda:0') tensor(4.0604e-06, device='cuda:0') tensor(1.1318e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6922, device='cuda:0') tensor(4.0562e-06, device='cuda:0') tensor(1.1312e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6928, device='cuda:0') tensor(4.0520e-06, device='cuda:0') tensor(1.1306e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6933, device='cuda:0') tensor(4.0479e-06, device='cuda:0') tensor(1.1299e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6939, device='cuda:0') tensor(4.0438e-06, device='cuda:0') tensor(1.1293e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6945, device='cuda:0') tensor(4.0397e-06, device='cuda:0') tensor(1.1287e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6950, device='cuda:0') tensor(4.0357e-06, device='cuda:0') tensor(1.1280e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6951, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1280e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6951, device='cuda:0') tensor(4.0357e-06, device='cuda:0') tensor(1.1279e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6952, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1279e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6952, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1278e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6953, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1278e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6953, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1277e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6954, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1277e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6954, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1276e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6954, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1276e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
 Percentile value: -13.695565223693848
Non-zero model percentage: 6.871959686279297%, Non-zero mask percentage: 6.871959686279297%

--- Pruning Level [12/24]: ---
conv1.weight         | nonzeros =     359 /    1728             ( 20.78%) | total_pruned =    1369 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2650 /   36864             (  7.19%) | total_pruned =   34214 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4767 /   36864             ( 12.93%) | total_pruned =   32097 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4700 /   36864             ( 12.75%) | total_pruned =   32164 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    5011 /   36864             ( 13.59%) | total_pruned =   31853 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   14199 /   73728             ( 19.26%) | total_pruned =   59529 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   23374 /  147456             ( 15.85%) | total_pruned =  124082 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1936 /    8192             ( 23.63%) | total_pruned =    6256 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    9952 /  147456             (  6.75%) | total_pruned =  137504 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    7298 /  147456             (  4.95%) | total_pruned =  140158 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   53035 /  294912             ( 17.98%) | total_pruned =  241877 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     167 /     256             ( 65.23%) | total_pruned =      89 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   50404 /  589824             (  8.55%) | total_pruned =  539420 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     139 /     256             ( 54.30%) | total_pruned =     117 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     170 /     256             ( 66.41%) | total_pruned =      86 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    3317 /   32768             ( 10.12%) | total_pruned =   29451 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     146 /     256             ( 57.03%) | total_pruned =     110 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   15357 /  589824             (  2.60%) | total_pruned =  574467 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      93 /     256             ( 36.33%) | total_pruned =     163 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     111 /     256             ( 43.36%) | total_pruned =     145 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   11158 /  589824             (  1.89%) | total_pruned =  578666 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   64652 / 1179648             (  5.48%) | total_pruned = 1114996 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     256 /     512             ( 50.00%) | total_pruned =     256 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     304 /     512             ( 59.38%) | total_pruned =     208 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  126022 / 2359296             (  5.34%) | total_pruned = 2233274 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     352 /     512             ( 68.75%) | total_pruned =     160 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    7533 /  131072             (  5.75%) | total_pruned =  123539 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     216 /     512             ( 42.19%) | total_pruned =     296 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     371 /     512             ( 72.46%) | total_pruned =     141 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  193858 / 2359296             (  8.22%) | total_pruned = 2165438 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     305 /     512             ( 59.57%) | total_pruned =     207 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  158214 / 2359296             (  6.71%) | total_pruned = 2201082 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4815 /    5120             ( 94.04%) | total_pruned =     305 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 768200, pruned : 10410562, total: 11178762, Compression rate :      14.55x  ( 93.13% pruned)
Train Epoch: 99/100 Loss: 0.326820 Accuracy: 77.88 95.75 % Best test Accuracy: 80.48%
tensor(-13.6955, device='cuda:0') tensor(4.0356e-06, device='cuda:0') tensor(1.1275e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000126
Average total loss: 2.302711
tensor(-13.7010, device='cuda:0') tensor(3.9895e-06, device='cuda:0') tensor(1.1213e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000125
Average total loss: 2.302710
tensor(-13.7065, device='cuda:0') tensor(3.9428e-06, device='cuda:0') tensor(1.1151e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000124
Average total loss: 2.302709
tensor(-13.7120, device='cuda:0') tensor(3.8978e-06, device='cuda:0') tensor(1.1091e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000124
Average total loss: 2.302709
tensor(-13.7174, device='cuda:0') tensor(3.8535e-06, device='cuda:0') tensor(1.1031e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000123
Average total loss: 2.302708
tensor(-13.7228, device='cuda:0') tensor(3.8093e-06, device='cuda:0') tensor(1.0972e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000122
Average total loss: 2.302707
tensor(-13.7281, device='cuda:0') tensor(3.7669e-06, device='cuda:0') tensor(1.0913e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000122
Average total loss: 2.302707
tensor(-13.7335, device='cuda:0') tensor(3.7243e-06, device='cuda:0') tensor(1.0855e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000121
Average total loss: 2.302706
tensor(-13.7388, device='cuda:0') tensor(3.6828e-06, device='cuda:0') tensor(1.0798e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000120
Average total loss: 2.302705
tensor(-13.7441, device='cuda:0') tensor(3.6423e-06, device='cuda:0') tensor(1.0741e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000120
Average total loss: 2.302705
tensor(-13.7493, device='cuda:0') tensor(3.6016e-06, device='cuda:0') tensor(1.0684e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000119
Average total loss: 2.302704
tensor(-13.7545, device='cuda:0') tensor(3.5628e-06, device='cuda:0') tensor(1.0629e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000119
Average total loss: 2.302703
tensor(-13.7597, device='cuda:0') tensor(3.5235e-06, device='cuda:0') tensor(1.0574e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7602, device='cuda:0') tensor(3.5193e-06, device='cuda:0') tensor(1.0568e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7608, device='cuda:0') tensor(3.5152e-06, device='cuda:0') tensor(1.0563e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7613, device='cuda:0') tensor(3.5111e-06, device='cuda:0') tensor(1.0557e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7618, device='cuda:0') tensor(3.5071e-06, device='cuda:0') tensor(1.0552e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7623, device='cuda:0') tensor(3.5031e-06, device='cuda:0') tensor(1.0547e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7628, device='cuda:0') tensor(3.4992e-06, device='cuda:0') tensor(1.0541e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7633, device='cuda:0') tensor(3.4952e-06, device='cuda:0') tensor(1.0536e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7638, device='cuda:0') tensor(3.4913e-06, device='cuda:0') tensor(1.0530e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302703
tensor(-13.7643, device='cuda:0') tensor(3.4874e-06, device='cuda:0') tensor(1.0525e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7649, device='cuda:0') tensor(3.4836e-06, device='cuda:0') tensor(1.0520e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7654, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0514e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7654, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0514e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7655, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0513e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7655, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0513e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7656, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0512e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7656, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0512e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000118
Average total loss: 2.302702
tensor(-13.7657, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0511e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7657, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0511e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7657, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0510e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7658, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0510e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7658, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
 Percentile value: -13.765909194946289
Non-zero model percentage: 5.497567176818848%, Non-zero mask percentage: 5.497567176818848%

--- Pruning Level [13/24]: ---
conv1.weight         | nonzeros =     359 /    1728             ( 20.78%) | total_pruned =    1369 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2508 /   36864             (  6.80%) | total_pruned =   34356 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4445 /   36864             ( 12.06%) | total_pruned =   32419 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4371 /   36864             ( 11.86%) | total_pruned =   32493 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4627 /   36864             ( 12.55%) | total_pruned =   32237 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   13229 /   73728             ( 17.94%) | total_pruned =   60499 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      72 /     128             ( 56.25%) | total_pruned =      56 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   21422 /  147456             ( 14.53%) | total_pruned =  126034 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1643 /    8192             ( 20.06%) | total_pruned =    6549 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    7031 /  147456             (  4.77%) | total_pruned =  140425 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    5172 /  147456             (  3.51%) | total_pruned =  142284 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      78 /     128             ( 60.94%) | total_pruned =      50 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   42892 /  294912             ( 14.54%) | total_pruned =  252020 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   39846 /  589824             (  6.76%) | total_pruned =  549978 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     136 /     256             ( 53.12%) | total_pruned =     120 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     170 /     256             ( 66.41%) | total_pruned =      86 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2669 /   32768             (  8.15%) | total_pruned =   30099 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     108 /     256             ( 42.19%) | total_pruned =     148 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     145 /     256             ( 56.64%) | total_pruned =     111 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   10598 /  589824             (  1.80%) | total_pruned =  579226 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      87 /     256             ( 33.98%) | total_pruned =     169 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     107 /     256             ( 41.80%) | total_pruned =     149 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    7670 /  589824             (  1.30%) | total_pruned =  582154 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     157 /     256             ( 61.33%) | total_pruned =      99 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   50013 / 1179648             (  4.24%) | total_pruned = 1129635 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     251 /     512             ( 49.02%) | total_pruned =     261 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     297 /     512             ( 58.01%) | total_pruned =     215 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   94502 / 2359296             (  4.01%) | total_pruned = 2264794 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     273 /     512             ( 53.32%) | total_pruned =     239 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     345 /     512             ( 67.38%) | total_pruned =     167 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    5771 /  131072             (  4.40%) | total_pruned =  125301 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     202 /     512             ( 39.45%) | total_pruned =     310 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     366 /     512             ( 71.48%) | total_pruned =     146 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  157483 / 2359296             (  6.67%) | total_pruned = 2201813 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     302 /     512             ( 58.98%) | total_pruned =     210 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     268 /     512             ( 52.34%) | total_pruned =     244 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  128004 / 2359296             (  5.43%) | total_pruned = 2231292 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4813 /    5120             ( 94.00%) | total_pruned =     307 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 614560, pruned : 10564202, total: 11178762, Compression rate :      18.19x  ( 94.50% pruned)
Train Epoch: 99/100 Loss: 0.383897 Accuracy: 76.83 92.60 % Best test Accuracy: 78.74%
tensor(-13.7659, device='cuda:0') tensor(3.4797e-06, device='cuda:0') tensor(1.0509e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302702
tensor(-13.7710, device='cuda:0') tensor(3.4428e-06, device='cuda:0') tensor(1.0455e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000117
Average total loss: 2.302701
tensor(-13.7761, device='cuda:0') tensor(3.4054e-06, device='cuda:0') tensor(1.0402e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000116
Average total loss: 2.302701
tensor(-13.7812, device='cuda:0') tensor(3.3699e-06, device='cuda:0') tensor(1.0349e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000115
Average total loss: 2.302700
tensor(-13.7863, device='cuda:0') tensor(3.3334e-06, device='cuda:0') tensor(1.0297e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000115
Average total loss: 2.302700
tensor(-13.7913, device='cuda:0') tensor(3.2987e-06, device='cuda:0') tensor(1.0245e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000114
Average total loss: 2.302699
tensor(-13.7963, device='cuda:0') tensor(3.2637e-06, device='cuda:0') tensor(1.0194e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000114
Average total loss: 2.302699
tensor(-13.8013, device='cuda:0') tensor(3.2299e-06, device='cuda:0') tensor(1.0143e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000113
Average total loss: 2.302698
tensor(-13.8063, device='cuda:0') tensor(3.1962e-06, device='cuda:0') tensor(1.0093e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000113
Average total loss: 2.302697
tensor(-13.8112, device='cuda:0') tensor(3.1633e-06, device='cuda:0') tensor(1.0043e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000112
Average total loss: 2.302697
tensor(-13.8161, device='cuda:0') tensor(3.1308e-06, device='cuda:0') tensor(9.9941e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000111
Average total loss: 2.302696
tensor(-13.8210, device='cuda:0') tensor(3.0988e-06, device='cuda:0') tensor(9.9454e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000111
Average total loss: 2.302696
tensor(-13.8258, device='cuda:0') tensor(3.0673e-06, device='cuda:0') tensor(9.8972e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000111
Average total loss: 2.302696
tensor(-13.8263, device='cuda:0') tensor(3.0628e-06, device='cuda:0') tensor(9.8926e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000111
Average total loss: 2.302696
tensor(-13.8268, device='cuda:0') tensor(3.0583e-06, device='cuda:0') tensor(9.8880e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000111
Average total loss: 2.302695
tensor(-13.8272, device='cuda:0') tensor(3.0538e-06, device='cuda:0') tensor(9.8833e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8277, device='cuda:0') tensor(3.0494e-06, device='cuda:0') tensor(9.8787e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8282, device='cuda:0') tensor(3.0450e-06, device='cuda:0') tensor(9.8741e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8286, device='cuda:0') tensor(3.0406e-06, device='cuda:0') tensor(9.8695e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8291, device='cuda:0') tensor(3.0363e-06, device='cuda:0') tensor(9.8649e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8296, device='cuda:0') tensor(3.0319e-06, device='cuda:0') tensor(9.8603e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8300, device='cuda:0') tensor(3.0276e-06, device='cuda:0') tensor(9.8557e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8305, device='cuda:0') tensor(3.0234e-06, device='cuda:0') tensor(9.8511e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8310, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8465e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8310, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8460e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8311, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8455e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8311, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8451e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8312, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8446e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8312, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8442e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8313, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8437e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8313, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8432e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8314, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8428e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8314, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8423e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8314, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8419e-12, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
 Percentile value: -13.831510543823242
Non-zero model percentage: 4.398054122924805%, Non-zero mask percentage: 4.398054122924805%

--- Pruning Level [14/24]: ---
conv1.weight         | nonzeros =     355 /    1728             ( 20.54%) | total_pruned =    1373 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2198 /   36864             (  5.96%) | total_pruned =   34666 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3801 /   36864             ( 10.31%) | total_pruned =   33063 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3615 /   36864             (  9.81%) | total_pruned =   33249 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3858 /   36864             ( 10.47%) | total_pruned =   33006 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   11114 /   73728             ( 15.07%) | total_pruned =   62614 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   17658 /  147456             ( 11.98%) | total_pruned =  129798 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1462 /    8192             ( 17.85%) | total_pruned =    6730 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      72 /     128             ( 56.25%) | total_pruned =      56 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    5189 /  147456             (  3.52%) | total_pruned =  142267 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3854 /  147456             (  2.61%) | total_pruned =  143602 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   35423 /  294912             ( 12.01%) | total_pruned =  259489 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   32360 /  589824             (  5.49%) | total_pruned =  557464 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     166 /     256             ( 64.84%) | total_pruned =      90 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2242 /   32768             (  6.84%) | total_pruned =   30526 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     100 /     256             ( 39.06%) | total_pruned =     156 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    7690 /  589824             (  1.30%) | total_pruned =  582134 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      79 /     256             ( 30.86%) | total_pruned =     177 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    5576 /  589824             (  0.95%) | total_pruned =  584248 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      83 /     256             ( 32.42%) | total_pruned =     173 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     157 /     256             ( 61.33%) | total_pruned =      99 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   40906 / 1179648             (  3.47%) | total_pruned = 1138742 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     251 /     512             ( 49.02%) | total_pruned =     261 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     292 /     512             ( 57.03%) | total_pruned =     220 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   79478 / 2359296             (  3.37%) | total_pruned = 2279818 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     271 /     512             ( 52.93%) | total_pruned =     241 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     342 /     512             ( 66.80%) | total_pruned =     170 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    4910 /  131072             (  3.75%) | total_pruned =  126162 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     194 /     512             ( 37.89%) | total_pruned =     318 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     360 /     512             ( 70.31%) | total_pruned =     152 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  123351 / 2359296             (  5.23%) | total_pruned = 2235945 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     298 /     512             ( 58.20%) | total_pruned =     214 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     261 /     512             ( 50.98%) | total_pruned =     251 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   96402 / 2359296             (  4.09%) | total_pruned = 2262894 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     335 /     512             ( 65.43%) | total_pruned =     177 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4812 /    5120             ( 93.98%) | total_pruned =     308 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       9 /      10             ( 90.00%) | total_pruned =       1 | shape = torch.Size([10])
alive: 491648, pruned : 10687114, total: 11178762, Compression rate :      22.74x  ( 95.60% pruned)
Train Epoch: 99/100 Loss: 0.287831 Accuracy: 75.72 90.55 % Best test Accuracy: 77.74%
tensor(-13.8315, device='cuda:0') tensor(3.0191e-06, device='cuda:0') tensor(9.8414e-12, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000110
Average total loss: 2.302695
tensor(-13.8363, device='cuda:0') tensor(2.9891e-06, device='cuda:0') tensor(9.7941e-12, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000109
Average total loss: 2.302694
tensor(-13.8411, device='cuda:0') tensor(2.9590e-06, device='cuda:0') tensor(9.7474e-12, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000109
Average total loss: 2.302694
tensor(-13.8459, device='cuda:0') tensor(2.9298e-06, device='cuda:0') tensor(9.7010e-12, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000108
Average total loss: 2.302693
tensor(-13.8506, device='cuda:0') tensor(2.9059e-06, device='cuda:0') tensor(9.6552e-12, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000108
Average total loss: 2.302693
tensor(-13.8553, device='cuda:0') tensor(2.8774e-06, device='cuda:0') tensor(9.6097e-12, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000107
Average total loss: 2.302692
tensor(-13.8600, device='cuda:0') tensor(2.8491e-06, device='cuda:0') tensor(9.5647e-12, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000107
Average total loss: 2.302692
tensor(-13.8647, device='cuda:0') tensor(2.8212e-06, device='cuda:0') tensor(9.5201e-12, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000106
Average total loss: 2.302691
tensor(-13.8693, device='cuda:0') tensor(2.7940e-06, device='cuda:0') tensor(9.4758e-12, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000106
Average total loss: 2.302691
tensor(-13.8740, device='cuda:0') tensor(2.7667e-06, device='cuda:0') tensor(9.4321e-12, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000105
Average total loss: 2.302690
tensor(-13.8786, device='cuda:0') tensor(2.7406e-06, device='cuda:0') tensor(9.3886e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000105
Average total loss: 2.302690
tensor(-13.8832, device='cuda:0') tensor(2.7138e-06, device='cuda:0') tensor(9.3457e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8878, device='cuda:0') tensor(2.6887e-06, device='cuda:0') tensor(9.3030e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8882, device='cuda:0') tensor(2.6869e-06, device='cuda:0') tensor(9.2987e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8887, device='cuda:0') tensor(2.6851e-06, device='cuda:0') tensor(9.2943e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8892, device='cuda:0') tensor(2.6832e-06, device='cuda:0') tensor(9.2900e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8896, device='cuda:0') tensor(2.6814e-06, device='cuda:0') tensor(9.2856e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8901, device='cuda:0') tensor(2.6797e-06, device='cuda:0') tensor(9.2813e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8906, device='cuda:0') tensor(2.6779e-06, device='cuda:0') tensor(9.2770e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8910, device='cuda:0') tensor(2.6761e-06, device='cuda:0') tensor(9.2726e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302689
tensor(-13.8915, device='cuda:0') tensor(2.6744e-06, device='cuda:0') tensor(9.2683e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302688
tensor(-13.8920, device='cuda:0') tensor(2.6727e-06, device='cuda:0') tensor(9.2640e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000104
Average total loss: 2.302688
tensor(-13.8924, device='cuda:0') tensor(2.6710e-06, device='cuda:0') tensor(9.2596e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8929, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2553e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8929, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2549e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8930, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2544e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8930, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2540e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8931, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2536e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8931, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2531e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8932, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2527e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8932, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2523e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8933, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2519e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8933, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2514e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2510e-12, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
 Percentile value: -13.893414497375488
Non-zero model percentage: 3.5184483528137207%, Non-zero mask percentage: 3.5184483528137207%

--- Pruning Level [15/24]: ---
conv1.weight         | nonzeros =     351 /    1728             ( 20.31%) | total_pruned =    1377 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1965 /   36864             (  5.33%) | total_pruned =   34899 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3350 /   36864             (  9.09%) | total_pruned =   33514 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3108 /   36864             (  8.43%) | total_pruned =   33756 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3322 /   36864             (  9.01%) | total_pruned =   33542 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    9606 /   73728             ( 13.03%) | total_pruned =   64122 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   14903 /  147456             ( 10.11%) | total_pruned =  132553 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      74 /     128             ( 57.81%) | total_pruned =      54 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1331 /    8192             ( 16.25%) | total_pruned =    6861 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      73 /     128             ( 57.03%) | total_pruned =      55 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    4087 /  147456             (  2.77%) | total_pruned =  143369 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    3001 /  147456             (  2.04%) | total_pruned =  144455 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      77 /     128             ( 60.16%) | total_pruned =      51 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   30237 /  294912             ( 10.25%) | total_pruned =  264675 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     163 /     256             ( 63.67%) | total_pruned =      93 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   27573 /  589824             (  4.67%) | total_pruned =  562251 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     163 /     256             ( 63.67%) | total_pruned =      93 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1901 /   32768             (  5.80%) | total_pruned =   30867 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     140 /     256             ( 54.69%) | total_pruned =     116 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5892 /  589824             (  1.00%) | total_pruned =  583932 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      76 /     256             ( 29.69%) | total_pruned =     180 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      89 /     256             ( 34.77%) | total_pruned =     167 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    4318 /  589824             (  0.73%) | total_pruned =  585506 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      79 /     256             ( 30.86%) | total_pruned =     177 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   33234 / 1179648             (  2.82%) | total_pruned = 1146414 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     247 /     512             ( 48.24%) | total_pruned =     265 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     285 /     512             ( 55.66%) | total_pruned =     227 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   59376 / 2359296             (  2.52%) | total_pruned = 2299920 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     265 /     512             ( 51.76%) | total_pruned =     247 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     339 /     512             ( 66.21%) | total_pruned =     173 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3708 /  131072             (  2.83%) | total_pruned =  127364 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     186 /     512             ( 36.33%) | total_pruned =     326 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     357 /     512             ( 69.73%) | total_pruned =     155 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   96518 / 2359296             (  4.09%) | total_pruned = 2262778 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     296 /     512             ( 57.81%) | total_pruned =     216 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     249 /     512             ( 48.63%) | total_pruned =     263 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   75422 / 2359296             (  3.20%) | total_pruned = 2283874 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     334 /     512             ( 65.23%) | total_pruned =     178 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4810 /    5120             ( 93.95%) | total_pruned =     310 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       8 /      10             ( 80.00%) | total_pruned =       2 | shape = torch.Size([10])
alive: 393319, pruned : 10785443, total: 11178762, Compression rate :      28.42x  ( 96.48% pruned)
Train Epoch: 99/100 Loss: 0.446458 Accuracy: 75.17 85.08 % Best test Accuracy: 76.60%
tensor(-13.8934, device='cuda:0') tensor(2.6693e-06, device='cuda:0') tensor(9.2506e-12, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.8979, device='cuda:0') tensor(2.6450e-06, device='cuda:0') tensor(9.2087e-12, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000103
Average total loss: 2.302688
tensor(-13.9024, device='cuda:0') tensor(2.6194e-06, device='cuda:0') tensor(9.1674e-12, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000102
Average total loss: 2.302687
tensor(-13.9069, device='cuda:0') tensor(2.5954e-06, device='cuda:0') tensor(9.1264e-12, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000102
Average total loss: 2.302687
tensor(-13.9114, device='cuda:0') tensor(2.5711e-06, device='cuda:0') tensor(9.0858e-12, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000101
Average total loss: 2.302686
tensor(-13.9158, device='cuda:0') tensor(2.5473e-06, device='cuda:0') tensor(9.0455e-12, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000101
Average total loss: 2.302686
tensor(-13.9203, device='cuda:0') tensor(2.5244e-06, device='cuda:0') tensor(9.0056e-12, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000100
Average total loss: 2.302685
tensor(-13.9246, device='cuda:0') tensor(2.5006e-06, device='cuda:0') tensor(8.9661e-12, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000100
Average total loss: 2.302685
tensor(-13.9290, device='cuda:0') tensor(2.4785e-06, device='cuda:0') tensor(8.9268e-12, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000100
Average total loss: 2.302684
tensor(-13.9334, device='cuda:0') tensor(2.4554e-06, device='cuda:0') tensor(8.8880e-12, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000099
Average total loss: 2.302684
tensor(-13.9377, device='cuda:0') tensor(2.4334e-06, device='cuda:0') tensor(8.8494e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000099
Average total loss: 2.302684
tensor(-13.9421, device='cuda:0') tensor(2.4118e-06, device='cuda:0') tensor(8.8112e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9464, device='cuda:0') tensor(2.3897e-06, device='cuda:0') tensor(8.7734e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9468, device='cuda:0') tensor(2.3872e-06, device='cuda:0') tensor(8.7697e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9472, device='cuda:0') tensor(2.3846e-06, device='cuda:0') tensor(8.7660e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9476, device='cuda:0') tensor(2.3821e-06, device='cuda:0') tensor(8.7623e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9481, device='cuda:0') tensor(2.3795e-06, device='cuda:0') tensor(8.7586e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9485, device='cuda:0') tensor(2.3771e-06, device='cuda:0') tensor(8.7549e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9489, device='cuda:0') tensor(2.3746e-06, device='cuda:0') tensor(8.7513e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9493, device='cuda:0') tensor(2.3721e-06, device='cuda:0') tensor(8.7476e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9497, device='cuda:0') tensor(2.3696e-06, device='cuda:0') tensor(8.7439e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9502, device='cuda:0') tensor(2.3672e-06, device='cuda:0') tensor(8.7402e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9506, device='cuda:0') tensor(2.3647e-06, device='cuda:0') tensor(8.7365e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302683
tensor(-13.9510, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7329e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9510, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7325e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9511, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7321e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9511, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7317e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9512, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7312e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9512, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7308e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9513, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7304e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9513, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7300e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9514, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7296e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9514, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7292e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3623e-06, device='cuda:0') tensor(8.7288e-12, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000098
Average total loss: 2.302682
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
 Percentile value: -13.951504707336426
Non-zero model percentage: 2.8147659301757812%, Non-zero mask percentage: 2.8147659301757812%

--- Pruning Level [16/24]: ---
conv1.weight         | nonzeros =     347 /    1728             ( 20.08%) | total_pruned =    1381 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1728 /   36864             (  4.69%) | total_pruned =   35136 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2898 /   36864             (  7.86%) | total_pruned =   33966 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2615 /   36864             (  7.09%) | total_pruned =   34249 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2788 /   36864             (  7.56%) | total_pruned =   34076 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    8009 /   73728             ( 10.86%) | total_pruned =   65719 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12273 /  147456             (  8.32%) | total_pruned =  135183 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1189 /    8192             ( 14.51%) | total_pruned =    7003 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      72 /     128             ( 56.25%) | total_pruned =      56 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2993 /  147456             (  2.03%) | total_pruned =  144463 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2233 /  147456             (  1.51%) | total_pruned =  145223 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   24880 /  294912             (  8.44%) | total_pruned =  270032 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     161 /     256             ( 62.89%) | total_pruned =      95 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   23203 /  589824             (  3.93%) | total_pruned =  566621 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     127 /     256             ( 49.61%) | total_pruned =     129 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1617 /   32768             (  4.93%) | total_pruned =   31151 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      95 /     256             ( 37.11%) | total_pruned =     161 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     138 /     256             ( 53.91%) | total_pruned =     118 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4512 /  589824             (  0.76%) | total_pruned =  585312 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      75 /     256             ( 29.30%) | total_pruned =     181 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      85 /     256             ( 33.20%) | total_pruned =     171 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3346 /  589824             (  0.57%) | total_pruned =  586478 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      73 /     256             ( 28.52%) | total_pruned =     183 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   27757 / 1179648             (  2.35%) | total_pruned = 1151891 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     240 /     512             ( 46.88%) | total_pruned =     272 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     279 /     512             ( 54.49%) | total_pruned =     233 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   48008 / 2359296             (  2.03%) | total_pruned = 2311288 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     259 /     512             ( 50.59%) | total_pruned =     253 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     336 /     512             ( 65.62%) | total_pruned =     176 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3071 /  131072             (  2.34%) | total_pruned =  128001 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     173 /     512             ( 33.79%) | total_pruned =     339 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     353 /     512             ( 68.95%) | total_pruned =     159 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   74487 / 2359296             (  3.16%) | total_pruned = 2284809 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     287 /     512             ( 56.05%) | total_pruned =     225 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     244 /     512             ( 47.66%) | total_pruned =     268 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   56688 / 2359296             (  2.40%) | total_pruned = 2302608 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     333 /     512             ( 65.04%) | total_pruned =     179 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4810 /    5120             ( 93.95%) | total_pruned =     310 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       8 /      10             ( 80.00%) | total_pruned =       2 | shape = torch.Size([10])
alive: 314656, pruned : 10864106, total: 11178762, Compression rate :      35.53x  ( 97.19% pruned)
Train Epoch: 99/100 Loss: 0.530909 Accuracy: 73.50 82.61 % Best test Accuracy: 74.32%
tensor(-13.9515, device='cuda:0') tensor(2.3622e-06, device='cuda:0') tensor(8.7284e-12, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000097
Average total loss: 2.302682
tensor(-13.9558, device='cuda:0') tensor(2.3404e-06, device='cuda:0') tensor(8.6913e-12, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000097
Average total loss: 2.302682
tensor(-13.9600, device='cuda:0') tensor(2.3201e-06, device='cuda:0') tensor(8.6545e-12, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000097
Average total loss: 2.302681
tensor(-13.9643, device='cuda:0') tensor(2.2998e-06, device='cuda:0') tensor(8.6179e-12, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000096
Average total loss: 2.302681
tensor(-13.9685, device='cuda:0') tensor(2.2792e-06, device='cuda:0') tensor(8.5817e-12, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000096
Average total loss: 2.302681
tensor(-13.9727, device='cuda:0') tensor(2.2601e-06, device='cuda:0') tensor(8.5457e-12, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000095
Average total loss: 2.302680
tensor(-13.9768, device='cuda:0') tensor(2.2400e-06, device='cuda:0') tensor(8.5101e-12, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000095
Average total loss: 2.302680
tensor(-13.9810, device='cuda:0') tensor(2.2205e-06, device='cuda:0') tensor(8.4748e-12, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000095
Average total loss: 2.302679
tensor(-13.9852, device='cuda:0') tensor(2.2023e-06, device='cuda:0') tensor(8.4396e-12, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000094
Average total loss: 2.302679
tensor(-13.9893, device='cuda:0') tensor(2.1825e-06, device='cuda:0') tensor(8.4050e-12, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000094
Average total loss: 2.302679
tensor(-13.9934, device='cuda:0') tensor(2.1642e-06, device='cuda:0') tensor(8.3705e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-13.9975, device='cuda:0') tensor(2.1462e-06, device='cuda:0') tensor(8.3363e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0015, device='cuda:0') tensor(2.1274e-06, device='cuda:0') tensor(8.3024e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0020, device='cuda:0') tensor(2.1264e-06, device='cuda:0') tensor(8.2990e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0024, device='cuda:0') tensor(2.1254e-06, device='cuda:0') tensor(8.2955e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0028, device='cuda:0') tensor(2.1245e-06, device='cuda:0') tensor(8.2920e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0032, device='cuda:0') tensor(2.1236e-06, device='cuda:0') tensor(8.2885e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302678
tensor(-14.0037, device='cuda:0') tensor(2.1226e-06, device='cuda:0') tensor(8.2850e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302677
tensor(-14.0041, device='cuda:0') tensor(2.1217e-06, device='cuda:0') tensor(8.2815e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302677
tensor(-14.0045, device='cuda:0') tensor(2.1208e-06, device='cuda:0') tensor(8.2780e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000093
Average total loss: 2.302677
tensor(-14.0049, device='cuda:0') tensor(2.1200e-06, device='cuda:0') tensor(8.2746e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0053, device='cuda:0') tensor(2.1191e-06, device='cuda:0') tensor(8.2711e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0057, device='cuda:0') tensor(2.1182e-06, device='cuda:0') tensor(8.2676e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0062, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2641e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0062, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2637e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0063, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2634e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0063, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2630e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0064, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2626e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0064, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2622e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0064, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2618e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0065, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2614e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0065, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2610e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0066, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2606e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0066, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2603e-12, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
 Percentile value: -14.006656646728516
Non-zero model percentage: 2.251814603805542%, Non-zero mask percentage: 2.251814603805542%

--- Pruning Level [17/24]: ---
conv1.weight         | nonzeros =     341 /    1728             ( 19.73%) | total_pruned =    1387 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1509 /   36864             (  4.09%) | total_pruned =   35355 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2524 /   36864             (  6.85%) | total_pruned =   34340 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2226 /   36864             (  6.04%) | total_pruned =   34638 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2389 /   36864             (  6.48%) | total_pruned =   34475 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      35 /      64             ( 54.69%) | total_pruned =      29 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    6787 /   73728             (  9.21%) | total_pruned =   66941 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   10292 /  147456             (  6.98%) | total_pruned =  137164 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1069 /    8192             ( 13.05%) | total_pruned =    7123 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2302 /  147456             (  1.56%) | total_pruned =  145154 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1765 /  147456             (  1.20%) | total_pruned =  145691 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   20963 /  294912             (  7.11%) | total_pruned =  273949 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     159 /     256             ( 62.11%) | total_pruned =      97 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     160 /     256             ( 62.50%) | total_pruned =      96 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   19328 /  589824             (  3.28%) | total_pruned =  570496 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     124 /     256             ( 48.44%) | total_pruned =     132 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1341 /   32768             (  4.09%) | total_pruned =   31427 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      88 /     256             ( 34.38%) | total_pruned =     168 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    3302 /  589824             (  0.56%) | total_pruned =  586522 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      66 /     256             ( 25.78%) | total_pruned =     190 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      79 /     256             ( 30.86%) | total_pruned =     177 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    2494 /  589824             (  0.42%) | total_pruned =  587330 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      71 /     256             ( 27.73%) | total_pruned =     185 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   22367 / 1179648             (  1.90%) | total_pruned = 1157281 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     236 /     512             ( 46.09%) | total_pruned =     276 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     276 /     512             ( 53.91%) | total_pruned =     236 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   37158 / 2359296             (  1.57%) | total_pruned = 2322138 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     253 /     512             ( 49.41%) | total_pruned =     259 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     331 /     512             ( 64.65%) | total_pruned =     181 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2420 /  131072             (  1.85%) | total_pruned =  128652 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     163 /     512             ( 31.84%) | total_pruned =     349 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     344 /     512             ( 67.19%) | total_pruned =     168 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   57448 / 2359296             (  2.43%) | total_pruned = 2301848 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     275 /     512             ( 53.71%) | total_pruned =     237 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     235 /     512             ( 45.90%) | total_pruned =     277 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   43818 / 2359296             (  1.86%) | total_pruned = 2315478 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     332 /     512             ( 64.84%) | total_pruned =     180 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4805 /    5120             ( 93.85%) | total_pruned =     315 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       8 /      10             ( 80.00%) | total_pruned =       2 | shape = torch.Size([10])
alive: 251725, pruned : 10927037, total: 11178762, Compression rate :      44.41x  ( 97.75% pruned)
Train Epoch: 99/100 Loss: 0.672853 Accuracy: 72.83 80.33 % Best test Accuracy: 73.08%
tensor(-14.0067, device='cuda:0') tensor(2.1174e-06, device='cuda:0') tensor(8.2599e-12, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0107, device='cuda:0') tensor(2.0989e-06, device='cuda:0') tensor(8.2267e-12, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000092
Average total loss: 2.302677
tensor(-14.0147, device='cuda:0') tensor(2.0813e-06, device='cuda:0') tensor(8.1937e-12, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000091
Average total loss: 2.302676
tensor(-14.0187, device='cuda:0') tensor(2.0648e-06, device='cuda:0') tensor(8.1608e-12, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000091
Average total loss: 2.302676
tensor(-14.0227, device='cuda:0') tensor(2.0470e-06, device='cuda:0') tensor(8.1284e-12, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000091
Average total loss: 2.302676
tensor(-14.0267, device='cuda:0') tensor(2.0301e-06, device='cuda:0') tensor(8.0962e-12, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000090
Average total loss: 2.302675
tensor(-14.0307, device='cuda:0') tensor(2.0143e-06, device='cuda:0') tensor(8.0641e-12, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000090
Average total loss: 2.302675
tensor(-14.0346, device='cuda:0') tensor(1.9971e-06, device='cuda:0') tensor(8.0324e-12, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000090
Average total loss: 2.302674
tensor(-14.0385, device='cuda:0') tensor(1.9808e-06, device='cuda:0') tensor(8.0010e-12, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000089
Average total loss: 2.302674
tensor(-14.0425, device='cuda:0') tensor(1.9655e-06, device='cuda:0') tensor(7.9696e-12, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000089
Average total loss: 2.302674
tensor(-14.0463, device='cuda:0') tensor(1.9490e-06, device='cuda:0') tensor(7.9387e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000089
Average total loss: 2.302673
tensor(-14.0502, device='cuda:0') tensor(1.9332e-06, device='cuda:0') tensor(7.9080e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0541, device='cuda:0') tensor(1.9185e-06, device='cuda:0') tensor(7.8773e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0545, device='cuda:0') tensor(1.9162e-06, device='cuda:0') tensor(7.8744e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0548, device='cuda:0') tensor(1.9140e-06, device='cuda:0') tensor(7.8715e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0552, device='cuda:0') tensor(1.9117e-06, device='cuda:0') tensor(7.8685e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0556, device='cuda:0') tensor(1.9094e-06, device='cuda:0') tensor(7.8656e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0560, device='cuda:0') tensor(1.9116e-06, device='cuda:0') tensor(7.8626e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0563, device='cuda:0') tensor(1.9096e-06, device='cuda:0') tensor(7.8597e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0567, device='cuda:0') tensor(1.9074e-06, device='cuda:0') tensor(7.8568e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0571, device='cuda:0') tensor(1.9052e-06, device='cuda:0') tensor(7.8538e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0575, device='cuda:0') tensor(1.9030e-06, device='cuda:0') tensor(7.8509e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0578, device='cuda:0') tensor(1.9009e-06, device='cuda:0') tensor(7.8479e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0582, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8450e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0583, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8446e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0583, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8443e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0584, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8439e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0584, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8435e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0584, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8432e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0585, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8428e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0585, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8424e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0586, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8421e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0586, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8417e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8987e-06, device='cuda:0') tensor(7.8413e-12, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000088
Average total loss: 2.302673
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
 Percentile value: -14.058682441711426
Non-zero model percentage: 1.8014516830444336%, Non-zero mask percentage: 1.8014516830444336%

--- Pruning Level [18/24]: ---
conv1.weight         | nonzeros =     334 /    1728             ( 19.33%) | total_pruned =    1394 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
bn1.bias             | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1325 /   36864             (  3.59%) | total_pruned =   35539 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2149 /   36864             (  5.83%) | total_pruned =   34715 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1867 /   36864             (  5.06%) | total_pruned =   34997 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2001 /   36864             (  5.43%) | total_pruned =   34863 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      35 /      64             ( 54.69%) | total_pruned =      29 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    5609 /   73728             (  7.61%) | total_pruned =   68119 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    8305 /  147456             (  5.63%) | total_pruned =  139151 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     950 /    8192             ( 11.60%) | total_pruned =    7242 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      71 /     128             ( 55.47%) | total_pruned =      57 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1676 /  147456             (  1.14%) | total_pruned =  145780 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1294 /  147456             (  0.88%) | total_pruned =  146162 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      54 /     128             ( 42.19%) | total_pruned =      74 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      76 /     128             ( 59.38%) | total_pruned =      52 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   17102 /  294912             (  5.80%) | total_pruned =  277810 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     157 /     256             ( 61.33%) | total_pruned =      99 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     159 /     256             ( 62.11%) | total_pruned =      97 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   15800 /  589824             (  2.68%) | total_pruned =  574024 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1122 /   32768             (  3.42%) | total_pruned =   31646 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      82 /     256             ( 32.03%) | total_pruned =     174 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2408 /  589824             (  0.41%) | total_pruned =  587416 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      59 /     256             ( 23.05%) | total_pruned =     197 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      75 /     256             ( 29.30%) | total_pruned =     181 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1843 /  589824             (  0.31%) | total_pruned =  587981 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      63 /     256             ( 24.61%) | total_pruned =     193 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     146 /     256             ( 57.03%) | total_pruned =     110 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   18157 / 1179648             (  1.54%) | total_pruned = 1161491 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     232 /     512             ( 45.31%) | total_pruned =     280 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   29056 / 2359296             (  1.23%) | total_pruned = 2330240 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     249 /     512             ( 48.63%) | total_pruned =     263 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1940 /  131072             (  1.48%) | total_pruned =  129132 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     153 /     512             ( 29.88%) | total_pruned =     359 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     341 /     512             ( 66.60%) | total_pruned =     171 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   44475 / 2359296             (  1.89%) | total_pruned = 2314821 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     271 /     512             ( 52.93%) | total_pruned =     241 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     225 /     512             ( 43.95%) | total_pruned =     287 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   34199 / 2359296             (  1.45%) | total_pruned = 2325097 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     328 /     512             ( 64.06%) | total_pruned =     184 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4801 /    5120             ( 93.77%) | total_pruned =     319 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       8 /      10             ( 80.00%) | total_pruned =       2 | shape = torch.Size([10])
alive: 201380, pruned : 10977382, total: 11178762, Compression rate :      55.51x  ( 98.20% pruned)
Train Epoch: 99/100 Loss: 0.766906 Accuracy: 69.63 75.47 % Best test Accuracy: 70.03%
tensor(-14.0587, device='cuda:0') tensor(1.8986e-06, device='cuda:0') tensor(7.8410e-12, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000087
Average total loss: 2.302672
tensor(-14.0626, device='cuda:0') tensor(1.8835e-06, device='cuda:0') tensor(7.8110e-12, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000087
Average total loss: 2.302672
tensor(-14.0664, device='cuda:0') tensor(1.8694e-06, device='cuda:0') tensor(7.7811e-12, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000087
Average total loss: 2.302672
tensor(-14.0702, device='cuda:0') tensor(1.8542e-06, device='cuda:0') tensor(7.7516e-12, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000086
Average total loss: 2.302671
tensor(-14.0740, device='cuda:0') tensor(1.8394e-06, device='cuda:0') tensor(7.7223e-12, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000086
Average total loss: 2.302671
tensor(-14.0778, device='cuda:0') tensor(1.8255e-06, device='cuda:0') tensor(7.6932e-12, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000086
Average total loss: 2.302671
tensor(-14.0815, device='cuda:0') tensor(1.8113e-06, device='cuda:0') tensor(7.6643e-12, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000086
Average total loss: 2.302670
tensor(-14.0853, device='cuda:0') tensor(1.7967e-06, device='cuda:0') tensor(7.6357e-12, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000085
Average total loss: 2.302670
tensor(-14.0890, device='cuda:0') tensor(1.7832e-06, device='cuda:0') tensor(7.6072e-12, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000085
Average total loss: 2.302670
tensor(-14.0927, device='cuda:0') tensor(1.7701e-06, device='cuda:0') tensor(7.5788e-12, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000085
Average total loss: 2.302669
tensor(-14.0964, device='cuda:0') tensor(1.7557e-06, device='cuda:0') tensor(7.5509e-12, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1001, device='cuda:0') tensor(1.7423e-06, device='cuda:0') tensor(7.5231e-12, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1038, device='cuda:0') tensor(1.7299e-06, device='cuda:0') tensor(7.4954e-12, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1042, device='cuda:0') tensor(1.7290e-06, device='cuda:0') tensor(7.4926e-12, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1046, device='cuda:0') tensor(1.7281e-06, device='cuda:0') tensor(7.4898e-12, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1049, device='cuda:0') tensor(1.7272e-06, device='cuda:0') tensor(7.4870e-12, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1053, device='cuda:0') tensor(1.7264e-06, device='cuda:0') tensor(7.4842e-12, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1057, device='cuda:0') tensor(1.7255e-06, device='cuda:0') tensor(7.4814e-12, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1061, device='cuda:0') tensor(1.7247e-06, device='cuda:0') tensor(7.4786e-12, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302669
tensor(-14.1064, device='cuda:0') tensor(1.7239e-06, device='cuda:0') tensor(7.4758e-12, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302668
tensor(-14.1068, device='cuda:0') tensor(1.7231e-06, device='cuda:0') tensor(7.4730e-12, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000084
Average total loss: 2.302668
tensor(-14.1072, device='cuda:0') tensor(1.7224e-06, device='cuda:0') tensor(7.4702e-12, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1075, device='cuda:0') tensor(1.7216e-06, device='cuda:0') tensor(7.4674e-12, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1079, device='cuda:0') tensor(1.7209e-06, device='cuda:0') tensor(7.4646e-12, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1080, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4642e-12, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1080, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4639e-12, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1081, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4635e-12, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1081, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4632e-12, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1082, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4629e-12, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1082, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4625e-12, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1082, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4622e-12, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1083, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4618e-12, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1083, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4615e-12, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000083
Average total loss: 2.302668
tensor(-14.1084, device='cuda:0') tensor(1.7208e-06, device='cuda:0') tensor(7.4611e-12, device='cuda:0')
