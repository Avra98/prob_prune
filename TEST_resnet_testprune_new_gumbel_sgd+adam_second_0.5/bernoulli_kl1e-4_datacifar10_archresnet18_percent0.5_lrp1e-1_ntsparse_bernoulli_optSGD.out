Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/12]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/100 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.5000e-05, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302701
Average KL loss: 175.345014
Average total loss: 177.647710
tensor(-3.5410, device='cuda:0') tensor(3.6329e-05, device='cuda:0') tensor(2.7375e-06, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302583
Average KL loss: 19.045783
Average total loss: 21.348366
tensor(-4.5139, device='cuda:0') tensor(2.8472e-05, device='cuda:0') tensor(1.0720e-06, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 9.321444
Average total loss: 11.624028
tensor(-5.0445, device='cuda:0') tensor(2.5220e-05, device='cuda:0') tensor(6.3626e-07, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302588
Average KL loss: 5.907176
Average total loss: 8.209764
tensor(-5.4350, device='cuda:0') tensor(2.4004e-05, device='cuda:0') tensor(4.3233e-07, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302590
Average KL loss: 4.165197
Average total loss: 6.467787
tensor(-5.7465, device='cuda:0') tensor(2.3219e-05, device='cuda:0') tensor(3.1738e-07, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302580
Average KL loss: 3.132825
Average total loss: 5.435405
tensor(-6.0063, device='cuda:0') tensor(2.2516e-05, device='cuda:0') tensor(2.4512e-07, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302589
Average KL loss: 2.461732
Average total loss: 4.764321
tensor(-6.2295, device='cuda:0') tensor(2.2062e-05, device='cuda:0') tensor(1.9627e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302587
Average KL loss: 1.996689
Average total loss: 4.299276
tensor(-6.4256, device='cuda:0') tensor(2.1662e-05, device='cuda:0') tensor(1.6144e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302584
Average KL loss: 1.658901
Average total loss: 3.961485
tensor(-6.6006, device='cuda:0') tensor(2.1463e-05, device='cuda:0') tensor(1.3559e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302584
Average KL loss: 1.404518
Average total loss: 3.707102
tensor(-6.7587, device='cuda:0') tensor(2.1390e-05, device='cuda:0') tensor(1.1580e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 1.207368
Average total loss: 3.509953
tensor(-6.9032, device='cuda:0') tensor(2.1283e-05, device='cuda:0') tensor(1.0025e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 1.050967
Average total loss: 3.353552
tensor(-7.0363, device='cuda:0') tensor(2.1191e-05, device='cuda:0') tensor(8.7782e-08, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302583
Average KL loss: 0.924471
Average total loss: 3.227054
tensor(-7.1598, device='cuda:0') tensor(2.1366e-05, device='cuda:0') tensor(7.7601e-08, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302587
Average KL loss: 0.820479
Average total loss: 3.123066
tensor(-7.2751, device='cuda:0') tensor(2.1426e-05, device='cuda:0') tensor(6.9166e-08, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302577
Average KL loss: 0.733786
Average total loss: 3.036363
tensor(-7.3832, device='cuda:0') tensor(2.1299e-05, device='cuda:0') tensor(6.2085e-08, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302587
Average KL loss: 0.660640
Average total loss: 2.963227
tensor(-7.4851, device='cuda:0') tensor(2.1203e-05, device='cuda:0') tensor(5.6076e-08, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.598268
Average total loss: 2.900853
tensor(-7.5815, device='cuda:0') tensor(2.1132e-05, device='cuda:0') tensor(5.0926e-08, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.544590
Average total loss: 2.847175
tensor(-7.6731, device='cuda:0') tensor(2.1127e-05, device='cuda:0') tensor(4.6473e-08, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302584
Average KL loss: 0.498012
Average total loss: 2.800596
tensor(-7.7604, device='cuda:0') tensor(2.1582e-05, device='cuda:0') tensor(4.2594e-08, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.457297
Average total loss: 2.759882
tensor(-7.8437, device='cuda:0') tensor(2.1428e-05, device='cuda:0') tensor(3.9191e-08, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.421470
Average total loss: 2.724055
tensor(-7.9235, device='cuda:0') tensor(2.1349e-05, device='cuda:0') tensor(3.6186e-08, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302586
Average KL loss: 0.389756
Average total loss: 2.692342
tensor(-8.0002, device='cuda:0') tensor(2.1014e-05, device='cuda:0') tensor(3.3518e-08, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302583
Average KL loss: 0.361529
Average total loss: 2.664112
tensor(-8.0739, device='cuda:0') tensor(2.0956e-05, device='cuda:0') tensor(3.1138e-08, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302589
Average KL loss: 0.336283
Average total loss: 2.638871
tensor(-8.1450, device='cuda:0') tensor(2.0826e-05, device='cuda:0') tensor(2.9003e-08, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.313599
Average total loss: 2.616184
tensor(-8.2136, device='cuda:0') tensor(2.0777e-05, device='cuda:0') tensor(2.7081e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.293134
Average total loss: 2.595719
tensor(-8.2799, device='cuda:0') tensor(2.0713e-05, device='cuda:0') tensor(2.5343e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.274597
Average total loss: 2.577182
tensor(-8.3442, device='cuda:0') tensor(2.0655e-05, device='cuda:0') tensor(2.3766e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.257749
Average total loss: 2.560334
tensor(-8.4066, device='cuda:0') tensor(2.0588e-05, device='cuda:0') tensor(2.2330e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.242385
Average total loss: 2.544970
tensor(-8.4671, device='cuda:0') tensor(2.0565e-05, device='cuda:0') tensor(2.1018e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.228331
Average total loss: 2.530916
tensor(-8.5260, device='cuda:0') tensor(2.0549e-05, device='cuda:0') tensor(1.9817e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.215438
Average total loss: 2.518023
tensor(-8.5834, device='cuda:0') tensor(2.0536e-05, device='cuda:0') tensor(1.8713e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302584
Average KL loss: 0.203580
Average total loss: 2.506163
tensor(-8.6392, device='cuda:0') tensor(2.0524e-05, device='cuda:0') tensor(1.7696e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.192645
Average total loss: 2.495230
tensor(-8.6938, device='cuda:0') tensor(2.0515e-05, device='cuda:0') tensor(1.6757e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.182539
Average total loss: 2.485124
tensor(-8.7470, device='cuda:0') tensor(2.0507e-05, device='cuda:0') tensor(1.5889e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302584
Average KL loss: 0.173177
Average total loss: 2.475762
tensor(-8.7991, device='cuda:0') tensor(2.0504e-05, device='cuda:0') tensor(1.5083e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.164487
Average total loss: 2.467072
tensor(-8.8500, device='cuda:0') tensor(2.0498e-05, device='cuda:0') tensor(1.4335e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.156406
Average total loss: 2.458991
tensor(-8.8998, device='cuda:0') tensor(2.0505e-05, device='cuda:0') tensor(1.3638e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.148877
Average total loss: 2.451462
tensor(-8.9486, device='cuda:0') tensor(2.0511e-05, device='cuda:0') tensor(1.2988e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302583
Average KL loss: 0.141849
Average total loss: 2.444431
tensor(-8.9965, device='cuda:0') tensor(2.0512e-05, device='cuda:0') tensor(1.2381e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.135279
Average total loss: 2.437863
tensor(-9.0435, device='cuda:0') tensor(2.0516e-05, device='cuda:0') tensor(1.1813e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.129126
Average total loss: 2.431711
tensor(-9.0896, device='cuda:0') tensor(2.0503e-05, device='cuda:0') tensor(1.1281e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.123356
Average total loss: 2.425941
tensor(-9.1349, device='cuda:0') tensor(2.0496e-05, device='cuda:0') tensor(1.0781e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.117937
Average total loss: 2.420523
tensor(-9.1794, device='cuda:0') tensor(2.0309e-05, device='cuda:0') tensor(1.0312e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302587
Average KL loss: 0.112842
Average total loss: 2.415429
tensor(-9.2232, device='cuda:0') tensor(2.0632e-05, device='cuda:0') tensor(9.8700e-09, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302586
Average KL loss: 0.108044
Average total loss: 2.410630
tensor(-9.2663, device='cuda:0') tensor(2.0703e-05, device='cuda:0') tensor(9.4539e-09, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.103521
Average total loss: 2.406106
tensor(-9.3088, device='cuda:0') tensor(2.0686e-05, device='cuda:0') tensor(9.0612e-09, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.099252
Average total loss: 2.401836
tensor(-9.3506, device='cuda:0') tensor(2.0670e-05, device='cuda:0') tensor(8.6903e-09, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.095218
Average total loss: 2.397803
tensor(-9.3917, device='cuda:0') tensor(2.0655e-05, device='cuda:0') tensor(8.3398e-09, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.091402
Average total loss: 2.393987
tensor(-9.4323, device='cuda:0') tensor(2.0640e-05, device='cuda:0') tensor(8.0080e-09, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.087789
Average total loss: 2.390374
tensor(-9.4724, device='cuda:0') tensor(2.0618e-05, device='cuda:0') tensor(7.6936e-09, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.084365
Average total loss: 2.386950
tensor(-9.5119, device='cuda:0') tensor(2.0604e-05, device='cuda:0') tensor(7.3955e-09, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.081116
Average total loss: 2.383701
tensor(-9.5509, device='cuda:0') tensor(2.0591e-05, device='cuda:0') tensor(7.1126e-09, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.078031
Average total loss: 2.380616
tensor(-9.5895, device='cuda:0') tensor(2.0578e-05, device='cuda:0') tensor(6.8439e-09, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.075100
Average total loss: 2.377685
tensor(-9.6275, device='cuda:0') tensor(2.0566e-05, device='cuda:0') tensor(6.5883e-09, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.072312
Average total loss: 2.374896
tensor(-9.6651, device='cuda:0') tensor(2.0554e-05, device='cuda:0') tensor(6.3452e-09, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.069657
Average total loss: 2.372242
tensor(-9.7023, device='cuda:0') tensor(2.0543e-05, device='cuda:0') tensor(6.1136e-09, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.067129
Average total loss: 2.369714
tensor(-9.7391, device='cuda:0') tensor(2.0532e-05, device='cuda:0') tensor(5.8930e-09, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.064718
Average total loss: 2.367303
tensor(-9.7754, device='cuda:0') tensor(2.0501e-05, device='cuda:0') tensor(5.6825e-09, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.062419
Average total loss: 2.365004
tensor(-9.8114, device='cuda:0') tensor(2.0490e-05, device='cuda:0') tensor(5.4817e-09, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.060223
Average total loss: 2.362808
tensor(-9.8470, device='cuda:0') tensor(2.0561e-05, device='cuda:0') tensor(5.2899e-09, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.058126
Average total loss: 2.360711
tensor(-9.8823, device='cuda:0') tensor(2.0551e-05, device='cuda:0') tensor(5.1066e-09, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.056121
Average total loss: 2.358706
tensor(-9.9172, device='cuda:0') tensor(2.0538e-05, device='cuda:0') tensor(4.9313e-09, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.054204
Average total loss: 2.356789
tensor(-9.9518, device='cuda:0') tensor(2.0527e-05, device='cuda:0') tensor(4.7637e-09, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.052369
Average total loss: 2.354955
tensor(-9.9861, device='cuda:0') tensor(2.0493e-05, device='cuda:0') tensor(4.6032e-09, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.050612
Average total loss: 2.353197
tensor(-10.0201, device='cuda:0') tensor(2.0480e-05, device='cuda:0') tensor(4.4494e-09, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.048928
Average total loss: 2.351513
tensor(-10.0538, device='cuda:0') tensor(2.0468e-05, device='cuda:0') tensor(4.3021e-09, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.047315
Average total loss: 2.349900
tensor(-10.0872, device='cuda:0') tensor(2.0457e-05, device='cuda:0') tensor(4.1608e-09, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.045767
Average total loss: 2.348352
tensor(-10.1203, device='cuda:0') tensor(2.0446e-05, device='cuda:0') tensor(4.0252e-09, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302580
Average KL loss: 0.044282
Average total loss: 2.346862
tensor(-10.1531, device='cuda:0') tensor(2.1017e-05, device='cuda:0') tensor(3.8952e-09, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.042856
Average total loss: 2.345441
tensor(-10.1857, device='cuda:0') tensor(2.1124e-05, device='cuda:0') tensor(3.7702e-09, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.041487
Average total loss: 2.344071
tensor(-10.2181, device='cuda:0') tensor(2.1067e-05, device='cuda:0') tensor(3.6502e-09, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.040171
Average total loss: 2.342756
tensor(-10.2502, device='cuda:0') tensor(2.1009e-05, device='cuda:0') tensor(3.5349e-09, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.038906
Average total loss: 2.341491
tensor(-10.2821, device='cuda:0') tensor(2.0958e-05, device='cuda:0') tensor(3.4240e-09, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.037689
Average total loss: 2.340274
tensor(-10.3137, device='cuda:0') tensor(2.0913e-05, device='cuda:0') tensor(3.3173e-09, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.036519
Average total loss: 2.339103
tensor(-10.3452, device='cuda:0') tensor(2.0888e-05, device='cuda:0') tensor(3.2146e-09, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.035392
Average total loss: 2.337977
tensor(-10.3764, device='cuda:0') tensor(2.0860e-05, device='cuda:0') tensor(3.1158e-09, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 0.034307
Average total loss: 2.336892
tensor(-10.4074, device='cuda:0') tensor(2.0826e-05, device='cuda:0') tensor(3.0206e-09, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.033263
Average total loss: 2.335848
tensor(-10.4382, device='cuda:0') tensor(2.0795e-05, device='cuda:0') tensor(2.9289e-09, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302583
Average KL loss: 0.032256
Average total loss: 2.334839
tensor(-10.4689, device='cuda:0') tensor(2.1489e-05, device='cuda:0') tensor(2.8406e-09, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.031286
Average total loss: 2.333871
tensor(-10.4993, device='cuda:0') tensor(2.1471e-05, device='cuda:0') tensor(2.7554e-09, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.030350
Average total loss: 2.332935
tensor(-10.5296, device='cuda:0') tensor(2.1330e-05, device='cuda:0') tensor(2.6733e-09, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.029448
Average total loss: 2.332033
tensor(-10.5597, device='cuda:0') tensor(2.1199e-05, device='cuda:0') tensor(2.5940e-09, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.028578
Average total loss: 2.331162
tensor(-10.5896, device='cuda:0') tensor(2.0849e-05, device='cuda:0') tensor(2.5176e-09, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 0.027737
Average total loss: 2.330322
tensor(-10.6194, device='cuda:0') tensor(2.0486e-05, device='cuda:0') tensor(2.4438e-09, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.026926
Average total loss: 2.329511
tensor(-10.6490, device='cuda:0') tensor(2.0474e-05, device='cuda:0') tensor(2.3725e-09, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 0.026143
Average total loss: 2.328728
tensor(-10.6784, device='cuda:0') tensor(2.0463e-05, device='cuda:0') tensor(2.3037e-09, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302585
Average KL loss: 0.025386
Average total loss: 2.327971
tensor(-10.7077, device='cuda:0') tensor(2.0453e-05, device='cuda:0') tensor(2.2372e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 0.024656
Average total loss: 2.327240
tensor(-10.7368, device='cuda:0') tensor(2.0442e-05, device='cuda:0') tensor(2.1729e-09, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.023949
Average total loss: 2.326534
tensor(-10.7658, device='cuda:0') tensor(2.0431e-05, device='cuda:0') tensor(2.1108e-09, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.023266
Average total loss: 2.325851
tensor(-10.7947, device='cuda:0') tensor(2.0420e-05, device='cuda:0') tensor(2.0508e-09, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.022606
Average total loss: 2.325191
tensor(-10.8234, device='cuda:0') tensor(2.0409e-05, device='cuda:0') tensor(1.9927e-09, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.021967
Average total loss: 2.324552
tensor(-10.8520, device='cuda:0') tensor(2.0398e-05, device='cuda:0') tensor(1.9365e-09, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302583
Average KL loss: 0.021349
Average total loss: 2.323932
tensor(-10.8805, device='cuda:0') tensor(2.0573e-05, device='cuda:0') tensor(1.8822e-09, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 0.020752
Average total loss: 2.323336
tensor(-10.9088, device='cuda:0') tensor(2.0558e-05, device='cuda:0') tensor(1.8296e-09, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 0.020173
Average total loss: 2.322758
tensor(-10.9370, device='cuda:0') tensor(2.0540e-05, device='cuda:0') tensor(1.7787e-09, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.019613
Average total loss: 2.322198
tensor(-10.9651, device='cuda:0') tensor(2.0522e-05, device='cuda:0') tensor(1.7294e-09, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 0.019071
Average total loss: 2.321655
tensor(-10.9931, device='cuda:0') tensor(2.0504e-05, device='cuda:0') tensor(1.6817e-09, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.018545
Average total loss: 2.321130
tensor(-11.0210, device='cuda:0') tensor(2.0507e-05, device='cuda:0') tensor(1.6355e-09, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 0.018037
Average total loss: 2.320622
tensor(-11.0487, device='cuda:0') tensor(2.0491e-05, device='cuda:0') tensor(1.5907e-09, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.017544
Average total loss: 2.320129
tensor(-11.0764, device='cuda:0') tensor(2.0473e-05, device='cuda:0') tensor(1.5474e-09, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 0.017066
Average total loss: 2.319651
tensor(-11.1039, device='cuda:0') tensor(2.0454e-05, device='cuda:0') tensor(1.5053e-09, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.016604
Average total loss: 2.319188
tensor(-11.1313, device='cuda:0') tensor(2.0436e-05, device='cuda:0') tensor(1.4646e-09, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.016155
Average total loss: 2.318740
tensor(-11.1587, device='cuda:0') tensor(2.0418e-05, device='cuda:0') tensor(1.4251e-09, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.015720
Average total loss: 2.318305
tensor(-11.1859, device='cuda:0') tensor(2.0400e-05, device='cuda:0') tensor(1.3868e-09, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.015298
Average total loss: 2.317883
tensor(-11.2131, device='cuda:0') tensor(2.0381e-05, device='cuda:0') tensor(1.3496e-09, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.014889
Average total loss: 2.317474
tensor(-11.2401, device='cuda:0') tensor(2.0363e-05, device='cuda:0') tensor(1.3136e-09, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.014492
Average total loss: 2.317077
tensor(-11.2671, device='cuda:0') tensor(2.0352e-05, device='cuda:0') tensor(1.2787e-09, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.014108
Average total loss: 2.316693
tensor(-11.2940, device='cuda:0') tensor(2.0334e-05, device='cuda:0') tensor(1.2448e-09, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.013734
Average total loss: 2.316319
tensor(-11.3207, device='cuda:0') tensor(2.0315e-05, device='cuda:0') tensor(1.2119e-09, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.013372
Average total loss: 2.315957
tensor(-11.3474, device='cuda:0') tensor(2.0296e-05, device='cuda:0') tensor(1.1800e-09, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.013020
Average total loss: 2.315605
tensor(-11.3741, device='cuda:0') tensor(2.0276e-05, device='cuda:0') tensor(1.1490e-09, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.012679
Average total loss: 2.315264
tensor(-11.4006, device='cuda:0') tensor(2.0257e-05, device='cuda:0') tensor(1.1189e-09, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.012347
Average total loss: 2.314932
tensor(-11.4270, device='cuda:0') tensor(2.0237e-05, device='cuda:0') tensor(1.0897e-09, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.012025
Average total loss: 2.314610
tensor(-11.4534, device='cuda:0') tensor(2.0217e-05, device='cuda:0') tensor(1.0613e-09, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.011713
Average total loss: 2.314298
tensor(-11.4797, device='cuda:0') tensor(2.0197e-05, device='cuda:0') tensor(1.0338e-09, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 0.011409
Average total loss: 2.313994
tensor(-11.5059, device='cuda:0') tensor(2.0177e-05, device='cuda:0') tensor(1.0071e-09, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 0.011115
Average total loss: 2.313700
tensor(-11.5320, device='cuda:0') tensor(2.0158e-05, device='cuda:0') tensor(9.8107e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.010828
Average total loss: 2.313413
tensor(-11.5581, device='cuda:0') tensor(2.0137e-05, device='cuda:0') tensor(9.5583e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.010550
Average total loss: 2.313135
tensor(-11.5841, device='cuda:0') tensor(2.0116e-05, device='cuda:0') tensor(9.3131e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.010280
Average total loss: 2.312865
tensor(-11.6100, device='cuda:0') tensor(2.0094e-05, device='cuda:0') tensor(9.0749e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.010017
Average total loss: 2.312602
tensor(-11.6358, device='cuda:0') tensor(2.0072e-05, device='cuda:0') tensor(8.8433e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.009762
Average total loss: 2.312347
tensor(-11.6616, device='cuda:0') tensor(2.0050e-05, device='cuda:0') tensor(8.6183e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.009514
Average total loss: 2.312099
tensor(-11.6873, device='cuda:0') tensor(2.0028e-05, device='cuda:0') tensor(8.3996e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302586
Average KL loss: 0.009273
Average total loss: 2.311858
tensor(-11.7130, device='cuda:0') tensor(1.9383e-05, device='cuda:0') tensor(8.1870e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 0.009038
Average total loss: 2.311623
tensor(-11.7385, device='cuda:0') tensor(1.9359e-05, device='cuda:0') tensor(7.9802e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.008810
Average total loss: 2.311395
tensor(-11.7640, device='cuda:0') tensor(1.9337e-05, device='cuda:0') tensor(7.7793e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.008589
Average total loss: 2.311174
tensor(-11.7895, device='cuda:0') tensor(1.9314e-05, device='cuda:0') tensor(7.5839e-10, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.008373
Average total loss: 2.310958
tensor(-11.8149, device='cuda:0') tensor(1.9291e-05, device='cuda:0') tensor(7.3938e-10, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.008164
Average total loss: 2.310748
tensor(-11.8402, device='cuda:0') tensor(1.9268e-05, device='cuda:0') tensor(7.2090e-10, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.007960
Average total loss: 2.310545
tensor(-11.8654, device='cuda:0') tensor(1.9244e-05, device='cuda:0') tensor(7.0293e-10, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.007762
Average total loss: 2.310347
tensor(-11.8906, device='cuda:0') tensor(1.9220e-05, device='cuda:0') tensor(6.8544e-10, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.007569
Average total loss: 2.310154
tensor(-11.9157, device='cuda:0') tensor(1.9196e-05, device='cuda:0') tensor(6.6844e-10, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 0.007381
Average total loss: 2.309966
tensor(-11.9408, device='cuda:0') tensor(1.9171e-05, device='cuda:0') tensor(6.5189e-10, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.007199
Average total loss: 2.309784
tensor(-11.9658, device='cuda:0') tensor(1.9146e-05, device='cuda:0') tensor(6.3579e-10, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.007021
Average total loss: 2.309606
tensor(-11.9908, device='cuda:0') tensor(1.9120e-05, device='cuda:0') tensor(6.2013e-10, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.006848
Average total loss: 2.309433
tensor(-12.0156, device='cuda:0') tensor(1.9094e-05, device='cuda:0') tensor(6.0488e-10, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.006680
Average total loss: 2.309265
tensor(-12.0405, device='cuda:0') tensor(1.9067e-05, device='cuda:0') tensor(5.9005e-10, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.006517
Average total loss: 2.309102
tensor(-12.0652, device='cuda:0') tensor(1.9040e-05, device='cuda:0') tensor(5.7562e-10, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.006357
Average total loss: 2.308942
tensor(-12.0899, device='cuda:0') tensor(1.9013e-05, device='cuda:0') tensor(5.6157e-10, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.006202
Average total loss: 2.308787
tensor(-12.1146, device='cuda:0') tensor(1.8984e-05, device='cuda:0') tensor(5.4789e-10, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.006052
Average total loss: 2.308636
tensor(-12.1392, device='cuda:0') tensor(1.8956e-05, device='cuda:0') tensor(5.3458e-10, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.005905
Average total loss: 2.308490
tensor(-12.1637, device='cuda:0') tensor(1.8927e-05, device='cuda:0') tensor(5.2162e-10, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.005762
Average total loss: 2.308347
tensor(-12.1882, device='cuda:0') tensor(1.8897e-05, device='cuda:0') tensor(5.0901e-10, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.005623
Average total loss: 2.308207
tensor(-12.2126, device='cuda:0') tensor(1.8867e-05, device='cuda:0') tensor(4.9673e-10, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.005487
Average total loss: 2.308072
tensor(-12.2370, device='cuda:0') tensor(1.8836e-05, device='cuda:0') tensor(4.8477e-10, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.005355
Average total loss: 2.307940
tensor(-12.2613, device='cuda:0') tensor(1.8805e-05, device='cuda:0') tensor(4.7313e-10, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.005227
Average total loss: 2.307812
tensor(-12.2856, device='cuda:0') tensor(1.8773e-05, device='cuda:0') tensor(4.6179e-10, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 0.005102
Average total loss: 2.307686
tensor(-12.3098, device='cuda:0') tensor(1.8740e-05, device='cuda:0') tensor(4.5075e-10, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.004980
Average total loss: 2.307565
tensor(-12.3339, device='cuda:0') tensor(1.8707e-05, device='cuda:0') tensor(4.4000e-10, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.004861
Average total loss: 2.307446
tensor(-12.3580, device='cuda:0') tensor(1.8673e-05, device='cuda:0') tensor(4.2953e-10, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.004746
Average total loss: 2.307330
tensor(-12.3820, device='cuda:0') tensor(1.8639e-05, device='cuda:0') tensor(4.1933e-10, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.004633
Average total loss: 2.307218
tensor(-12.4060, device='cuda:0') tensor(1.8604e-05, device='cuda:0') tensor(4.0940e-10, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.004523
Average total loss: 2.307108
tensor(-12.4299, device='cuda:0') tensor(1.8568e-05, device='cuda:0') tensor(3.9972e-10, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.004417
Average total loss: 2.307001
tensor(-12.4538, device='cuda:0') tensor(1.8532e-05, device='cuda:0') tensor(3.9030e-10, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.004313
Average total loss: 2.306897
tensor(-12.4776, device='cuda:0') tensor(1.8495e-05, device='cuda:0') tensor(3.8112e-10, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.004211
Average total loss: 2.306796
tensor(-12.5013, device='cuda:0') tensor(1.8457e-05, device='cuda:0') tensor(3.7217e-10, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.004113
Average total loss: 2.306697
tensor(-12.5250, device='cuda:0') tensor(1.8419e-05, device='cuda:0') tensor(3.6346e-10, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.004016
Average total loss: 2.306601
tensor(-12.5487, device='cuda:0') tensor(1.8379e-05, device='cuda:0') tensor(3.5497e-10, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.003923
Average total loss: 2.306507
tensor(-12.5722, device='cuda:0') tensor(1.8339e-05, device='cuda:0') tensor(3.4670e-10, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.003831
Average total loss: 2.306416
tensor(-12.5958, device='cuda:0') tensor(1.8299e-05, device='cuda:0') tensor(3.3864e-10, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.003742
Average total loss: 2.306327
tensor(-12.6192, device='cuda:0') tensor(1.8257e-05, device='cuda:0') tensor(3.3078e-10, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.003656
Average total loss: 2.306241
tensor(-12.6426, device='cuda:0') tensor(1.8215e-05, device='cuda:0') tensor(3.2313e-10, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.003571
Average total loss: 2.306156
tensor(-12.6660, device='cuda:0') tensor(1.8172e-05, device='cuda:0') tensor(3.1567e-10, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.003489
Average total loss: 2.306074
tensor(-12.6893, device='cuda:0') tensor(1.8128e-05, device='cuda:0') tensor(3.0841e-10, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.003409
Average total loss: 2.305994
tensor(-12.7125, device='cuda:0') tensor(1.8083e-05, device='cuda:0') tensor(3.0132e-10, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.003330
Average total loss: 2.305915
tensor(-12.7357, device='cuda:0') tensor(1.8038e-05, device='cuda:0') tensor(2.9442e-10, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.003254
Average total loss: 2.305839
tensor(-12.7588, device='cuda:0') tensor(1.7991e-05, device='cuda:0') tensor(2.8769e-10, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302585
Average KL loss: 0.003180
Average total loss: 2.305765
tensor(-12.7818, device='cuda:0') tensor(1.7944e-05, device='cuda:0') tensor(2.8114e-10, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.003108
Average total loss: 2.305693
tensor(-12.8048, device='cuda:0') tensor(1.7896e-05, device='cuda:0') tensor(2.7475e-10, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.003037
Average total loss: 2.305622
tensor(-12.8278, device='cuda:0') tensor(1.7847e-05, device='cuda:0') tensor(2.6852e-10, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.002968
Average total loss: 2.305553
tensor(-12.8506, device='cuda:0') tensor(1.7797e-05, device='cuda:0') tensor(2.6244e-10, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.002901
Average total loss: 2.305486
tensor(-12.8735, device='cuda:0') tensor(1.7747e-05, device='cuda:0') tensor(2.5652e-10, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.002836
Average total loss: 2.305421
tensor(-12.8962, device='cuda:0') tensor(1.7695e-05, device='cuda:0') tensor(2.5075e-10, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.002772
Average total loss: 2.305357
tensor(-12.9189, device='cuda:0') tensor(1.7643e-05, device='cuda:0') tensor(2.4513e-10, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302585
Average KL loss: 0.002710
Average total loss: 2.305295
tensor(-12.9415, device='cuda:0') tensor(1.7590e-05, device='cuda:0') tensor(2.3964e-10, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.002650
Average total loss: 2.305234
tensor(-12.9641, device='cuda:0') tensor(1.7535e-05, device='cuda:0') tensor(2.3430e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.002590
Average total loss: 2.305175
tensor(-12.9866, device='cuda:0') tensor(1.7480e-05, device='cuda:0') tensor(2.2908e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.002533
Average total loss: 2.305118
tensor(-13.0090, device='cuda:0') tensor(1.7424e-05, device='cuda:0') tensor(2.2400e-10, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.002477
Average total loss: 2.305062
tensor(-13.0314, device='cuda:0') tensor(1.7367e-05, device='cuda:0') tensor(2.1905e-10, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.002422
Average total loss: 2.305007
tensor(-13.0537, device='cuda:0') tensor(1.7309e-05, device='cuda:0') tensor(2.1421e-10, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.002369
Average total loss: 2.304954
tensor(-13.0760, device='cuda:0') tensor(1.7249e-05, device='cuda:0') tensor(2.0950e-10, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302585
Average KL loss: 0.002317
Average total loss: 2.304902
tensor(-13.0981, device='cuda:0') tensor(1.7189e-05, device='cuda:0') tensor(2.0491e-10, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302585
Average KL loss: 0.002266
Average total loss: 2.304851
tensor(-13.1202, device='cuda:0') tensor(1.7129e-05, device='cuda:0') tensor(2.0043e-10, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302585
Average KL loss: 0.002217
Average total loss: 2.304801
tensor(-13.1423, device='cuda:0') tensor(1.7066e-05, device='cuda:0') tensor(1.9606e-10, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302585
Average KL loss: 0.002168
Average total loss: 2.304753
tensor(-13.1643, device='cuda:0') tensor(1.7004e-05, device='cuda:0') tensor(1.9180e-10, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302585
Average KL loss: 0.002121
Average total loss: 2.304706
tensor(-13.1862, device='cuda:0') tensor(1.6940e-05, device='cuda:0') tensor(1.8764e-10, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302585
Average KL loss: 0.002075
Average total loss: 2.304660
tensor(-13.2080, device='cuda:0') tensor(1.6875e-05, device='cuda:0') tensor(1.8359e-10, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302585
Average KL loss: 0.002031
Average total loss: 2.304615
tensor(-13.2298, device='cuda:0') tensor(1.6809e-05, device='cuda:0') tensor(1.7964e-10, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302585
Average KL loss: 0.001987
Average total loss: 2.304572
tensor(-13.2514, device='cuda:0') tensor(1.6742e-05, device='cuda:0') tensor(1.7578e-10, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302585
Average KL loss: 0.001944
Average total loss: 2.304529
tensor(-13.2731, device='cuda:0') tensor(1.6674e-05, device='cuda:0') tensor(1.7202e-10, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302585
Average KL loss: 0.001903
Average total loss: 2.304488
tensor(-13.2946, device='cuda:0') tensor(1.6605e-05, device='cuda:0') tensor(1.6835e-10, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302585
Average KL loss: 0.001862
Average total loss: 2.304447
tensor(-13.3161, device='cuda:0') tensor(1.6535e-05, device='cuda:0') tensor(1.6478e-10, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302585
Average KL loss: 0.001823
Average total loss: 2.304408
tensor(-13.3375, device='cuda:0') tensor(1.6464e-05, device='cuda:0') tensor(1.6129e-10, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302585
Average KL loss: 0.001784
Average total loss: 2.304369
tensor(-13.3588, device='cuda:0') tensor(1.6392e-05, device='cuda:0') tensor(1.5788e-10, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302585
Average KL loss: 0.001747
Average total loss: 2.304332
tensor(-13.3801, device='cuda:0') tensor(1.6319e-05, device='cuda:0') tensor(1.5456e-10, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302585
Average KL loss: 0.001710
Average total loss: 2.304295
tensor(-13.4013, device='cuda:0') tensor(1.6245e-05, device='cuda:0') tensor(1.5132e-10, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302585
Average KL loss: 0.001674
Average total loss: 2.304259
tensor(-13.4224, device='cuda:0') tensor(1.6169e-05, device='cuda:0') tensor(1.4816e-10, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302585
Average KL loss: 0.001639
Average total loss: 2.304224
tensor(-13.4434, device='cuda:0') tensor(1.6093e-05, device='cuda:0') tensor(1.4508e-10, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302585
Average KL loss: 0.001605
Average total loss: 2.304190
tensor(-13.4643, device='cuda:0') tensor(1.6016e-05, device='cuda:0') tensor(1.4207e-10, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302585
Average KL loss: 0.001572
Average total loss: 2.304157
 Percentile value: -13.485241889953613
Non-zero model percentage: 49.999996185302734%, Non-zero mask percentage: 49.999996185302734%

--- Pruning Level [1/12]: ---
conv1.weight         | nonzeros =     309 /    1728             ( 17.88%) | total_pruned =    1419 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
bn1.bias             | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3484 /   36864             (  9.45%) | total_pruned =   33380 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6688 /   36864             ( 18.14%) | total_pruned =   30176 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6818 /   36864             ( 18.50%) | total_pruned =   30046 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7402 /   36864             ( 20.08%) | total_pruned =   29462 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20151 /   73728             ( 27.33%) | total_pruned =   53577 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   33885 /  147456             ( 22.98%) | total_pruned =  113571 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2232 /    8192             ( 27.25%) | total_pruned =    5960 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   15646 /  147456             ( 10.61%) | total_pruned =  131810 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   11822 /  147456             (  8.02%) | total_pruned =  135634 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   67209 /  294912             ( 22.79%) | total_pruned =  227703 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   67108 /  589824             ( 11.38%) | total_pruned =  522716 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4258 /   32768             ( 12.99%) | total_pruned =   28510 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   25845 /  589824             (  4.38%) | total_pruned =  563979 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   18804 /  589824             (  3.19%) | total_pruned =  571020 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   88441 / 1179648             (  7.50%) | total_pruned = 1091207 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 1042142 / 2359296             ( 44.17%) | total_pruned = 1317154 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     275 /     512             ( 53.71%) | total_pruned =     237 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     328 /     512             ( 64.06%) | total_pruned =     184 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  117151 /  131072             ( 89.38%) | total_pruned =   13921 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     334 /     512             ( 65.23%) | total_pruned =     178 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2004103 / 2359296             ( 84.94%) | total_pruned =  355193 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2035652 / 2359296             ( 86.28%) | total_pruned =  323644 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3201 /    5120             ( 62.52%) | total_pruned =    1919 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 5589381, pruned : 5589381, total: 11178762, Compression rate :       2.00x  ( 50.00% pruned)
Train Epoch: 64/100 Loss: 0.018540 Accuracy: 84.70 100.00 % Best test Accuracy: 84.70%
tensor(-13.4852, device='cuda:0') tensor(1.7025e-05, device='cuda:0') tensor(1.3914e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001506
Average total loss: 2.304091
tensor(-13.5509, device='cuda:0') tensor(1.3624e-05, device='cuda:0') tensor(1.3030e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001413
Average total loss: 2.303998
tensor(-13.6127, device='cuda:0') tensor(1.1317e-05, device='cuda:0') tensor(1.2249e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001331
Average total loss: 2.303915
tensor(-13.6709, device='cuda:0') tensor(9.6060e-06, device='cuda:0') tensor(1.1556e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001257
Average total loss: 2.303842
tensor(-13.7259, device='cuda:0') tensor(8.2803e-06, device='cuda:0') tensor(1.0937e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001192
Average total loss: 2.303777
tensor(-13.7780, device='cuda:0') tensor(7.2244e-06, device='cuda:0') tensor(1.0382e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001133
Average total loss: 2.303718
tensor(-13.8275, device='cuda:0') tensor(6.3661e-06, device='cuda:0') tensor(9.8804e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001079
Average total loss: 2.303664
tensor(-13.8747, device='cuda:0') tensor(5.6569e-06, device='cuda:0') tensor(9.4251e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001031
Average total loss: 2.303615
tensor(-13.9198, device='cuda:0') tensor(5.0637e-06, device='cuda:0') tensor(9.0099e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000986
Average total loss: 2.303571
tensor(-13.9629, device='cuda:0') tensor(4.5614e-06, device='cuda:0') tensor(8.6298e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000945
Average total loss: 2.303530
tensor(-14.0042, device='cuda:0') tensor(4.1321e-06, device='cuda:0') tensor(8.2805e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000908
Average total loss: 2.303493
tensor(-14.0439, device='cuda:0') tensor(3.7619e-06, device='cuda:0') tensor(7.9585e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000873
Average total loss: 2.303458
tensor(-14.0820, device='cuda:0') tensor(3.4402e-06, device='cuda:0') tensor(7.6605e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000841
Average total loss: 2.303426
tensor(-14.1188, device='cuda:0') tensor(3.1588e-06, device='cuda:0') tensor(7.3841e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000811
Average total loss: 2.303396
tensor(-14.1542, device='cuda:0') tensor(2.9111e-06, device='cuda:0') tensor(7.1269e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000783
Average total loss: 2.303368
tensor(-14.1885, device='cuda:0') tensor(2.6919e-06, device='cuda:0') tensor(6.8871e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000757
Average total loss: 2.303342
tensor(-14.2215, device='cuda:0') tensor(2.4969e-06, device='cuda:0') tensor(6.6628e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000733
Average total loss: 2.303318
tensor(-14.2536, device='cuda:0') tensor(2.3226e-06, device='cuda:0') tensor(6.4528e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000710
Average total loss: 2.303295
tensor(-14.2846, device='cuda:0') tensor(2.1662e-06, device='cuda:0') tensor(6.2555e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000689
Average total loss: 2.303274
tensor(-14.3147, device='cuda:0') tensor(2.0252e-06, device='cuda:0') tensor(6.0700e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000669
Average total loss: 2.303254
tensor(-14.3440, device='cuda:0') tensor(1.8978e-06, device='cuda:0') tensor(5.8951e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000650
Average total loss: 2.303235
tensor(-14.3724, device='cuda:0') tensor(1.7821e-06, device='cuda:0') tensor(5.7301e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000632
Average total loss: 2.303217
tensor(-14.4000, device='cuda:0') tensor(1.6769e-06, device='cuda:0') tensor(5.5740e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000615
Average total loss: 2.303200
tensor(-14.4269, device='cuda:0') tensor(1.5807e-06, device='cuda:0') tensor(5.4262e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000599
Average total loss: 2.303184
tensor(-14.4530, device='cuda:0') tensor(1.4926e-06, device='cuda:0') tensor(5.2860e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000584
Average total loss: 2.303168
tensor(-14.4785, device='cuda:0') tensor(1.4118e-06, device='cuda:0') tensor(5.1529e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000569
Average total loss: 2.303154
tensor(-14.5034, device='cuda:0') tensor(1.3374e-06, device='cuda:0') tensor(5.0264e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000555
Average total loss: 2.303140
tensor(-14.5277, device='cuda:0') tensor(1.2688e-06, device='cuda:0') tensor(4.9059e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000542
Average total loss: 2.303127
tensor(-14.5514, device='cuda:0') tensor(1.2054e-06, device='cuda:0') tensor(4.7910e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000535
Average total loss: 2.303120
tensor(-14.5537, device='cuda:0') tensor(1.1995e-06, device='cuda:0') tensor(4.7798e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000534
Average total loss: 2.303119
tensor(-14.5560, device='cuda:0') tensor(1.1937e-06, device='cuda:0') tensor(4.7687e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000532
Average total loss: 2.303117
tensor(-14.5584, device='cuda:0') tensor(1.1881e-06, device='cuda:0') tensor(4.7575e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000531
Average total loss: 2.303116
tensor(-14.5607, device='cuda:0') tensor(1.1822e-06, device='cuda:0') tensor(4.7465e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000530
Average total loss: 2.303115
tensor(-14.5630, device='cuda:0') tensor(1.1760e-06, device='cuda:0') tensor(4.7357e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000529
Average total loss: 2.303114
tensor(-14.5653, device='cuda:0') tensor(1.1699e-06, device='cuda:0') tensor(4.7248e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000528
Average total loss: 2.303112
tensor(-14.5676, device='cuda:0') tensor(1.1639e-06, device='cuda:0') tensor(4.7140e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000526
Average total loss: 2.303111
tensor(-14.5698, device='cuda:0') tensor(1.1582e-06, device='cuda:0') tensor(4.7032e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000525
Average total loss: 2.303110
tensor(-14.5721, device='cuda:0') tensor(1.1525e-06, device='cuda:0') tensor(4.6925e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000524
Average total loss: 2.303109
tensor(-14.5744, device='cuda:0') tensor(1.1470e-06, device='cuda:0') tensor(4.6818e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000523
Average total loss: 2.303108
tensor(-14.5767, device='cuda:0') tensor(1.1416e-06, device='cuda:0') tensor(4.6710e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5769, device='cuda:0') tensor(1.1412e-06, device='cuda:0') tensor(4.6700e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5772, device='cuda:0') tensor(1.1408e-06, device='cuda:0') tensor(4.6689e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5774, device='cuda:0') tensor(1.1405e-06, device='cuda:0') tensor(4.6678e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5776, device='cuda:0') tensor(1.1400e-06, device='cuda:0') tensor(4.6667e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000522
Average total loss: 2.303107
tensor(-14.5779, device='cuda:0') tensor(1.1397e-06, device='cuda:0') tensor(4.6656e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5781, device='cuda:0') tensor(1.1392e-06, device='cuda:0') tensor(4.6645e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5783, device='cuda:0') tensor(1.1389e-06, device='cuda:0') tensor(4.6634e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5786, device='cuda:0') tensor(1.1384e-06, device='cuda:0') tensor(4.6623e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5788, device='cuda:0') tensor(1.1381e-06, device='cuda:0') tensor(4.6612e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5791, device='cuda:0') tensor(1.1376e-06, device='cuda:0') tensor(4.6601e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1373e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1365e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1358e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1350e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1343e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1335e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1328e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1320e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1313e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1305e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1298e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.000521
Average total loss: 2.303106
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
 Percentile value: -14.579291343688965
Non-zero model percentage: 25.000003814697266%, Non-zero mask percentage: 25.000003814697266%

--- Pruning Level [2/12]: ---
conv1.weight         | nonzeros =     309 /    1728             ( 17.88%) | total_pruned =    1419 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
bn1.bias             | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3484 /   36864             (  9.45%) | total_pruned =   33380 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    6688 /   36864             ( 18.14%) | total_pruned =   30176 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    6818 /   36864             ( 18.50%) | total_pruned =   30046 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7402 /   36864             ( 20.08%) | total_pruned =   29462 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      32 /      64             ( 50.00%) | total_pruned =      32 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20151 /   73728             ( 27.33%) | total_pruned =   53577 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   33885 /  147456             ( 22.98%) | total_pruned =  113571 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2232 /    8192             ( 27.25%) | total_pruned =    5960 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      64 /     128             ( 50.00%) | total_pruned =      64 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   15646 /  147456             ( 10.61%) | total_pruned =  131810 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      65 /     128             ( 50.78%) | total_pruned =      63 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   11822 /  147456             (  8.02%) | total_pruned =  135634 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   67209 /  294912             ( 22.79%) | total_pruned =  227703 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   67108 /  589824             ( 11.38%) | total_pruned =  522716 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     151 /     256             ( 58.98%) | total_pruned =     105 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    4258 /   32768             ( 12.99%) | total_pruned =   28510 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   25845 /  589824             (  4.38%) | total_pruned =  563979 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   18804 /  589824             (  3.19%) | total_pruned =  571020 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   88441 / 1179648             (  7.50%) | total_pruned = 1091207 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     248 /     512             ( 48.44%) | total_pruned =     264 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  180105 / 2359296             (  7.63%) | total_pruned = 2179191 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     235 /     512             ( 45.90%) | total_pruned =     277 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     284 /     512             ( 55.47%) | total_pruned =     228 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   10608 /  131072             (  8.09%) | total_pruned =  120464 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     292 /     512             ( 57.03%) | total_pruned =     220 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  280639 / 2359296             ( 11.90%) | total_pruned = 2078657 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     244 /     512             ( 47.66%) | total_pruned =     268 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1934766 / 2359296             ( 82.01%) | total_pruned =  424530 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3201 /    5120             ( 62.52%) | total_pruned =    1919 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 2794691, pruned : 8384071, total: 11178762, Compression rate :       4.00x  ( 75.00% pruned)
Train Epoch: 58/100 Loss: 0.026148 Accuracy: 83.51 100.00 % Best test Accuracy: 84.47%
tensor(-14.5793, device='cuda:0') tensor(1.1290e-06, device='cuda:0') tensor(4.6591e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000515
Average total loss: 2.303100
tensor(-14.6018, device='cuda:0') tensor(1.0748e-06, device='cuda:0') tensor(4.5552e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000504
Average total loss: 2.303089
tensor(-14.6239, device='cuda:0') tensor(1.0243e-06, device='cuda:0') tensor(4.4558e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000493
Average total loss: 2.303078
tensor(-14.6455, device='cuda:0') tensor(9.7727e-07, device='cuda:0') tensor(4.3606e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000482
Average total loss: 2.303067
tensor(-14.6666, device='cuda:0') tensor(9.3345e-07, device='cuda:0') tensor(4.2694e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000472
Average total loss: 2.303057
tensor(-14.6873, device='cuda:0') tensor(8.9251e-07, device='cuda:0') tensor(4.1820e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000463
Average total loss: 2.303048
tensor(-14.7076, device='cuda:0') tensor(8.5424e-07, device='cuda:0') tensor(4.0980e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000454
Average total loss: 2.303039
tensor(-14.7275, device='cuda:0') tensor(8.1836e-07, device='cuda:0') tensor(4.0174e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000445
Average total loss: 2.303030
tensor(-14.7469, device='cuda:0') tensor(7.8468e-07, device='cuda:0') tensor(3.9399e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000436
Average total loss: 2.303021
tensor(-14.7661, device='cuda:0') tensor(7.5309e-07, device='cuda:0') tensor(3.8653e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000428
Average total loss: 2.303013
tensor(-14.7848, device='cuda:0') tensor(7.2339e-07, device='cuda:0') tensor(3.7935e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000420
Average total loss: 2.303005
tensor(-14.8032, device='cuda:0') tensor(6.9539e-07, device='cuda:0') tensor(3.7243e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000413
Average total loss: 2.302998
tensor(-14.8213, device='cuda:0') tensor(6.6898e-07, device='cuda:0') tensor(3.6576e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000409
Average total loss: 2.302993
tensor(-14.8231, device='cuda:0') tensor(6.6631e-07, device='cuda:0') tensor(3.6511e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000408
Average total loss: 2.302993
tensor(-14.8248, device='cuda:0') tensor(6.6373e-07, device='cuda:0') tensor(3.6446e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000407
Average total loss: 2.302992
tensor(-14.8266, device='cuda:0') tensor(6.6121e-07, device='cuda:0') tensor(3.6382e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000406
Average total loss: 2.302991
tensor(-14.8284, device='cuda:0') tensor(6.5876e-07, device='cuda:0') tensor(3.6317e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000406
Average total loss: 2.302991
tensor(-14.8302, device='cuda:0') tensor(6.5637e-07, device='cuda:0') tensor(3.6253e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000405
Average total loss: 2.302990
tensor(-14.8319, device='cuda:0') tensor(6.5404e-07, device='cuda:0') tensor(3.6188e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000404
Average total loss: 2.302989
tensor(-14.8337, device='cuda:0') tensor(6.5177e-07, device='cuda:0') tensor(3.6124e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000403
Average total loss: 2.302988
tensor(-14.8355, device='cuda:0') tensor(6.4954e-07, device='cuda:0') tensor(3.6060e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000403
Average total loss: 2.302988
tensor(-14.8373, device='cuda:0') tensor(6.4736e-07, device='cuda:0') tensor(3.5996e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000402
Average total loss: 2.302987
tensor(-14.8390, device='cuda:0') tensor(6.4520e-07, device='cuda:0') tensor(3.5932e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8408, device='cuda:0') tensor(6.4284e-07, device='cuda:0') tensor(3.5869e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8410, device='cuda:0') tensor(6.4278e-07, device='cuda:0') tensor(3.5862e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8412, device='cuda:0') tensor(6.4272e-07, device='cuda:0') tensor(3.5855e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8414, device='cuda:0') tensor(6.4267e-07, device='cuda:0') tensor(3.5849e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302986
tensor(-14.8416, device='cuda:0') tensor(6.4261e-07, device='cuda:0') tensor(3.5842e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302985
tensor(-14.8417, device='cuda:0') tensor(6.4256e-07, device='cuda:0') tensor(3.5835e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000401
Average total loss: 2.302985
tensor(-14.8419, device='cuda:0') tensor(6.4250e-07, device='cuda:0') tensor(3.5829e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8421, device='cuda:0') tensor(6.4244e-07, device='cuda:0') tensor(3.5822e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8423, device='cuda:0') tensor(6.4239e-07, device='cuda:0') tensor(3.5815e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8425, device='cuda:0') tensor(6.4233e-07, device='cuda:0') tensor(3.5809e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8427, device='cuda:0') tensor(6.4228e-07, device='cuda:0') tensor(3.5802e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4222e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4221e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4220e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000400
Average total loss: 2.302985
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
 Percentile value: -14.842873573303223
Non-zero model percentage: 12.500005722045898%, Non-zero mask percentage: 12.500005722045898%

--- Pruning Level [3/12]: ---
conv1.weight         | nonzeros =     288 /    1728             ( 16.67%) | total_pruned =    1440 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2452 /   36864             (  6.65%) | total_pruned =   34412 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4335 /   36864             ( 11.76%) | total_pruned =   32529 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4291 /   36864             ( 11.64%) | total_pruned =   32573 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4506 /   36864             ( 12.22%) | total_pruned =   32358 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12916 /   73728             ( 17.52%) | total_pruned =   60812 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   20480 /  147456             ( 13.89%) | total_pruned =  126976 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1570 /    8192             ( 19.17%) | total_pruned =    6622 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    6843 /  147456             (  4.64%) | total_pruned =  140613 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    5160 /  147456             (  3.50%) | total_pruned =  142296 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   40669 /  294912             ( 13.79%) | total_pruned =  254243 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     118 /     256             ( 46.09%) | total_pruned =     138 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   38034 /  589824             (  6.45%) | total_pruned =  551790 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2467 /   32768             (  7.53%) | total_pruned =   30301 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   10917 /  589824             (  1.85%) | total_pruned =  578907 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      87 /     256             ( 33.98%) | total_pruned =     169 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    7832 /  589824             (  1.33%) | total_pruned =  581992 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47161 / 1179648             (  4.00%) | total_pruned = 1132487 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     213 /     512             ( 41.60%) | total_pruned =     299 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   90654 / 2359296             (  3.84%) | total_pruned = 2268642 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     223 /     512             ( 43.55%) | total_pruned =     289 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    5397 /  131072             (  4.12%) | total_pruned =  125675 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     183 /     512             ( 35.74%) | total_pruned =     329 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  143646 / 2359296             (  6.09%) | total_pruned = 2215650 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     228 /     512             ( 44.53%) | total_pruned =     284 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  939524 / 2359296             ( 39.82%) | total_pruned = 1419772 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3201 /    5120             ( 62.52%) | total_pruned =    1919 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1397346, pruned : 9781416, total: 11178762, Compression rate :       8.00x  ( 87.50% pruned)
Train Epoch: 76/100 Loss: 0.048534 Accuracy: 80.70 99.99 % Best test Accuracy: 82.05%
tensor(-14.8429, device='cuda:0') tensor(6.4219e-07, device='cuda:0') tensor(3.5795e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000397
Average total loss: 2.302982
tensor(-14.8602, device='cuda:0') tensor(6.1885e-07, device='cuda:0') tensor(3.5179e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000390
Average total loss: 2.302975
tensor(-14.8773, device='cuda:0') tensor(5.9671e-07, device='cuda:0') tensor(3.4583e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000383
Average total loss: 2.302968
tensor(-14.8941, device='cuda:0') tensor(5.7571e-07, device='cuda:0') tensor(3.4007e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000377
Average total loss: 2.302962
tensor(-14.9106, device='cuda:0') tensor(5.5582e-07, device='cuda:0') tensor(3.3450e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000371
Average total loss: 2.302956
tensor(-14.9269, device='cuda:0') tensor(5.3694e-07, device='cuda:0') tensor(3.2910e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000365
Average total loss: 2.302950
tensor(-14.9429, device='cuda:0') tensor(5.1902e-07, device='cuda:0') tensor(3.2388e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000359
Average total loss: 2.302944
tensor(-14.9586, device='cuda:0') tensor(5.0196e-07, device='cuda:0') tensor(3.1882e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000354
Average total loss: 2.302939
tensor(-14.9741, device='cuda:0') tensor(4.8577e-07, device='cuda:0') tensor(3.1392e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000348
Average total loss: 2.302933
tensor(-14.9894, device='cuda:0') tensor(4.7033e-07, device='cuda:0') tensor(3.0917e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000343
Average total loss: 2.302928
tensor(-15.0044, device='cuda:0') tensor(4.5563e-07, device='cuda:0') tensor(3.0455e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000338
Average total loss: 2.302923
tensor(-15.0192, device='cuda:0') tensor(4.4162e-07, device='cuda:0') tensor(3.0008e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000333
Average total loss: 2.302918
tensor(-15.0338, device='cuda:0') tensor(4.2821e-07, device='cuda:0') tensor(2.9573e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000330
Average total loss: 2.302915
tensor(-15.0353, device='cuda:0') tensor(4.2699e-07, device='cuda:0') tensor(2.9530e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000330
Average total loss: 2.302915
tensor(-15.0367, device='cuda:0') tensor(4.2580e-07, device='cuda:0') tensor(2.9488e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000329
Average total loss: 2.302914
tensor(-15.0382, device='cuda:0') tensor(4.2465e-07, device='cuda:0') tensor(2.9445e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000329
Average total loss: 2.302914
tensor(-15.0396, device='cuda:0') tensor(4.2351e-07, device='cuda:0') tensor(2.9402e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-15.0411, device='cuda:0') tensor(4.2240e-07, device='cuda:0') tensor(2.9360e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000328
Average total loss: 2.302913
tensor(-15.0425, device='cuda:0') tensor(4.2131e-07, device='cuda:0') tensor(2.9317e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302912
tensor(-15.0440, device='cuda:0') tensor(4.2024e-07, device='cuda:0') tensor(2.9275e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302912
tensor(-15.0454, device='cuda:0') tensor(4.1919e-07, device='cuda:0') tensor(2.9232e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000327
Average total loss: 2.302911
tensor(-15.0468, device='cuda:0') tensor(4.1814e-07, device='cuda:0') tensor(2.9190e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000326
Average total loss: 2.302911
tensor(-15.0483, device='cuda:0') tensor(4.1681e-07, device='cuda:0') tensor(2.9148e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000326
Average total loss: 2.302911
tensor(-15.0497, device='cuda:0') tensor(4.1528e-07, device='cuda:0') tensor(2.9108e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0498, device='cuda:0') tensor(4.1522e-07, device='cuda:0') tensor(2.9104e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0500, device='cuda:0') tensor(4.1516e-07, device='cuda:0') tensor(2.9099e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0501, device='cuda:0') tensor(4.1510e-07, device='cuda:0') tensor(2.9095e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0502, device='cuda:0') tensor(4.1504e-07, device='cuda:0') tensor(2.9091e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0504, device='cuda:0') tensor(4.1498e-07, device='cuda:0') tensor(2.9087e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0505, device='cuda:0') tensor(4.1492e-07, device='cuda:0') tensor(2.9083e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0507, device='cuda:0') tensor(4.1486e-07, device='cuda:0') tensor(2.9079e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0508, device='cuda:0') tensor(4.1480e-07, device='cuda:0') tensor(2.9075e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0509, device='cuda:0') tensor(4.1474e-07, device='cuda:0') tensor(2.9071e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0511, device='cuda:0') tensor(4.1468e-07, device='cuda:0') tensor(2.9067e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000325
Average total loss: 2.302910
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
 Percentile value: -15.051224708557129
Non-zero model percentage: 6.250002861022949%, Non-zero mask percentage: 6.250002861022949%

--- Pruning Level [4/12]: ---
conv1.weight         | nonzeros =     288 /    1728             ( 16.67%) | total_pruned =    1440 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2452 /   36864             (  6.65%) | total_pruned =   34412 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    4335 /   36864             ( 11.76%) | total_pruned =   32529 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    4291 /   36864             ( 11.64%) | total_pruned =   32573 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    4506 /   36864             ( 12.22%) | total_pruned =   32358 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   12916 /   73728             ( 17.52%) | total_pruned =   60812 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   20480 /  147456             ( 13.89%) | total_pruned =  126976 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1570 /    8192             ( 19.17%) | total_pruned =    6622 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    6843 /  147456             (  4.64%) | total_pruned =  140613 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    5160 /  147456             (  3.50%) | total_pruned =  142296 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   40669 /  294912             ( 13.79%) | total_pruned =  254243 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     118 /     256             ( 46.09%) | total_pruned =     138 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   38034 /  589824             (  6.45%) | total_pruned =  551790 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     143 /     256             ( 55.86%) | total_pruned =     113 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2467 /   32768             (  7.53%) | total_pruned =   30301 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   10917 /  589824             (  1.85%) | total_pruned =  578907 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      87 /     256             ( 33.98%) | total_pruned =     169 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    7832 /  589824             (  1.33%) | total_pruned =  581992 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   47161 / 1179648             (  4.00%) | total_pruned = 1132487 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     213 /     512             ( 41.60%) | total_pruned =     299 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   90654 / 2359296             (  3.84%) | total_pruned = 2268642 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     223 /     512             ( 43.55%) | total_pruned =     289 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     269 /     512             ( 52.54%) | total_pruned =     243 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    5397 /  131072             (  4.12%) | total_pruned =  125675 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     183 /     512             ( 35.74%) | total_pruned =     329 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  143646 / 2359296             (  6.09%) | total_pruned = 2215650 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     228 /     512             ( 44.53%) | total_pruned =     284 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  240851 / 2359296             ( 10.21%) | total_pruned = 2118445 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3201 /    5120             ( 62.52%) | total_pruned =    1919 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 698673, pruned : 10480089, total: 11178762, Compression rate :      16.00x  ( 93.75% pruned)
Train Epoch: 85/100 Loss: 0.076897 Accuracy: 80.71 99.97 % Best test Accuracy: 81.50%
tensor(-15.0512, device='cuda:0') tensor(4.1463e-07, device='cuda:0') tensor(2.9063e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000323
Average total loss: 2.302908
tensor(-15.0654, device='cuda:0') tensor(4.0249e-07, device='cuda:0') tensor(2.8655e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000318
Average total loss: 2.302903
tensor(-15.0793, device='cuda:0') tensor(3.9085e-07, device='cuda:0') tensor(2.8258e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000314
Average total loss: 2.302899
tensor(-15.0930, device='cuda:0') tensor(3.7969e-07, device='cuda:0') tensor(2.7873e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000310
Average total loss: 2.302894
tensor(-15.1066, device='cuda:0') tensor(3.6903e-07, device='cuda:0') tensor(2.7497e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000305
Average total loss: 2.302890
tensor(-15.1200, device='cuda:0') tensor(3.5881e-07, device='cuda:0') tensor(2.7132e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000301
Average total loss: 2.302886
tensor(-15.1332, device='cuda:0') tensor(3.4899e-07, device='cuda:0') tensor(2.6776e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000297
Average total loss: 2.302882
tensor(-15.1462, device='cuda:0') tensor(3.3957e-07, device='cuda:0') tensor(2.6429e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000294
Average total loss: 2.302878
tensor(-15.1591, device='cuda:0') tensor(3.3055e-07, device='cuda:0') tensor(2.6091e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000290
Average total loss: 2.302875
tensor(-15.1718, device='cuda:0') tensor(3.2185e-07, device='cuda:0') tensor(2.5762e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000286
Average total loss: 2.302871
tensor(-15.1843, device='cuda:0') tensor(3.1352e-07, device='cuda:0') tensor(2.5441e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000283
Average total loss: 2.302868
tensor(-15.1967, device='cuda:0') tensor(3.0552e-07, device='cuda:0') tensor(2.5128e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000279
Average total loss: 2.302864
tensor(-15.2089, device='cuda:0') tensor(2.9780e-07, device='cuda:0') tensor(2.4822e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2102, device='cuda:0') tensor(2.9709e-07, device='cuda:0') tensor(2.4792e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2114, device='cuda:0') tensor(2.9641e-07, device='cuda:0') tensor(2.4762e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000277
Average total loss: 2.302862
tensor(-15.2126, device='cuda:0') tensor(2.9575e-07, device='cuda:0') tensor(2.4732e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2138, device='cuda:0') tensor(2.9510e-07, device='cuda:0') tensor(2.4702e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2150, device='cuda:0') tensor(2.9446e-07, device='cuda:0') tensor(2.4672e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000276
Average total loss: 2.302861
tensor(-15.2162, device='cuda:0') tensor(2.9384e-07, device='cuda:0') tensor(2.4642e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2174, device='cuda:0') tensor(2.9323e-07, device='cuda:0') tensor(2.4612e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2187, device='cuda:0') tensor(2.9264e-07, device='cuda:0') tensor(2.4582e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000275
Average total loss: 2.302860
tensor(-15.2199, device='cuda:0') tensor(2.9205e-07, device='cuda:0') tensor(2.4552e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2211, device='cuda:0') tensor(2.9148e-07, device='cuda:0') tensor(2.4522e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2223, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4493e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2224, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4489e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2226, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4486e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2227, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4482e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2229, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4479e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302859
tensor(-15.2230, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4475e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2231, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4472e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2233, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4469e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000274
Average total loss: 2.302858
tensor(-15.2234, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4465e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2236, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4462e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2237, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4458e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000273
Average total loss: 2.302858
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
 Percentile value: -15.22383975982666
Non-zero model percentage: 3.1250059604644775%, Non-zero mask percentage: 3.1250059604644775%

--- Pruning Level [5/12]: ---
conv1.weight         | nonzeros =     275 /    1728             ( 15.91%) | total_pruned =    1453 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1766 /   36864             (  4.79%) | total_pruned =   35098 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3055 /   36864             (  8.29%) | total_pruned =   33809 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    2814 /   36864             (  7.63%) | total_pruned =   34050 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    2939 /   36864             (  7.97%) | total_pruned =   33925 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    8475 /   73728             ( 11.49%) | total_pruned =   65253 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      56 /     128             ( 43.75%) | total_pruned =      72 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   12979 /  147456             (  8.80%) | total_pruned =  134477 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1174 /    8192             ( 14.33%) | total_pruned =    7018 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    3412 /  147456             (  2.31%) | total_pruned =  144044 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2588 /  147456             (  1.76%) | total_pruned =  144868 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      55 /     128             ( 42.97%) | total_pruned =      73 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   25943 /  294912             (  8.80%) | total_pruned =  268969 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     134 /     256             ( 52.34%) | total_pruned =     122 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   23823 /  589824             (  4.04%) | total_pruned =  566001 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     100 /     256             ( 39.06%) | total_pruned =     156 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     131 /     256             ( 51.17%) | total_pruned =     125 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1574 /   32768             (  4.80%) | total_pruned =   31194 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      88 /     256             ( 34.38%) | total_pruned =     168 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     122 /     256             ( 47.66%) | total_pruned =     134 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    5295 /  589824             (  0.90%) | total_pruned =  584529 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      75 /     256             ( 29.30%) | total_pruned =     181 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      78 /     256             ( 30.47%) | total_pruned =     178 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3884 /  589824             (  0.66%) | total_pruned =  585940 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      73 /     256             ( 28.52%) | total_pruned =     183 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     123 /     256             ( 48.05%) | total_pruned =     133 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   28026 / 1179648             (  2.38%) | total_pruned = 1151622 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     201 /     512             ( 39.26%) | total_pruned =     311 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     221 /     512             ( 43.16%) | total_pruned =     291 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   50515 / 2359296             (  2.14%) | total_pruned = 2308781 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     206 /     512             ( 40.23%) | total_pruned =     306 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     262 /     512             ( 51.17%) | total_pruned =     250 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3107 /  131072             (  2.37%) | total_pruned =  127965 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     155 /     512             ( 30.27%) | total_pruned =     357 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     267 /     512             ( 52.15%) | total_pruned =     245 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   91300 / 2359296             (  3.87%) | total_pruned = 2267996 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     214 /     512             ( 41.80%) | total_pruned =     298 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   68971 / 2359296             (  2.92%) | total_pruned = 2290325 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     233 /     512             ( 45.51%) | total_pruned =     279 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3188 /    5120             ( 62.27%) | total_pruned =    1932 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 349337, pruned : 10829425, total: 11178762, Compression rate :      32.00x  ( 96.87% pruned)
Train Epoch: 99/100 Loss: 0.142839 Accuracy: 77.11 98.10 % Best test Accuracy: 79.64%
tensor(-15.2238, device='cuda:0') tensor(2.9093e-07, device='cuda:0') tensor(2.4455e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000272
Average total loss: 2.302857
tensor(-15.2358, device='cuda:0') tensor(2.8380e-07, device='cuda:0') tensor(2.4166e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000269
Average total loss: 2.302853
tensor(-15.2475, device='cuda:0') tensor(2.7692e-07, device='cuda:0') tensor(2.3883e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000265
Average total loss: 2.302850
tensor(-15.2592, device='cuda:0') tensor(2.7029e-07, device='cuda:0') tensor(2.3607e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000262
Average total loss: 2.302847
tensor(-15.2707, device='cuda:0') tensor(2.6388e-07, device='cuda:0') tensor(2.3337e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000259
Average total loss: 2.302844
tensor(-15.2820, device='cuda:0') tensor(2.5769e-07, device='cuda:0') tensor(2.3073e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000257
Average total loss: 2.302841
tensor(-15.2933, device='cuda:0') tensor(2.5171e-07, device='cuda:0') tensor(2.2815e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000254
Average total loss: 2.302839
tensor(-15.3044, device='cuda:0') tensor(2.4598e-07, device='cuda:0') tensor(2.2563e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000251
Average total loss: 2.302836
tensor(-15.3154, device='cuda:0') tensor(2.4041e-07, device='cuda:0') tensor(2.2316e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000248
Average total loss: 2.302833
tensor(-15.3263, device='cuda:0') tensor(2.3504e-07, device='cuda:0') tensor(2.2075e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000245
Average total loss: 2.302830
tensor(-15.3370, device='cuda:0') tensor(2.2983e-07, device='cuda:0') tensor(2.1839e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000243
Average total loss: 2.302828
tensor(-15.3476, device='cuda:0') tensor(2.2483e-07, device='cuda:0') tensor(2.1607e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000240
Average total loss: 2.302825
tensor(-15.3582, device='cuda:0') tensor(2.1995e-07, device='cuda:0') tensor(2.1381e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000239
Average total loss: 2.302824
tensor(-15.3592, device='cuda:0') tensor(2.1931e-07, device='cuda:0') tensor(2.1359e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000239
Average total loss: 2.302824
tensor(-15.3602, device='cuda:0') tensor(2.1869e-07, device='cuda:0') tensor(2.1337e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3613, device='cuda:0') tensor(2.1809e-07, device='cuda:0') tensor(2.1315e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3623, device='cuda:0') tensor(2.1751e-07, device='cuda:0') tensor(2.1293e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3633, device='cuda:0') tensor(2.1694e-07, device='cuda:0') tensor(2.1272e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000238
Average total loss: 2.302823
tensor(-15.3643, device='cuda:0') tensor(2.1638e-07, device='cuda:0') tensor(2.1250e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3654, device='cuda:0') tensor(2.1584e-07, device='cuda:0') tensor(2.1228e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3664, device='cuda:0') tensor(2.1530e-07, device='cuda:0') tensor(2.1206e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3674, device='cuda:0') tensor(2.1478e-07, device='cuda:0') tensor(2.1184e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000237
Average total loss: 2.302822
tensor(-15.3685, device='cuda:0') tensor(2.1428e-07, device='cuda:0') tensor(2.1162e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3695, device='cuda:0') tensor(2.1378e-07, device='cuda:0') tensor(2.1141e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3696, device='cuda:0') tensor(2.1377e-07, device='cuda:0') tensor(2.1139e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3697, device='cuda:0') tensor(2.1376e-07, device='cuda:0') tensor(2.1137e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3698, device='cuda:0') tensor(2.1376e-07, device='cuda:0') tensor(2.1135e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3699, device='cuda:0') tensor(2.1375e-07, device='cuda:0') tensor(2.1133e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3700, device='cuda:0') tensor(2.1374e-07, device='cuda:0') tensor(2.1131e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3700, device='cuda:0') tensor(2.1373e-07, device='cuda:0') tensor(2.1129e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3701, device='cuda:0') tensor(2.1372e-07, device='cuda:0') tensor(2.1127e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3702, device='cuda:0') tensor(2.1372e-07, device='cuda:0') tensor(2.1125e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3703, device='cuda:0') tensor(2.1371e-07, device='cuda:0') tensor(2.1123e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3704, device='cuda:0') tensor(2.1370e-07, device='cuda:0') tensor(2.1121e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000236
Average total loss: 2.302821
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
 Percentile value: -15.370489120483398
Non-zero model percentage: 1.5625073909759521%, Non-zero mask percentage: 1.5625073909759521%

--- Pruning Level [6/12]: ---
conv1.weight         | nonzeros =     257 /    1728             ( 14.87%) | total_pruned =    1471 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1163 /   36864             (  3.15%) | total_pruned =   35701 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1977 /   36864             (  5.36%) | total_pruned =   34887 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      24 /      64             ( 37.50%) | total_pruned =      40 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1665 /   36864             (  4.52%) | total_pruned =   35199 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1793 /   36864             (  4.86%) | total_pruned =   35071 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    4979 /   73728             (  6.75%) | total_pruned =   68749 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    7310 /  147456             (  4.96%) | total_pruned =  140146 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      58 /     128             ( 45.31%) | total_pruned =      70 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     845 /    8192             ( 10.31%) | total_pruned =    7347 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1493 /  147456             (  1.01%) | total_pruned =  145963 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      42 /     128             ( 32.81%) | total_pruned =      86 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      40 /     128             ( 31.25%) | total_pruned =      88 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1139 /  147456             (  0.77%) | total_pruned =  146317 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   14987 /  294912             (  5.08%) | total_pruned =  279925 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     113 /     256             ( 44.14%) | total_pruned =     143 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     130 /     256             ( 50.78%) | total_pruned =     126 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   13947 /  589824             (  2.36%) | total_pruned =  575877 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     116 /     256             ( 45.31%) | total_pruned =     140 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     938 /   32768             (  2.86%) | total_pruned =   31830 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      73 /     256             ( 28.52%) | total_pruned =     183 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     112 /     256             ( 43.75%) | total_pruned =     144 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2267 /  589824             (  0.38%) | total_pruned =  587557 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      60 /     256             ( 23.44%) | total_pruned =     196 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      67 /     256             ( 26.17%) | total_pruned =     189 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    1692 /  589824             (  0.29%) | total_pruned =  588132 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      65 /     256             ( 25.39%) | total_pruned =     191 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     114 /     256             ( 44.53%) | total_pruned =     142 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   15605 / 1179648             (  1.32%) | total_pruned = 1164043 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     181 /     512             ( 35.35%) | total_pruned =     331 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     202 /     512             ( 39.45%) | total_pruned =     310 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   25195 / 2359296             (  1.07%) | total_pruned = 2334101 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     192 /     512             ( 37.50%) | total_pruned =     320 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     253 /     512             ( 49.41%) | total_pruned =     259 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1620 /  131072             (  1.24%) | total_pruned =  129452 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     127 /     512             ( 24.80%) | total_pruned =     385 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     257 /     512             ( 50.20%) | total_pruned =     255 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   38666 / 2359296             (  1.64%) | total_pruned = 2320630 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     203 /     512             ( 39.65%) | total_pruned =     309 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     184 /     512             ( 35.94%) | total_pruned =     328 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   30047 / 2359296             (  1.27%) | total_pruned = 2329249 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     448 /     512             ( 87.50%) | total_pruned =      64 | shape = torch.Size([512])
linear.weight        | nonzeros =    3176 /    5120             ( 62.03%) | total_pruned =    1944 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 174669, pruned : 11004093, total: 11178762, Compression rate :      64.00x  ( 98.44% pruned)
Train Epoch: 99/100 Loss: 0.522770 Accuracy: 75.61 88.94 % Best test Accuracy: 77.11%
tensor(-15.3705, device='cuda:0') tensor(2.1369e-07, device='cuda:0') tensor(2.1119e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000235
Average total loss: 2.302820
tensor(-15.3808, device='cuda:0') tensor(2.0919e-07, device='cuda:0') tensor(2.0903e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000233
Average total loss: 2.302817
tensor(-15.3910, device='cuda:0') tensor(2.0481e-07, device='cuda:0') tensor(2.0691e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000230
Average total loss: 2.302815
tensor(-15.4011, device='cuda:0') tensor(2.0057e-07, device='cuda:0') tensor(2.0483e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000228
Average total loss: 2.302813
tensor(-15.4111, device='cuda:0') tensor(1.9647e-07, device='cuda:0') tensor(2.0280e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000226
Average total loss: 2.302810
tensor(-15.4209, device='cuda:0') tensor(1.9250e-07, device='cuda:0') tensor(2.0080e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000223
Average total loss: 2.302808
tensor(-15.4307, device='cuda:0') tensor(1.8864e-07, device='cuda:0') tensor(1.9885e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000221
Average total loss: 2.302806
tensor(-15.4404, device='cuda:0') tensor(1.8489e-07, device='cuda:0') tensor(1.9693e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000219
Average total loss: 2.302804
tensor(-15.4500, device='cuda:0') tensor(1.8125e-07, device='cuda:0') tensor(1.9505e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000217
Average total loss: 2.302802
tensor(-15.4595, device='cuda:0') tensor(1.7772e-07, device='cuda:0') tensor(1.9320e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000215
Average total loss: 2.302800
tensor(-15.4690, device='cuda:0') tensor(1.7430e-07, device='cuda:0') tensor(1.9139e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000213
Average total loss: 2.302798
tensor(-15.4783, device='cuda:0') tensor(1.7095e-07, device='cuda:0') tensor(1.8961e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000211
Average total loss: 2.302796
tensor(-15.4875, device='cuda:0') tensor(1.6771e-07, device='cuda:0') tensor(1.8787e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000210
Average total loss: 2.302795
tensor(-15.4885, device='cuda:0') tensor(1.6751e-07, device='cuda:0') tensor(1.8769e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000210
Average total loss: 2.302795
tensor(-15.4894, device='cuda:0') tensor(1.6731e-07, device='cuda:0') tensor(1.8751e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000210
Average total loss: 2.302794
tensor(-15.4903, device='cuda:0') tensor(1.6712e-07, device='cuda:0') tensor(1.8734e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302794
tensor(-15.4913, device='cuda:0') tensor(1.6693e-07, device='cuda:0') tensor(1.8716e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302794
tensor(-15.4922, device='cuda:0') tensor(1.6675e-07, device='cuda:0') tensor(1.8699e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302794
tensor(-15.4932, device='cuda:0') tensor(1.6657e-07, device='cuda:0') tensor(1.8681e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302794
tensor(-15.4941, device='cuda:0') tensor(1.6639e-07, device='cuda:0') tensor(1.8664e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000209
Average total loss: 2.302793
tensor(-15.4950, device='cuda:0') tensor(1.6621e-07, device='cuda:0') tensor(1.8647e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4959, device='cuda:0') tensor(1.6600e-07, device='cuda:0') tensor(1.8629e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4968, device='cuda:0') tensor(1.6546e-07, device='cuda:0') tensor(1.8613e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4977, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8596e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4978, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8594e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4979, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8593e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4980, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8591e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4981, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8589e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4982, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8587e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4983, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8586e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4984, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8584e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4985, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8582e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4986, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8580e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4987, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8579e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000208
Average total loss: 2.302793
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
 Percentile value: -15.498720169067383
Non-zero model percentage: 0.781258225440979%, Non-zero mask percentage: 0.781258225440979%

--- Pruning Level [7/12]: ---
conv1.weight         | nonzeros =     230 /    1728             ( 13.31%) | total_pruned =    1498 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     705 /   36864             (  1.91%) | total_pruned =   36159 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1105 /   36864             (  3.00%) | total_pruned =   35759 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     844 /   36864             (  2.29%) | total_pruned =   36020 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      12 /      64             ( 18.75%) | total_pruned =      52 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     899 /   36864             (  2.44%) | total_pruned =   35965 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    2351 /   73728             (  3.19%) | total_pruned =   71377 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      40 /     128             ( 31.25%) | total_pruned =      88 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      39 /     128             ( 30.47%) | total_pruned =      89 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    3334 /  147456             (  2.26%) | total_pruned =  144122 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      51 /     128             ( 39.84%) | total_pruned =      77 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     536 /    8192             (  6.54%) | total_pruned =    7656 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     453 /  147456             (  0.31%) | total_pruned =  147003 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      30 /     128             ( 23.44%) | total_pruned =      98 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     410 /  147456             (  0.28%) | total_pruned =  147046 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    8009 /  294912             (  2.72%) | total_pruned =  286903 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     109 /     256             ( 42.58%) | total_pruned =     147 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    7995 /  589824             (  1.36%) | total_pruned =  581829 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      82 /     256             ( 32.03%) | total_pruned =     174 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     104 /     256             ( 40.62%) | total_pruned =     152 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     543 /   32768             (  1.66%) | total_pruned =   32225 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     881 /  589824             (  0.15%) | total_pruned =  588943 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      50 /     256             ( 19.53%) | total_pruned =     206 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      52 /     256             ( 20.31%) | total_pruned =     204 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     683 /  589824             (  0.12%) | total_pruned =  589141 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      45 /     256             ( 17.58%) | total_pruned =     211 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    8433 / 1179648             (  0.71%) | total_pruned = 1171215 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     181 /     512             ( 35.35%) | total_pruned =     331 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   11732 / 2359296             (  0.50%) | total_pruned = 2347564 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     170 /     512             ( 33.20%) | total_pruned =     342 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     238 /     512             ( 46.48%) | total_pruned =     274 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     868 /  131072             (  0.66%) | total_pruned =  130204 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      98 /     512             ( 19.14%) | total_pruned =     414 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     245 /     512             ( 47.85%) | total_pruned =     267 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   17359 / 2359296             (  0.74%) | total_pruned = 2341937 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     185 /     512             ( 36.13%) | total_pruned =     327 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     163 /     512             ( 31.84%) | total_pruned =     349 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   13320 / 2359296             (  0.56%) | total_pruned = 2345976 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     216 /     512             ( 42.19%) | total_pruned =     296 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3139 /    5120             ( 61.31%) | total_pruned =    1981 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 87335, pruned : 11091427, total: 11178762, Compression rate :     128.00x  ( 99.22% pruned)
Train Epoch: 99/100 Loss: 0.806415 Accuracy: 70.57 76.69 % Best test Accuracy: 70.66%
tensor(-15.4988, device='cuda:0') tensor(1.6494e-07, device='cuda:0') tensor(1.8577e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000207
Average total loss: 2.302792
tensor(-15.5078, device='cuda:0') tensor(1.6187e-07, device='cuda:0') tensor(1.8410e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000205
Average total loss: 2.302790
tensor(-15.5168, device='cuda:0') tensor(1.5890e-07, device='cuda:0') tensor(1.8245e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000203
Average total loss: 2.302788
tensor(-15.5257, device='cuda:0') tensor(1.5602e-07, device='cuda:0') tensor(1.8083e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000201
Average total loss: 2.302786
tensor(-15.5345, device='cuda:0') tensor(1.5319e-07, device='cuda:0') tensor(1.7925e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000200
Average total loss: 2.302784
tensor(-15.5432, device='cuda:0') tensor(1.5046e-07, device='cuda:0') tensor(1.7768e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000198
Average total loss: 2.302783
tensor(-15.5519, device='cuda:0') tensor(1.4780e-07, device='cuda:0') tensor(1.7615e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000196
Average total loss: 2.302781
tensor(-15.5605, device='cuda:0') tensor(1.4519e-07, device='cuda:0') tensor(1.7464e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000194
Average total loss: 2.302779
tensor(-15.5690, device='cuda:0') tensor(1.4269e-07, device='cuda:0') tensor(1.7316e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000193
Average total loss: 2.302778
tensor(-15.5775, device='cuda:0') tensor(1.4021e-07, device='cuda:0') tensor(1.7171e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000191
Average total loss: 2.302776
tensor(-15.5859, device='cuda:0') tensor(1.3783e-07, device='cuda:0') tensor(1.7027e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000190
Average total loss: 2.302774
tensor(-15.5942, device='cuda:0') tensor(1.3548e-07, device='cuda:0') tensor(1.6886e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000188
Average total loss: 2.302773
tensor(-15.6024, device='cuda:0') tensor(1.3322e-07, device='cuda:0') tensor(1.6748e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000187
Average total loss: 2.302772
tensor(-15.6032, device='cuda:0') tensor(1.3311e-07, device='cuda:0') tensor(1.6734e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000187
Average total loss: 2.302772
tensor(-15.6041, device='cuda:0') tensor(1.3299e-07, device='cuda:0') tensor(1.6720e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.000187
Average total loss: 2.302772
tensor(-15.6049, device='cuda:0') tensor(1.3262e-07, device='cuda:0') tensor(1.6706e-11, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.000187
Average total loss: 2.302772
tensor(-15.6057, device='cuda:0') tensor(1.3221e-07, device='cuda:0') tensor(1.6693e-11, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.000187
Average total loss: 2.302771
tensor(-15.6065, device='cuda:0') tensor(1.3180e-07, device='cuda:0') tensor(1.6680e-11, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6073, device='cuda:0') tensor(1.3141e-07, device='cuda:0') tensor(1.6666e-11, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6081, device='cuda:0') tensor(1.3102e-07, device='cuda:0') tensor(1.6653e-11, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6089, device='cuda:0') tensor(1.3065e-07, device='cuda:0') tensor(1.6640e-11, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6097, device='cuda:0') tensor(1.3029e-07, device='cuda:0') tensor(1.6627e-11, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6105, device='cuda:0') tensor(1.2994e-07, device='cuda:0') tensor(1.6614e-11, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302771
tensor(-15.6113, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6600e-11, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302770
tensor(-15.6114, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6599e-11, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302770
tensor(-15.6114, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6597e-11, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302770
tensor(-15.6115, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6596e-11, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.000186
Average total loss: 2.302770
tensor(-15.6116, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6594e-11, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6117, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6593e-11, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6118, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6591e-11, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6119, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6590e-11, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6120, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6588e-11, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6121, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6586e-11, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6122, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6585e-11, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
 Percentile value: -15.612212181091309
Non-zero model percentage: 0.39063358306884766%, Non-zero mask percentage: 0.39063358306884766%

--- Pruning Level [8/12]: ---
conv1.weight         | nonzeros =     196 /    1728             ( 11.34%) | total_pruned =    1532 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
bn1.bias             | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     439 /   36864             (  1.19%) | total_pruned =   36425 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     682 /   36864             (  1.85%) | total_pruned =   36182 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     466 /   36864             (  1.26%) | total_pruned =   36398 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     517 /   36864             (  1.40%) | total_pruned =   36347 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    1211 /   73728             (  1.64%) | total_pruned =   72517 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      36 /     128             ( 28.12%) | total_pruned =      92 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    1747 /  147456             (  1.18%) | total_pruned =  145709 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      46 /     128             ( 35.94%) | total_pruned =      82 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      50 /     128             ( 39.06%) | total_pruned =      78 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     388 /    8192             (  4.74%) | total_pruned =    7804 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     170 /  147456             (  0.12%) | total_pruned =  147286 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      22 /     128             ( 17.19%) | total_pruned =     106 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     167 /  147456             (  0.11%) | total_pruned =  147289 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    3799 /  294912             (  1.29%) | total_pruned =  291113 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      97 /     256             ( 37.89%) | total_pruned =     159 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     102 /     256             ( 39.84%) | total_pruned =     154 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    4103 /  589824             (  0.70%) | total_pruned =  585721 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      73 /     256             ( 28.52%) | total_pruned =     183 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     308 /   32768             (  0.94%) | total_pruned =   32460 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      49 /     256             ( 19.14%) | total_pruned =     207 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      81 /     256             ( 31.64%) | total_pruned =     175 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     285 /  589824             (  0.05%) | total_pruned =  589539 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      31 /     256             ( 12.11%) | total_pruned =     225 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      39 /     256             ( 15.23%) | total_pruned =     217 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     234 /  589824             (  0.04%) | total_pruned =  589590 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      25 /     256             (  9.77%) | total_pruned =     231 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =      80 /     256             ( 31.25%) | total_pruned =     176 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    4141 / 1179648             (  0.35%) | total_pruned = 1175507 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     138 /     512             ( 26.95%) | total_pruned =     374 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     153 /     512             ( 29.88%) | total_pruned =     359 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    4970 / 2359296             (  0.21%) | total_pruned = 2354326 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     150 /     512             ( 29.30%) | total_pruned =     362 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     225 /     512             ( 43.95%) | total_pruned =     287 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     405 /  131072             (  0.31%) | total_pruned =  130667 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      72 /     512             ( 14.06%) | total_pruned =     440 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     237 /     512             ( 46.29%) | total_pruned =     275 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    7315 / 2359296             (  0.31%) | total_pruned = 2351981 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     131 /     512             ( 25.59%) | total_pruned =     381 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    5974 / 2359296             (  0.25%) | total_pruned = 2353322 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     192 /     512             ( 37.50%) | total_pruned =     320 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3073 /    5120             ( 60.02%) | total_pruned =    2047 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 43668, pruned : 11135094, total: 11178762, Compression rate :     255.99x  ( 99.61% pruned)
Train Epoch: 99/100 Loss: 0.924142 Accuracy: 61.02 63.15 % Best test Accuracy: 61.19%
tensor(-15.6123, device='cuda:0') tensor(1.2959e-07, device='cuda:0') tensor(1.6583e-11, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.000185
Average total loss: 2.302770
tensor(-15.6204, device='cuda:0') tensor(1.2746e-07, device='cuda:0') tensor(1.6450e-11, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.000183
Average total loss: 2.302768
tensor(-15.6284, device='cuda:0') tensor(1.2534e-07, device='cuda:0') tensor(1.6318e-11, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.000182
Average total loss: 2.302767
tensor(-15.6364, device='cuda:0') tensor(1.2332e-07, device='cuda:0') tensor(1.6189e-11, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.000180
Average total loss: 2.302765
tensor(-15.6443, device='cuda:0') tensor(1.2134e-07, device='cuda:0') tensor(1.6061e-11, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.000179
Average total loss: 2.302764
tensor(-15.6521, device='cuda:0') tensor(1.1938e-07, device='cuda:0') tensor(1.5936e-11, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.000177
Average total loss: 2.302762
tensor(-15.6599, device='cuda:0') tensor(1.1749e-07, device='cuda:0') tensor(1.5813e-11, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.000176
Average total loss: 2.302761
tensor(-15.6676, device='cuda:0') tensor(1.1566e-07, device='cuda:0') tensor(1.5691e-11, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.000175
Average total loss: 2.302760
tensor(-15.6753, device='cuda:0') tensor(1.1383e-07, device='cuda:0') tensor(1.5571e-11, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.000173
Average total loss: 2.302758
tensor(-15.6829, device='cuda:0') tensor(1.1206e-07, device='cuda:0') tensor(1.5453e-11, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.000172
Average total loss: 2.302757
tensor(-15.6904, device='cuda:0') tensor(1.1034e-07, device='cuda:0') tensor(1.5337e-11, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.000171
Average total loss: 2.302756
tensor(-15.6979, device='cuda:0') tensor(1.0867e-07, device='cuda:0') tensor(1.5223e-11, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.000170
Average total loss: 2.302754
tensor(-15.7053, device='cuda:0') tensor(1.0703e-07, device='cuda:0') tensor(1.5110e-11, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.000169
Average total loss: 2.302754
tensor(-15.7061, device='cuda:0') tensor(1.0692e-07, device='cuda:0') tensor(1.5099e-11, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.000169
Average total loss: 2.302754
tensor(-15.7068, device='cuda:0') tensor(1.0682e-07, device='cuda:0') tensor(1.5088e-11, device='cuda:0')
Epoch 15
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 16
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 17
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 18
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 19
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 20
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 21
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 22
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 23
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 24
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 25
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 26
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 27
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 28
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 29
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 30
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 31
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 32
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 33
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 34
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 35
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 36
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 37
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 38
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 39
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 40
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 41
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 42
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 43
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 44
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 45
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 46
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 47
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 48
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 49
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 50
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 51
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 52
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 53
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 54
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 55
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
 Percentile value: nan
Non-zero model percentage: 0.19530785083770752%, Non-zero mask percentage: 0.19531679153442383%

--- Pruning Level [9/12]: ---
conv1.weight         | nonzeros =       0 /    1728             (  0.00%) | total_pruned =    1728 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =       0 /   73728             (  0.00%) | total_pruned =   73728 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =       0 /    8192             (  0.00%) | total_pruned =    8192 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =       0 /  294912             (  0.00%) | total_pruned =  294912 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =       0 /   32768             (  0.00%) | total_pruned =   32768 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =       0 / 1179648             (  0.00%) | total_pruned = 1179648 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    3437 / 2359296             (  0.15%) | total_pruned = 2355859 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     150 /     512             ( 29.30%) | total_pruned =     362 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     225 /     512             ( 43.95%) | total_pruned =     287 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     405 /  131072             (  0.31%) | total_pruned =  130667 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      72 /     512             ( 14.06%) | total_pruned =     440 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     236 /     512             ( 46.09%) | total_pruned =     276 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    7315 / 2359296             (  0.31%) | total_pruned = 2351981 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     131 /     512             ( 25.59%) | total_pruned =     381 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    5974 / 2359296             (  0.25%) | total_pruned = 2353322 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     192 /     512             ( 37.50%) | total_pruned =     320 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3073 /    5120             ( 60.02%) | total_pruned =    2047 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 21833, pruned : 11156929, total: 11178762, Compression rate :     512.01x  ( 99.80% pruned)
Train Epoch: 56/100 Loss: 2.302496 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 1
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 2
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 3
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 4
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 5
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 6
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 7
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 8
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 9
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 10
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 11
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 12
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 13
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 14
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 15
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 16
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 17
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 18
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 19
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 20
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 21
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 22
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 23
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 24
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 25
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 26
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 27
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 28
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 29
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 30
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 31
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 32
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 33
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 34
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 35
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 36
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 37
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 38
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 39
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 40
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 41
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 42
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 43
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 44
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 45
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 46
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 47
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 48
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 49
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 50
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 51
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 52
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 53
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 54
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
 Percentile value: nan
Non-zero model percentage: 0.09765839576721191%, Non-zero mask percentage: 0.09765839576721191%

--- Pruning Level [10/12]: ---
conv1.weight         | nonzeros =       0 /    1728             (  0.00%) | total_pruned =    1728 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =       0 /   73728             (  0.00%) | total_pruned =   73728 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =       0 /    8192             (  0.00%) | total_pruned =    8192 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =       0 /  294912             (  0.00%) | total_pruned =  294912 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =       0 /   32768             (  0.00%) | total_pruned =   32768 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =       0 / 1179648             (  0.00%) | total_pruned = 1179648 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =       0 / 2359296             (  0.00%) | total_pruned = 2359296 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =       0 /  131072             (  0.00%) | total_pruned =  131072 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     924 / 2359296             (  0.04%) | total_pruned = 2358372 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     166 /     512             ( 32.42%) | total_pruned =     346 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     131 /     512             ( 25.59%) | total_pruned =     381 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    5974 / 2359296             (  0.25%) | total_pruned = 2353322 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     192 /     512             ( 37.50%) | total_pruned =     320 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3073 /    5120             ( 60.02%) | total_pruned =    2047 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 10917, pruned : 11167845, total: 11178762, Compression rate :    1023.98x  ( 99.90% pruned)
Train Epoch: 56/100 Loss: 2.302588 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 1
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 2
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 3
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 4
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 5
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 6
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 7
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 8
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 9
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 10
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 11
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 12
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 13
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 14
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 15
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 16
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 17
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 18
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 19
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 20
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 21
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 22
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 23
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 24
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 25
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 26
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 27
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 28
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 29
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 30
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 31
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 32
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 33
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 34
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 35
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 36
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 37
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 38
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 39
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 40
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 41
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 42
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 43
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 44
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 45
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 46
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 47
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 48
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 49
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 50
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 51
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 52
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 53
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
Epoch 54
Average batch original loss after noise: nan
Average KL loss: nan
Average total loss: nan
tensor(nan, device='cuda:0') tensor(nan, device='cuda:0') tensor(nan, device='cuda:0')
 Percentile value: nan
Non-zero model percentage: 0.04883367195725441%, Non-zero mask percentage: 0.04883367195725441%

--- Pruning Level [11/12]: ---
conv1.weight         | nonzeros =       0 /    1728             (  0.00%) | total_pruned =    1728 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =       0 /   36864             (  0.00%) | total_pruned =   36864 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =       0 /   73728             (  0.00%) | total_pruned =   73728 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =       0 /    8192             (  0.00%) | total_pruned =    8192 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       0 /  147456             (  0.00%) | total_pruned =  147456 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =       0 /  294912             (  0.00%) | total_pruned =  294912 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =       0 /   32768             (  0.00%) | total_pruned =   32768 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =       0 / 1179648             (  0.00%) | total_pruned = 1179648 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =       0 / 2359296             (  0.00%) | total_pruned = 2359296 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =       0 /  131072             (  0.00%) | total_pruned =  131072 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =       0 / 2359296             (  0.00%) | total_pruned = 2359296 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    1737 / 2359296             (  0.07%) | total_pruned = 2357559 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     192 /     512             ( 37.50%) | total_pruned =     320 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     447 /     512             ( 87.30%) | total_pruned =      65 | shape = torch.Size([512])
linear.weight        | nonzeros =    3073 /    5120             ( 60.02%) | total_pruned =    2047 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 5459, pruned : 11173303, total: 11178762, Compression rate :    2047.77x  ( 99.95% pruned)
Train Epoch: 56/100 Loss: 2.302509 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
