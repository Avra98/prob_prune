Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/7]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 61/200 Loss: 0.000080 Accuracy: 86.83 100.00 % Best test Accuracy: 86.83%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.084489
Average KL loss: 4930.767531
Average total loss: 4932.852220
tensor(-0.4838, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.112657
Average KL loss: 3688.614490
Average total loss: 3690.727320
tensor(-0.9418, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.161801
Average KL loss: 2703.641367
Average total loss: 2705.803292
tensor(-1.3511, device='cuda:0') tensor(0.0037, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.249486
Average KL loss: 1998.757046
Average total loss: 2001.006634
tensor(-1.7060, device='cuda:0') tensor(0.0056, device='cuda:0') tensor(0.0001, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.249163
Average KL loss: 1513.512536
Average total loss: 1515.761779
tensor(-2.0116, device='cuda:0') tensor(0.0073, device='cuda:0') tensor(0.0001, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.300835
Average KL loss: 1178.407212
Average total loss: 1180.708090
tensor(-2.2764, device='cuda:0') tensor(0.0088, device='cuda:0') tensor(8.4538e-05, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.321276
Average KL loss: 941.849880
Average total loss: 944.171199
tensor(-2.5084, device='cuda:0') tensor(0.0100, device='cuda:0') tensor(6.9751e-05, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.343360
Average KL loss: 770.183754
Average total loss: 772.527149
tensor(-2.7139, device='cuda:0') tensor(0.0110, device='cuda:0') tensor(5.8491e-05, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.338530
Average KL loss: 642.147312
Average total loss: 644.485870
tensor(-2.8982, device='cuda:0') tensor(0.0118, device='cuda:0') tensor(4.9704e-05, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.347316
Average KL loss: 544.231698
Average total loss: 546.579038
tensor(-3.0650, device='cuda:0') tensor(0.0125, device='cuda:0') tensor(4.2788e-05, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.345450
Average KL loss: 467.672809
Average total loss: 470.018280
tensor(-3.2173, device='cuda:0') tensor(0.0130, device='cuda:0') tensor(3.7244e-05, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.346164
Average KL loss: 406.642265
Average total loss: 408.988448
tensor(-3.3574, device='cuda:0') tensor(0.0134, device='cuda:0') tensor(3.2733e-05, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.342476
Average KL loss: 357.164314
Average total loss: 359.506806
tensor(-3.4872, device='cuda:0') tensor(0.0138, device='cuda:0') tensor(2.9006e-05, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.341151
Average KL loss: 316.454575
Average total loss: 318.795741
tensor(-3.6081, device='cuda:0') tensor(0.0141, device='cuda:0') tensor(2.5902e-05, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.334715
Average KL loss: 282.521495
Average total loss: 284.856221
tensor(-3.7213, device='cuda:0') tensor(0.0143, device='cuda:0') tensor(2.3259e-05, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.330519
Average KL loss: 253.912811
Average total loss: 256.243342
tensor(-3.8278, device='cuda:0') tensor(0.0145, device='cuda:0') tensor(2.1037e-05, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.327267
Average KL loss: 229.546809
Average total loss: 231.874086
tensor(-3.9284, device='cuda:0') tensor(0.0147, device='cuda:0') tensor(1.9097e-05, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.322975
Average KL loss: 208.606826
Average total loss: 210.929811
tensor(-4.0237, device='cuda:0') tensor(0.0149, device='cuda:0') tensor(1.7436e-05, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.319710
Average KL loss: 190.464943
Average total loss: 192.784661
tensor(-4.1144, device='cuda:0') tensor(0.0150, device='cuda:0') tensor(1.5980e-05, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.317101
Average KL loss: 174.630946
Average total loss: 176.948057
tensor(-4.2008, device='cuda:0') tensor(0.0151, device='cuda:0') tensor(1.4702e-05, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.323144
Average KL loss: 160.720924
Average total loss: 163.044075
tensor(-4.2835, device='cuda:0') tensor(0.0152, device='cuda:0') tensor(1.3567e-05, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.317702
Average KL loss: 148.427297
Average total loss: 150.745006
tensor(-4.3627, device='cuda:0') tensor(0.0152, device='cuda:0') tensor(1.2570e-05, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.315585
Average KL loss: 137.503569
Average total loss: 139.819160
tensor(-4.4388, device='cuda:0') tensor(0.0153, device='cuda:0') tensor(1.1675e-05, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.317022
Average KL loss: 127.748049
Average total loss: 130.065077
tensor(-4.5121, device='cuda:0') tensor(0.0154, device='cuda:0') tensor(1.0871e-05, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.311849
Average KL loss: 118.996011
Average total loss: 121.307866
tensor(-4.5828, device='cuda:0') tensor(0.0154, device='cuda:0') tensor(1.0142e-05, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.311646
Average KL loss: 111.110641
Average total loss: 113.422292
tensor(-4.6510, device='cuda:0') tensor(0.0154, device='cuda:0') tensor(9.4937e-06, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.310210
Average KL loss: 103.978440
Average total loss: 106.288654
tensor(-4.7171, device='cuda:0') tensor(0.0155, device='cuda:0') tensor(8.9030e-06, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.309258
Average KL loss: 97.504584
Average total loss: 99.813846
tensor(-4.7811, device='cuda:0') tensor(0.0155, device='cuda:0') tensor(8.3579e-06, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.309658
Average KL loss: 91.608389
Average total loss: 93.918051
tensor(-4.8431, device='cuda:0') tensor(0.0155, device='cuda:0') tensor(7.8652e-06, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.307757
Average KL loss: 86.221471
Average total loss: 88.529232
tensor(-4.9035, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(7.4096e-06, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.308278
Average KL loss: 81.285691
Average total loss: 83.593972
tensor(-4.9622, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(6.9952e-06, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.305868
Average KL loss: 76.750803
Average total loss: 79.056675
tensor(-5.0193, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(6.6174e-06, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.307133
Average KL loss: 72.573843
Average total loss: 74.880978
tensor(-5.0750, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(6.2591e-06, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.305327
Average KL loss: 68.717358
Average total loss: 71.022689
tensor(-5.1294, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(5.9366e-06, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.304309
Average KL loss: 65.148642
Average total loss: 67.452954
tensor(-5.1825, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(5.6338e-06, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.304137
Average KL loss: 61.839186
Average total loss: 64.143327
tensor(-5.2344, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.3506e-06, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.304982
Average KL loss: 58.764017
Average total loss: 61.069002
tensor(-5.2851, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.0892e-06, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.304815
Average KL loss: 55.901056
Average total loss: 58.205874
tensor(-5.3349, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.8444e-06, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.305034
Average KL loss: 53.230976
Average total loss: 55.536013
tensor(-5.3836, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.6171e-06, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.304793
Average KL loss: 50.736736
Average total loss: 53.041531
tensor(-5.4314, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.4020e-06, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.303182
Average KL loss: 48.402902
Average total loss: 50.706086
tensor(-5.4783, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.2051e-06, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.303809
Average KL loss: 46.215768
Average total loss: 48.519579
tensor(-5.5243, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.0177e-06, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.303688
Average KL loss: 44.163158
Average total loss: 46.466848
tensor(-5.5696, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(3.8408e-06, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.303670
Average KL loss: 42.234240
Average total loss: 44.537912
tensor(-5.6140, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(3.6762e-06, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.303121
Average KL loss: 40.419113
Average total loss: 42.722235
tensor(-5.6578, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(3.5193e-06, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302911
Average KL loss: 38.708927
Average total loss: 41.011840
tensor(-5.7008, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(3.3725e-06, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302498
Average KL loss: 37.095637
Average total loss: 39.398136
tensor(-5.7432, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(3.2328e-06, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302790
Average KL loss: 35.572131
Average total loss: 37.874923
tensor(-5.7850, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.1020e-06, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.303231
Average KL loss: 34.131754
Average total loss: 36.434987
tensor(-5.8262, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.9779e-06, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302651
Average KL loss: 32.768514
Average total loss: 35.071166
tensor(-5.8668, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.8604e-06, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302970
Average KL loss: 31.477230
Average total loss: 33.780202
tensor(-5.9068, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.7493e-06, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302953
Average KL loss: 30.252701
Average total loss: 32.555655
tensor(-5.9463, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.6429e-06, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302528
Average KL loss: 29.090473
Average total loss: 31.393002
tensor(-5.9854, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.5429e-06, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302544
Average KL loss: 27.986438
Average total loss: 30.288984
tensor(-6.0239, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.4468e-06, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302747
Average KL loss: 26.936846
Average total loss: 29.239594
tensor(-6.0620, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.3558e-06, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302795
Average KL loss: 25.938141
Average total loss: 28.240937
tensor(-6.0996, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.2705e-06, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302609
Average KL loss: 24.987107
Average total loss: 27.289717
tensor(-6.1369, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.1867e-06, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302566
Average KL loss: 24.080832
Average total loss: 26.383399
tensor(-6.1737, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.1079e-06, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302577
Average KL loss: 23.216560
Average total loss: 25.519139
tensor(-6.2101, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.0331e-06, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302601
Average KL loss: 22.391778
Average total loss: 24.694380
tensor(-6.2461, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.9614e-06, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 21.604153
Average total loss: 23.906738
tensor(-6.2818, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.8929e-06, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302592
Average KL loss: 20.851536
Average total loss: 23.154129
tensor(-6.3172, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.8286e-06, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302619
Average KL loss: 20.131930
Average total loss: 22.434550
tensor(-6.3522, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.7648e-06, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302591
Average KL loss: 19.443477
Average total loss: 21.746069
tensor(-6.3869, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.7049e-06, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302587
Average KL loss: 18.784444
Average total loss: 21.087032
tensor(-6.4212, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.6475e-06, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302582
Average KL loss: 18.153225
Average total loss: 20.455808
tensor(-6.4553, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.5925e-06, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302593
Average KL loss: 17.548317
Average total loss: 19.850911
tensor(-6.4891, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.5398e-06, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302566
Average KL loss: 16.968333
Average total loss: 19.270900
tensor(-6.5226, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.4892e-06, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302598
Average KL loss: 16.411958
Average total loss: 18.714557
tensor(-6.5558, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.4407e-06, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302574
Average KL loss: 15.877972
Average total loss: 18.180547
tensor(-6.5888, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.3941e-06, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302579
Average KL loss: 15.365238
Average total loss: 17.667818
tensor(-6.6215, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.3493e-06, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302583
Average KL loss: 14.872686
Average total loss: 17.175269
tensor(-6.6540, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.3063e-06, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 14.399314
Average total loss: 16.701900
tensor(-6.6863, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.2649e-06, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302572
Average KL loss: 13.944182
Average total loss: 16.246755
tensor(-6.7183, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.2252e-06, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 13.506406
Average total loss: 15.808991
tensor(-6.7501, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.1869e-06, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302581
Average KL loss: 13.085158
Average total loss: 15.387740
tensor(-6.7817, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.1500e-06, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302581
Average KL loss: 12.679659
Average total loss: 14.982240
tensor(-6.8131, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.1146e-06, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302583
Average KL loss: 12.289173
Average total loss: 14.591757
tensor(-6.8443, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.0804e-06, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302586
Average KL loss: 11.913008
Average total loss: 14.215594
tensor(-6.8753, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.0475e-06, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302581
Average KL loss: 11.550512
Average total loss: 13.853094
tensor(-6.9062, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(1.0158e-06, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 11.201069
Average total loss: 13.503654
tensor(-6.9368, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(9.8516e-07, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302588
Average KL loss: 10.864098
Average total loss: 13.166687
tensor(-6.9673, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(9.5564e-07, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302590
Average KL loss: 10.539050
Average total loss: 12.841640
tensor(-6.9976, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(9.2717e-07, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302583
Average KL loss: 10.225406
Average total loss: 12.527990
tensor(-7.0277, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(8.9968e-07, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 9.922675
Average total loss: 12.225260
tensor(-7.0577, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(8.7315e-07, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 9.630392
Average total loss: 11.932978
tensor(-7.0875, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(8.4752e-07, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302579
Average KL loss: 9.348115
Average total loss: 11.650694
tensor(-7.1172, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(8.2277e-07, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302587
Average KL loss: 9.075428
Average total loss: 11.378015
tensor(-7.1468, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(7.9885e-07, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302586
Average KL loss: 8.811934
Average total loss: 11.114520
tensor(-7.1762, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(7.7574e-07, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302584
Average KL loss: 8.557257
Average total loss: 10.859841
tensor(-7.2054, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(7.5339e-07, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302587
Average KL loss: 8.311040
Average total loss: 10.613627
tensor(-7.2346, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(7.3179e-07, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302586
Average KL loss: 8.072942
Average total loss: 10.375529
tensor(-7.2636, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(7.1089e-07, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302600
Average KL loss: 7.842644
Average total loss: 10.145244
tensor(-7.2925, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(6.9067e-07, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302584
Average KL loss: 7.619836
Average total loss: 9.922421
tensor(-7.3212, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(6.7111e-07, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302584
Average KL loss: 7.404228
Average total loss: 9.706812
tensor(-7.3499, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(6.5217e-07, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302579
Average KL loss: 7.195540
Average total loss: 9.498120
tensor(-7.3784, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(6.3384e-07, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302581
Average KL loss: 6.993508
Average total loss: 9.296089
tensor(-7.4069, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(6.1610e-07, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302586
Average KL loss: 6.797879
Average total loss: 9.100465
tensor(-7.4352, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.9891e-07, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302595
Average KL loss: 6.608411
Average total loss: 8.911007
tensor(-7.4634, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.8226e-07, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302573
Average KL loss: 6.424876
Average total loss: 8.727450
tensor(-7.4915, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.6613e-07, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302586
Average KL loss: 6.247054
Average total loss: 8.549640
tensor(-7.5195, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.5050e-07, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302581
Average KL loss: 6.074735
Average total loss: 8.377316
tensor(-7.5475, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.3535e-07, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 5.907719
Average total loss: 8.210305
tensor(-7.5753, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.2067e-07, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302595
Average KL loss: 5.745815
Average total loss: 8.048410
tensor(-7.6031, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(5.0643e-07, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302586
Average KL loss: 5.588839
Average total loss: 7.891425
tensor(-7.6307, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.9262e-07, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302595
Average KL loss: 5.436616
Average total loss: 7.739211
tensor(-7.6583, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.7924e-07, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 5.288978
Average total loss: 7.591563
tensor(-7.6858, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.6625e-07, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302582
Average KL loss: 5.145766
Average total loss: 7.448348
tensor(-7.7132, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.5365e-07, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 5.006824
Average total loss: 7.309409
tensor(-7.7405, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.4143e-07, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 4.872007
Average total loss: 7.174592
tensor(-7.7678, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.2956e-07, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302586
Average KL loss: 4.741172
Average total loss: 7.043758
tensor(-7.7950, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.1805e-07, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302578
Average KL loss: 4.614184
Average total loss: 6.916762
tensor(-7.8221, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(4.0687e-07, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302586
Average KL loss: 4.490913
Average total loss: 6.793499
tensor(-7.8491, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.9602e-07, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 4.371235
Average total loss: 6.673820
tensor(-7.8761, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.8549e-07, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302589
Average KL loss: 4.255029
Average total loss: 6.557618
tensor(-7.9030, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.7526e-07, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302584
Average KL loss: 4.142181
Average total loss: 6.444765
tensor(-7.9299, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.6532e-07, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 4.032580
Average total loss: 6.335165
tensor(-7.9567, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.5567e-07, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 3.926119
Average total loss: 6.228704
tensor(-7.9834, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.4630e-07, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 3.822699
Average total loss: 6.125284
tensor(-8.0101, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.3719e-07, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 3.722220
Average total loss: 6.024805
tensor(-8.0367, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.2834e-07, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 3.624586
Average total loss: 5.927171
tensor(-8.0632, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.1974e-07, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 3.529708
Average total loss: 5.832293
tensor(-8.0897, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.1138e-07, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302582
Average KL loss: 3.437498
Average total loss: 5.740081
tensor(-8.1162, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(3.0326e-07, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302585
Average KL loss: 3.347873
Average total loss: 5.650459
tensor(-8.1426, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.9536e-07, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 3.260751
Average total loss: 5.563336
tensor(-8.1689, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.8769e-07, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 3.176054
Average total loss: 5.478639
tensor(-8.1952, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.8022e-07, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 3.093706
Average total loss: 5.396291
tensor(-8.2214, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.7297e-07, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 3.013635
Average total loss: 5.316220
tensor(-8.2476, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.6591e-07, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302586
Average KL loss: 2.935771
Average total loss: 5.238357
tensor(-8.2738, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.5905e-07, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 2.860047
Average total loss: 5.162632
tensor(-8.2999, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.5237e-07, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 2.786398
Average total loss: 5.088983
tensor(-8.3260, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.4588e-07, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302587
Average KL loss: 2.714761
Average total loss: 5.017348
tensor(-8.3520, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.3957e-07, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 2.645075
Average total loss: 4.947660
tensor(-8.3780, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.3343e-07, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 2.577280
Average total loss: 4.879865
tensor(-8.4039, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.2745e-07, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 2.511322
Average total loss: 4.813907
tensor(-8.4298, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.2164e-07, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 2.447147
Average total loss: 4.749732
tensor(-8.4557, device='cuda:0') tensor(0.0158, device='cuda:0') tensor(2.1598e-07, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 2.384702
Average total loss: 4.687287
tensor(-8.4815, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(2.1047e-07, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 2.323936
Average total loss: 4.626521
tensor(-8.5073, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(2.0511e-07, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 2.264794
Average total loss: 4.567379
tensor(-8.5331, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.9990e-07, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 2.207235
Average total loss: 4.509821
tensor(-8.5588, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.9482e-07, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 2.151216
Average total loss: 4.453801
tensor(-8.5845, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.8988e-07, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 2.096686
Average total loss: 4.399271
tensor(-8.6102, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.8507e-07, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302584
Average KL loss: 2.043600
Average total loss: 4.346184
tensor(-8.6358, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.8039e-07, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 1.991925
Average total loss: 4.294510
tensor(-8.6614, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.7583e-07, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 1.941617
Average total loss: 4.244202
tensor(-8.6869, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.7140e-07, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 1.892630
Average total loss: 4.195215
tensor(-8.7125, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.6708e-07, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 1.844941
Average total loss: 4.147526
tensor(-8.7380, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.6287e-07, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 1.798499
Average total loss: 4.101084
tensor(-8.7635, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.5877e-07, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 1.753275
Average total loss: 4.055860
tensor(-8.7889, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.5478e-07, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 1.709241
Average total loss: 4.011826
tensor(-8.8143, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.5090e-07, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 1.666346
Average total loss: 3.968931
tensor(-8.8397, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.4711e-07, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 1.624582
Average total loss: 3.927167
tensor(-8.8651, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.4343e-07, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 1.583892
Average total loss: 3.886477
tensor(-8.8905, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.3984e-07, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302593
Average KL loss: 1.544272
Average total loss: 3.846865
tensor(-8.9158, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.3634e-07, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 1.505668
Average total loss: 3.808253
tensor(-8.9411, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.3294e-07, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 1.468075
Average total loss: 3.770660
tensor(-8.9664, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.2962e-07, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 1.431441
Average total loss: 3.734026
tensor(-8.9916, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.2639e-07, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302590
Average KL loss: 1.395766
Average total loss: 3.698356
tensor(-9.0168, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.2324e-07, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 1.360996
Average total loss: 3.663581
tensor(-9.0420, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.2017e-07, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 1.327135
Average total loss: 3.629720
tensor(-9.0672, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.1718e-07, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 1.294129
Average total loss: 3.596714
tensor(-9.0924, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.1427e-07, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 1.261984
Average total loss: 3.564569
tensor(-9.1175, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.1143e-07, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 1.230651
Average total loss: 3.533236
tensor(-9.1427, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.0867e-07, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 1.200126
Average total loss: 3.502711
tensor(-9.1678, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.0598e-07, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 1.170382
Average total loss: 3.472967
tensor(-9.1929, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.0335e-07, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 1.141386
Average total loss: 3.443971
tensor(-9.2179, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(1.0079e-07, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 1.113145
Average total loss: 3.415730
tensor(-9.2430, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(9.8298e-08, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 1.085608
Average total loss: 3.388194
tensor(-9.2680, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(9.5867e-08, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 1.058777
Average total loss: 3.361362
tensor(-9.2930, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(9.3499e-08, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 1.032632
Average total loss: 3.335217
tensor(-9.3180, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(9.1191e-08, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 1.007137
Average total loss: 3.309722
tensor(-9.3430, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(8.8941e-08, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.982300
Average total loss: 3.284885
tensor(-9.3680, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(8.6749e-08, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.958089
Average total loss: 3.260674
tensor(-9.3929, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(8.4611e-08, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302586
Average KL loss: 0.934479
Average total loss: 3.237065
tensor(-9.4179, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(8.2527e-08, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302584
Average KL loss: 0.911479
Average total loss: 3.214064
tensor(-9.4428, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(8.0497e-08, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.889055
Average total loss: 3.191640
tensor(-9.4677, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(7.8517e-08, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.867186
Average total loss: 3.169771
tensor(-9.4926, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(7.6586e-08, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.845880
Average total loss: 3.148465
tensor(-9.5174, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(7.4706e-08, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.825108
Average total loss: 3.127693
tensor(-9.5423, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(7.2872e-08, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.804847
Average total loss: 3.107432
tensor(-9.5672, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(7.1083e-08, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302588
Average KL loss: 0.785103
Average total loss: 3.087691
tensor(-9.5920, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.9340e-08, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302585
Average KL loss: 0.765858
Average total loss: 3.068443
tensor(-9.6168, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.7641e-08, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302585
Average KL loss: 0.747086
Average total loss: 3.049671
tensor(-9.6416, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.5983e-08, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302585
Average KL loss: 0.728782
Average total loss: 3.031367
tensor(-9.6664, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.4368e-08, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302585
Average KL loss: 0.710948
Average total loss: 3.013533
tensor(-9.6911, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.2793e-08, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302585
Average KL loss: 0.693554
Average total loss: 2.996139
tensor(-9.7159, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(6.1257e-08, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302585
Average KL loss: 0.676586
Average total loss: 2.979171
tensor(-9.7407, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.9758e-08, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302585
Average KL loss: 0.660048
Average total loss: 2.962633
tensor(-9.7654, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.8299e-08, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302585
Average KL loss: 0.643928
Average total loss: 2.946513
tensor(-9.7901, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.6875e-08, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302585
Average KL loss: 0.628203
Average total loss: 2.930788
tensor(-9.8148, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.5486e-08, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302585
Average KL loss: 0.612863
Average total loss: 2.915448
tensor(-9.8396, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.4132e-08, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302585
Average KL loss: 0.597914
Average total loss: 2.900498
tensor(-9.8642, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.2813e-08, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302591
Average KL loss: 0.583338
Average total loss: 2.885929
tensor(-9.8889, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.1525e-08, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302585
Average KL loss: 0.569119
Average total loss: 2.871704
tensor(-9.9136, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(5.0269e-08, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302585
Average KL loss: 0.555247
Average total loss: 2.857832
tensor(-9.9383, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.9045e-08, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302585
Average KL loss: 0.541728
Average total loss: 2.844313
tensor(-9.9629, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.7851e-08, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302585
Average KL loss: 0.528546
Average total loss: 2.831131
tensor(-9.9875, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.6687e-08, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302585
Average KL loss: 0.515686
Average total loss: 2.818271
tensor(-10.0121, device='cuda:0') tensor(0.0157, device='cuda:0') tensor(4.5551e-08, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302585
Average KL loss: 0.503140
Average total loss: 2.805725
tensor(-10.0368, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(4.4443e-08, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302588
Average KL loss: 0.490907
Average total loss: 2.793496
 Percentile value: -10.03895092010498
Non-zero model percentage: 30.000001907348633%, Non-zero mask percentage: 30.000001907348633%

--- Pruning Level [1/7]: ---
conv1.weight         | nonzeros =    1715 /    1728             ( 99.25%) | total_pruned =      13 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   34095 /   36864             ( 92.49%) | total_pruned =    2769 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   34194 /   36864             ( 92.76%) | total_pruned =    2670 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   33552 /   36864             ( 91.02%) | total_pruned =    3312 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   33291 /   36864             ( 90.31%) | total_pruned =    3573 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      63 /      64             ( 98.44%) | total_pruned =       1 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   65993 /   73728             ( 89.51%) | total_pruned =    7735 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  124852 /  147456             ( 84.67%) | total_pruned =   22604 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    7995 /    8192             ( 97.60%) | total_pruned =     197 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  115991 /  147456             ( 78.66%) | total_pruned =   31465 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  113030 /  147456             ( 76.65%) | total_pruned =   34426 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  231319 /  294912             ( 78.44%) | total_pruned =   63593 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  396257 /  589824             ( 67.18%) | total_pruned =  193567 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   30552 /   32768             ( 93.24%) | total_pruned =    2216 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  272769 /  589824             ( 46.25%) | total_pruned =  317055 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  252407 /  589824             ( 42.79%) | total_pruned =  337417 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  596130 / 1179648             ( 50.53%) | total_pruned =  583518 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  528154 / 2359296             ( 22.39%) | total_pruned = 1831142 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  103030 /  131072             ( 78.61%) | total_pruned =   28042 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  261573 / 2359296             ( 11.09%) | total_pruned = 2097723 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     500 /     512             ( 97.66%) | total_pruned =      12 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  102085 / 2359296             (  4.33%) | total_pruned = 2257211 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
linear.weight        | nonzeros =    5055 /    5120             ( 98.73%) | total_pruned =      65 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3353629, pruned : 7825133, total: 11178762, Compression rate :       3.33x  ( 70.00% pruned)
Train Epoch: 29/200 Loss: 0.000481 Accuracy: 87.02 100.00 % Best test Accuracy: 87.06%
tensor(-10.0614, device='cuda:0') tensor(0.0156, device='cuda:0') tensor(4.3363e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.403998
Average total loss: 2.706583
tensor(-10.4331, device='cuda:0') tensor(0.0142, device='cuda:0') tensor(2.9850e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.286895
Average total loss: 2.589479
tensor(-10.7379, device='cuda:0') tensor(0.0132, device='cuda:0') tensor(2.1984e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.216871
Average total loss: 2.519456
tensor(-10.9903, device='cuda:0') tensor(0.0124, device='cuda:0') tensor(1.7064e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.171502
Average total loss: 2.474087
tensor(-11.2054, device='cuda:0') tensor(0.0118, device='cuda:0') tensor(1.3752e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.140138
Average total loss: 2.442722
tensor(-11.3928, device='cuda:0') tensor(0.0113, device='cuda:0') tensor(1.1396e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.117386
Average total loss: 2.419971
tensor(-11.5585, device='cuda:0') tensor(0.0108, device='cuda:0') tensor(9.6510e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.100258
Average total loss: 2.402843
tensor(-11.7071, device='cuda:0') tensor(0.0105, device='cuda:0') tensor(8.3150e-09, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.086979
Average total loss: 2.389564
tensor(-11.8418, device='cuda:0') tensor(0.0101, device='cuda:0') tensor(7.2651e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.076434
Average total loss: 2.379019
tensor(-11.9648, device='cuda:0') tensor(0.0099, device='cuda:0') tensor(6.4220e-09, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.067893
Average total loss: 2.370478
tensor(-12.0781, device='cuda:0') tensor(0.0096, device='cuda:0') tensor(5.7328e-09, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.060857
Average total loss: 2.363442
tensor(-12.1830, device='cuda:0') tensor(0.0093, device='cuda:0') tensor(5.1605e-09, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.054978
Average total loss: 2.357563
tensor(-12.2807, device='cuda:0') tensor(0.0091, device='cuda:0') tensor(4.6791e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.050004
Average total loss: 2.352589
tensor(-12.3721, device='cuda:0') tensor(0.0089, device='cuda:0') tensor(4.2694e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.045751
Average total loss: 2.348336
tensor(-12.4580, device='cuda:0') tensor(0.0087, device='cuda:0') tensor(3.9172e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.042079
Average total loss: 2.344664
tensor(-12.5390, device='cuda:0') tensor(0.0085, device='cuda:0') tensor(3.6118e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.038882
Average total loss: 2.341467
tensor(-12.6156, device='cuda:0') tensor(0.0084, device='cuda:0') tensor(3.3449e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.036078
Average total loss: 2.338663
tensor(-12.6882, device='cuda:0') tensor(0.0082, device='cuda:0') tensor(3.1099e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.033602
Average total loss: 2.336186
tensor(-12.7574, device='cuda:0') tensor(0.0080, device='cuda:0') tensor(2.9016e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.031401
Average total loss: 2.333986
tensor(-12.8233, device='cuda:0') tensor(0.0079, device='cuda:0') tensor(2.7161e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.029436
Average total loss: 2.332021
tensor(-12.8863, device='cuda:0') tensor(0.0078, device='cuda:0') tensor(2.5499e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.027671
Average total loss: 2.330256
tensor(-12.9467, device='cuda:0') tensor(0.0076, device='cuda:0') tensor(2.4003e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.026079
Average total loss: 2.328664
tensor(-13.0045, device='cuda:0') tensor(0.0075, device='cuda:0') tensor(2.2650e-09, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.024637
Average total loss: 2.327222
tensor(-13.0601, device='cuda:0') tensor(0.0074, device='cuda:0') tensor(2.1422e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.023326
Average total loss: 2.325911
tensor(-13.1136, device='cuda:0') tensor(0.0072, device='cuda:0') tensor(2.0304e-09, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.022129
Average total loss: 2.324714
tensor(-13.1652, device='cuda:0') tensor(0.0071, device='cuda:0') tensor(1.9281e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.021033
Average total loss: 2.323618
tensor(-13.2149, device='cuda:0') tensor(0.0070, device='cuda:0') tensor(1.8343e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.020026
Average total loss: 2.322611
tensor(-13.2630, device='cuda:0') tensor(0.0069, device='cuda:0') tensor(1.7480e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.019099
Average total loss: 2.321684
tensor(-13.3095, device='cuda:0') tensor(0.0068, device='cuda:0') tensor(1.6684e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.018242
Average total loss: 2.320827
tensor(-13.3545, device='cuda:0') tensor(0.0067, device='cuda:0') tensor(1.5948e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.017449
Average total loss: 2.320034
tensor(-13.3982, device='cuda:0') tensor(0.0066, device='cuda:0') tensor(1.5265e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.016713
Average total loss: 2.319298
tensor(-13.4405, device='cuda:0') tensor(0.0065, device='cuda:0') tensor(1.4631e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.016028
Average total loss: 2.318613
tensor(-13.4816, device='cuda:0') tensor(0.0064, device='cuda:0') tensor(1.4040e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.015390
Average total loss: 2.317975
tensor(-13.5216, device='cuda:0') tensor(0.0063, device='cuda:0') tensor(1.3489e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.014793
Average total loss: 2.317378
tensor(-13.5605, device='cuda:0') tensor(0.0062, device='cuda:0') tensor(1.2973e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.014235
Average total loss: 2.316820
tensor(-13.5983, device='cuda:0') tensor(0.0061, device='cuda:0') tensor(1.2491e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.013712
Average total loss: 2.316297
tensor(-13.6352, device='cuda:0') tensor(0.0060, device='cuda:0') tensor(1.2038e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.013221
Average total loss: 2.315806
tensor(-13.6711, device='cuda:0') tensor(0.0059, device='cuda:0') tensor(1.1612e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.012759
Average total loss: 2.315344
tensor(-13.7061, device='cuda:0') tensor(0.0058, device='cuda:0') tensor(1.1211e-09, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.012324
Average total loss: 2.314909
tensor(-13.7403, device='cuda:0') tensor(0.0057, device='cuda:0') tensor(1.0834e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.011913
Average total loss: 2.314498
tensor(-13.7737, device='cuda:0') tensor(0.0057, device='cuda:0') tensor(1.0477e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.011526
Average total loss: 2.314111
tensor(-13.8063, device='cuda:0') tensor(0.0056, device='cuda:0') tensor(1.0140e-09, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.011159
Average total loss: 2.313744
tensor(-13.8382, device='cuda:0') tensor(0.0055, device='cuda:0') tensor(9.8211e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.010812
Average total loss: 2.313397
tensor(-13.8693, device='cuda:0') tensor(0.0054, device='cuda:0') tensor(9.5189e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.010483
Average total loss: 2.313068
tensor(-13.8998, device='cuda:0') tensor(0.0053, device='cuda:0') tensor(9.2323e-10, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.010170
Average total loss: 2.312755
tensor(-13.9297, device='cuda:0') tensor(0.0053, device='cuda:0') tensor(8.9600e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.009873
Average total loss: 2.312458
tensor(-13.9589, device='cuda:0') tensor(0.0052, device='cuda:0') tensor(8.7012e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.009591
Average total loss: 2.312176
tensor(-13.9876, device='cuda:0') tensor(0.0051, device='cuda:0') tensor(8.4549e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.009322
Average total loss: 2.311907
tensor(-14.0157, device='cuda:0') tensor(0.0050, device='cuda:0') tensor(8.2203e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.009066
Average total loss: 2.311651
tensor(-14.0432, device='cuda:0') tensor(0.0050, device='cuda:0') tensor(7.9966e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.008821
Average total loss: 2.311406
tensor(-14.0702, device='cuda:0') tensor(0.0049, device='cuda:0') tensor(7.7831e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.008588
Average total loss: 2.311173
tensor(-14.0967, device='cuda:0') tensor(0.0048, device='cuda:0') tensor(7.5792e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.008365
Average total loss: 2.310950
tensor(-14.1226, device='cuda:0') tensor(0.0048, device='cuda:0') tensor(7.3843e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.008152
Average total loss: 2.310737
tensor(-14.1482, device='cuda:0') tensor(0.0047, device='cuda:0') tensor(7.1978e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.007948
Average total loss: 2.310533
tensor(-14.1732, device='cuda:0') tensor(0.0046, device='cuda:0') tensor(7.0192e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.007752
Average total loss: 2.310337
tensor(-14.1978, device='cuda:0') tensor(0.0046, device='cuda:0') tensor(6.8481e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.007565
Average total loss: 2.310150
tensor(-14.2220, device='cuda:0') tensor(0.0045, device='cuda:0') tensor(6.6840e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.007385
Average total loss: 2.309970
tensor(-14.2458, device='cuda:0') tensor(0.0044, device='cuda:0') tensor(6.5266e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.007213
Average total loss: 2.309797
tensor(-14.2692, device='cuda:0') tensor(0.0044, device='cuda:0') tensor(6.3754e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.007047
Average total loss: 2.309632
tensor(-14.2922, device='cuda:0') tensor(0.0043, device='cuda:0') tensor(6.2302e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.006888
Average total loss: 2.309473
tensor(-14.3148, device='cuda:0') tensor(0.0042, device='cuda:0') tensor(6.0906e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.006735
Average total loss: 2.309319
tensor(-14.3370, device='cuda:0') tensor(0.0042, device='cuda:0') tensor(5.9563e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.006587
Average total loss: 2.309172
tensor(-14.3589, device='cuda:0') tensor(0.0041, device='cuda:0') tensor(5.8270e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.006445
Average total loss: 2.309030
tensor(-14.3804, device='cuda:0') tensor(0.0041, device='cuda:0') tensor(5.7025e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.006309
Average total loss: 2.308893
tensor(-14.4017, device='cuda:0') tensor(0.0040, device='cuda:0') tensor(5.5825e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.006177
Average total loss: 2.308762
tensor(-14.4225, device='cuda:0') tensor(0.0039, device='cuda:0') tensor(5.4668e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.006050
Average total loss: 2.308635
tensor(-14.4431, device='cuda:0') tensor(0.0039, device='cuda:0') tensor(5.3552e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.005927
Average total loss: 2.308512
tensor(-14.4634, device='cuda:0') tensor(0.0038, device='cuda:0') tensor(5.2475e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.005809
Average total loss: 2.308394
tensor(-14.4834, device='cuda:0') tensor(0.0038, device='cuda:0') tensor(5.1435e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302585
Average KL loss: 0.005694
Average total loss: 2.308279
tensor(-14.5030, device='cuda:0') tensor(0.0037, device='cuda:0') tensor(5.0430e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.005584
Average total loss: 2.308169
tensor(-14.5224, device='cuda:0') tensor(0.0037, device='cuda:0') tensor(4.9459e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.005477
Average total loss: 2.308062
tensor(-14.5415, device='cuda:0') tensor(0.0036, device='cuda:0') tensor(4.8520e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.005374
Average total loss: 2.307959
tensor(-14.5604, device='cuda:0') tensor(0.0036, device='cuda:0') tensor(4.7611e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.005274
Average total loss: 2.307859
tensor(-14.5790, device='cuda:0') tensor(0.0035, device='cuda:0') tensor(4.6732e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.005177
Average total loss: 2.307762
tensor(-14.5973, device='cuda:0') tensor(0.0035, device='cuda:0') tensor(4.5881e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.005084
Average total loss: 2.307668
tensor(-14.6154, device='cuda:0') tensor(0.0034, device='cuda:0') tensor(4.5056e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.004993
Average total loss: 2.307578
tensor(-14.6333, device='cuda:0') tensor(0.0034, device='cuda:0') tensor(4.4257e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 0.004905
Average total loss: 2.307490
tensor(-14.6509, device='cuda:0') tensor(0.0033, device='cuda:0') tensor(4.3483e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.004820
Average total loss: 2.307404
tensor(-14.6682, device='cuda:0') tensor(0.0033, device='cuda:0') tensor(4.2732e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302585
Average KL loss: 0.004737
Average total loss: 2.307322
tensor(-14.6854, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.2004e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.004657
Average total loss: 2.307241
tensor(-14.7023, device='cuda:0') tensor(0.0032, device='cuda:0') tensor(4.1297e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.004579
Average total loss: 2.307164
tensor(-14.7190, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(4.0610e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.004503
Average total loss: 2.307088
tensor(-14.7356, device='cuda:0') tensor(0.0031, device='cuda:0') tensor(3.9944e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.004430
Average total loss: 2.307015
tensor(-14.7519, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(3.9297e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 0.004358
Average total loss: 2.306943
tensor(-14.7680, device='cuda:0') tensor(0.0030, device='cuda:0') tensor(3.8668e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.004289
Average total loss: 2.306874
tensor(-14.7839, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(3.8056e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 0.004221
Average total loss: 2.306806
tensor(-14.7996, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(3.7461e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302585
Average KL loss: 0.004156
Average total loss: 2.306741
tensor(-14.8151, device='cuda:0') tensor(0.0029, device='cuda:0') tensor(3.6883e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 0.004092
Average total loss: 2.306677
tensor(-14.8304, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(3.6321e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.004030
Average total loss: 2.306615
tensor(-14.8456, device='cuda:0') tensor(0.0028, device='cuda:0') tensor(3.5773e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.003970
Average total loss: 2.306555
tensor(-14.8605, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(3.5240e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.003911
Average total loss: 2.306496
tensor(-14.8754, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(3.4721e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.003854
Average total loss: 2.306438
tensor(-14.8900, device='cuda:0') tensor(0.0027, device='cuda:0') tensor(3.4215e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 0.003798
Average total loss: 2.306383
tensor(-14.9045, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(3.3723e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 0.003743
Average total loss: 2.306328
tensor(-14.9188, device='cuda:0') tensor(0.0026, device='cuda:0') tensor(3.3243e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 0.003690
Average total loss: 2.306275
tensor(-14.9329, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(3.2775e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.003639
Average total loss: 2.306224
tensor(-14.9469, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(3.2319e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 0.003588
Average total loss: 2.306173
tensor(-14.9607, device='cuda:0') tensor(0.0025, device='cuda:0') tensor(3.1874e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.003539
Average total loss: 2.306124
tensor(-14.9744, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(3.1439e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 0.003491
Average total loss: 2.306076
tensor(-14.9879, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(3.1016e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.003444
Average total loss: 2.306029
tensor(-15.0013, device='cuda:0') tensor(0.0024, device='cuda:0') tensor(3.0602e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 0.003399
Average total loss: 2.305984
tensor(-15.0146, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(3.0199e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.003354
Average total loss: 2.305939
tensor(-15.0277, device='cuda:0') tensor(0.0023, device='cuda:0') tensor(2.9805e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.003311
Average total loss: 2.305896
tensor(-15.0406, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(2.9420e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.003268
Average total loss: 2.305853
tensor(-15.0535, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(2.9044e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.003227
Average total loss: 2.305811
tensor(-15.0662, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(2.8677e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.003186
Average total loss: 2.305771
tensor(-15.0787, device='cuda:0') tensor(0.0022, device='cuda:0') tensor(2.8318e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.003146
Average total loss: 2.305731
tensor(-15.0912, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(2.7967e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.003107
Average total loss: 2.305692
tensor(-15.1035, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(2.7624e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.003070
Average total loss: 2.305654
tensor(-15.1157, device='cuda:0') tensor(0.0021, device='cuda:0') tensor(2.7288e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.003032
Average total loss: 2.305617
tensor(-15.1278, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.6960e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.002996
Average total loss: 2.305581
tensor(-15.1397, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.6639e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.002961
Average total loss: 2.305546
tensor(-15.1515, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.6325e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.002926
Average total loss: 2.305511
tensor(-15.1633, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.6018e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.002892
Average total loss: 2.305477
tensor(-15.1749, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.5717e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.002859
Average total loss: 2.305444
tensor(-15.1864, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.5423e-10, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 0.002826
Average total loss: 2.305411
tensor(-15.1977, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(2.5134e-10, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 0.002794
Average total loss: 2.305379
tensor(-15.2090, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(2.4852e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.002763
Average total loss: 2.305348
tensor(-15.2202, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(2.4575e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.002732
Average total loss: 2.305317
tensor(-15.2313, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(2.4304e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.002702
Average total loss: 2.305287
tensor(-15.2422, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(2.4039e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.002673
Average total loss: 2.305258
tensor(-15.2531, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(2.3779e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.002644
Average total loss: 2.305229
tensor(-15.2638, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(2.3524e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.002616
Average total loss: 2.305201
tensor(-15.2745, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(2.3274e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302585
Average KL loss: 0.002588
Average total loss: 2.305173
tensor(-15.2851, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(2.3028e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 0.002561
Average total loss: 2.305146
tensor(-15.2956, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(2.2788e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.002534
Average total loss: 2.305119
tensor(-15.3059, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(2.2552e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.002508
Average total loss: 2.305093
tensor(-15.3162, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(2.2321e-10, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.002483
Average total loss: 2.305068
tensor(-15.3264, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.2094e-10, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.002458
Average total loss: 2.305043
tensor(-15.3365, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.1872e-10, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.002433
Average total loss: 2.305018
tensor(-15.3465, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.1653e-10, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.002409
Average total loss: 2.304994
tensor(-15.3565, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.1439e-10, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.002385
Average total loss: 2.304970
tensor(-15.3663, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.1228e-10, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 0.002362
Average total loss: 2.304947
tensor(-15.3761, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.1022e-10, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.002339
Average total loss: 2.304924
tensor(-15.3858, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.0819e-10, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.002316
Average total loss: 2.304901
tensor(-15.3954, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.0619e-10, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.002294
Average total loss: 2.304879
tensor(-15.4049, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.0424e-10, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.002273
Average total loss: 2.304857
tensor(-15.4143, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(2.0231e-10, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.002251
Average total loss: 2.304836
tensor(-15.4237, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(2.0043e-10, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.002230
Average total loss: 2.304815
tensor(-15.4330, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(1.9857e-10, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.002210
Average total loss: 2.304795
tensor(-15.4422, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(1.9675e-10, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.002190
Average total loss: 2.304774
tensor(-15.4513, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(1.9496e-10, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.002170
Average total loss: 2.304755
tensor(-15.4604, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(1.9319e-10, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.002150
Average total loss: 2.304735
tensor(-15.4694, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.9146e-10, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.002131
Average total loss: 2.304716
tensor(-15.4783, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8976e-10, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.002112
Average total loss: 2.304697
tensor(-15.4872, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8809e-10, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.002094
Average total loss: 2.304678
tensor(-15.4959, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8644e-10, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.002083
Average total loss: 2.304668
tensor(-15.4968, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8627e-10, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 0.002081
Average total loss: 2.304666
tensor(-15.4977, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8611e-10, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.002080
Average total loss: 2.304664
tensor(-15.4986, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8594e-10, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.002078
Average total loss: 2.304663
tensor(-15.4995, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8578e-10, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.002076
Average total loss: 2.304661
tensor(-15.5004, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8561e-10, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.002074
Average total loss: 2.304659
tensor(-15.5013, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8545e-10, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.002072
Average total loss: 2.304657
tensor(-15.5022, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8528e-10, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.002070
Average total loss: 2.304655
tensor(-15.5031, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8512e-10, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.002068
Average total loss: 2.304653
tensor(-15.5039, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8495e-10, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304651
tensor(-15.5048, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8479e-10, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5057, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8462e-10, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.002063
Average total loss: 2.304648
tensor(-15.5066, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8446e-10, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.002062
Average total loss: 2.304647
tensor(-15.5067, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8444e-10, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.002062
Average total loss: 2.304647
tensor(-15.5068, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8442e-10, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.002062
Average total loss: 2.304646
tensor(-15.5069, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8440e-10, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.002061
Average total loss: 2.304646
tensor(-15.5070, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8439e-10, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.002061
Average total loss: 2.304646
tensor(-15.5071, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8437e-10, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.002061
Average total loss: 2.304646
tensor(-15.5072, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8435e-10, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.002061
Average total loss: 2.304646
tensor(-15.5073, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8434e-10, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.002061
Average total loss: 2.304645
tensor(-15.5074, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8432e-10, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5075, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8430e-10, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5075, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8428e-10, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.002060
Average total loss: 2.304645
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
 Percentile value: -15.473658561706543
Non-zero model percentage: 9.000003814697266%, Non-zero mask percentage: 9.000003814697266%

--- Pruning Level [2/7]: ---
conv1.weight         | nonzeros =    1701 /    1728             ( 98.44%) | total_pruned =      27 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   28824 /   36864             ( 78.19%) | total_pruned =    8040 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   29272 /   36864             ( 79.41%) | total_pruned =    7592 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   27243 /   36864             ( 73.90%) | total_pruned =    9621 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   26809 /   36864             ( 72.72%) | total_pruned =   10055 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   51451 /   73728             ( 69.78%) | total_pruned =   22277 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   85449 /  147456             ( 57.95%) | total_pruned =   62007 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    7609 /    8192             ( 92.88%) | total_pruned =     583 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   64300 /  147456             ( 43.61%) | total_pruned =   83156 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   59910 /  147456             ( 40.63%) | total_pruned =   87546 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  128724 /  294912             ( 43.65%) | total_pruned =  166188 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  150294 /  589824             ( 25.48%) | total_pruned =  439530 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   26188 /   32768             ( 79.92%) | total_pruned =    6580 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   40850 /  589824             (  6.93%) | total_pruned =  548974 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   36598 /  589824             (  6.20%) | total_pruned =  553226 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  113596 / 1179648             (  9.63%) | total_pruned = 1066052 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   37059 / 2359296             (  1.57%) | total_pruned = 2322237 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   56741 /  131072             ( 43.29%) | total_pruned =   74331 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   11105 / 2359296             (  0.47%) | total_pruned = 2348191 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     409 /     512             ( 79.88%) | total_pruned =     103 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    7977 / 2359296             (  0.34%) | total_pruned = 2351319 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     498 /     512             ( 97.27%) | total_pruned =      14 | shape = torch.Size([512])
linear.weight        | nonzeros =    4905 /    5120             ( 95.80%) | total_pruned =     215 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1006089, pruned : 10172673, total: 11178762, Compression rate :      11.11x  ( 91.00% pruned)
Train Epoch: 23/200 Loss: 0.002924 Accuracy: 86.16 100.00 % Best test Accuracy: 86.48%
tensor(-15.5076, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(1.8427e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.002051
Average total loss: 2.304636
tensor(-15.5165, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.8264e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.002033
Average total loss: 2.304618
tensor(-15.5252, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.8104e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.002015
Average total loss: 2.304600
tensor(-15.5339, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(1.7947e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001998
Average total loss: 2.304583
tensor(-15.5425, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7793e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001981
Average total loss: 2.304566
tensor(-15.5511, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7641e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001964
Average total loss: 2.304549
tensor(-15.5596, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7492e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001947
Average total loss: 2.304532
tensor(-15.5680, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7346e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001931
Average total loss: 2.304516
tensor(-15.5763, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(1.7201e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001915
Average total loss: 2.304500
tensor(-15.5845, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.7060e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001899
Average total loss: 2.304484
tensor(-15.5927, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6920e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001884
Average total loss: 2.304469
tensor(-15.6009, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6783e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001869
Average total loss: 2.304454
tensor(-15.6089, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6648e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001860
Average total loss: 2.304445
tensor(-15.6097, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6635e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001859
Average total loss: 2.304444
tensor(-15.6105, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6621e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001857
Average total loss: 2.304442
tensor(-15.6113, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6608e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001856
Average total loss: 2.304441
tensor(-15.6121, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6595e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001854
Average total loss: 2.304439
tensor(-15.6129, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6582e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001853
Average total loss: 2.304438
tensor(-15.6137, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6568e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001851
Average total loss: 2.304436
tensor(-15.6145, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6555e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001850
Average total loss: 2.304435
tensor(-15.6153, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6542e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6161, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001847
Average total loss: 2.304432
tensor(-15.6169, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6516e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001846
Average total loss: 2.304430
tensor(-15.6177, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6502e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001845
Average total loss: 2.304430
tensor(-15.6178, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6501e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001845
Average total loss: 2.304429
tensor(-15.6179, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6499e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-15.6180, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6498e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-15.6181, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6496e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-15.6182, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6495e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-15.6183, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6493e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-15.6184, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6492e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6184, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6490e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6185, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6488e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6186, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6487e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001843
Average total loss: 2.304428
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
 Percentile value: -15.515527725219727
Non-zero model percentage: 2.7000038623809814%, Non-zero mask percentage: 2.7000038623809814%

--- Pruning Level [3/7]: ---
conv1.weight         | nonzeros =    1674 /    1728             ( 96.88%) | total_pruned =      54 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   20526 /   36864             ( 55.68%) | total_pruned =   16338 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      61 /      64             ( 95.31%) | total_pruned =       3 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   20991 /   36864             ( 56.94%) | total_pruned =   15873 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   17459 /   36864             ( 47.36%) | total_pruned =   19405 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   16500 /   36864             ( 44.76%) | total_pruned =   20364 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   29246 /   73728             ( 39.67%) | total_pruned =   44482 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   36146 /  147456             ( 24.51%) | total_pruned =  111310 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    6796 /    8192             ( 82.96%) | total_pruned =    1396 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   17212 /  147456             ( 11.67%) | total_pruned =  130244 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   14488 /  147456             (  9.83%) | total_pruned =  132968 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   33073 /  294912             ( 11.21%) | total_pruned =  261839 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   22154 /  589824             (  3.76%) | total_pruned =  567670 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   18228 /   32768             ( 55.63%) | total_pruned =   14540 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2257 /  589824             (  0.38%) | total_pruned =  587567 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    2328 /  589824             (  0.39%) | total_pruned =  587496 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    7136 / 1179648             (  0.60%) | total_pruned = 1172512 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    3206 / 2359296             (  0.14%) | total_pruned = 2356090 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   16326 /  131072             ( 12.46%) | total_pruned =  114746 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     998 / 2359296             (  0.04%) | total_pruned = 2358298 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     507 /     512             ( 99.02%) | total_pruned =       5 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      92 /     512             ( 17.97%) | total_pruned =     420 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    1366 / 2359296             (  0.06%) | total_pruned = 2357930 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     419 /     512             ( 81.84%) | total_pruned =      93 | shape = torch.Size([512])
linear.weight        | nonzeros =    4648 /    5120             ( 90.78%) | total_pruned =     472 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 301827, pruned : 10876935, total: 11178762, Compression rate :      37.04x  ( 97.30% pruned)
Train Epoch: 30/200 Loss: 0.002610 Accuracy: 83.09 100.00 % Best test Accuracy: 83.32%
tensor(-15.6187, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(1.6485e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001836
Average total loss: 2.304421
tensor(-15.6267, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.6355e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001821
Average total loss: 2.304406
tensor(-15.6345, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.6227e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001807
Average total loss: 2.304392
tensor(-15.6423, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.6100e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001793
Average total loss: 2.304378
tensor(-15.6501, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.5976e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001779
Average total loss: 2.304364
tensor(-15.6578, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.5854e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001766
Average total loss: 2.304350
tensor(-15.6654, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(1.5733e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001752
Average total loss: 2.304337
tensor(-15.6729, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5614e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001739
Average total loss: 2.304324
tensor(-15.6805, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5497e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001726
Average total loss: 2.304311
tensor(-15.6879, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5382e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001713
Average total loss: 2.304298
tensor(-15.6953, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5268e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001701
Average total loss: 2.304286
tensor(-15.7027, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5156e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001688
Average total loss: 2.304273
tensor(-15.7100, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5046e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001681
Average total loss: 2.304266
tensor(-15.7107, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5035e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001680
Average total loss: 2.304265
tensor(-15.7114, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5025e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001679
Average total loss: 2.304264
tensor(-15.7121, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5014e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001678
Average total loss: 2.304263
tensor(-15.7128, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.5003e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001677
Average total loss: 2.304262
tensor(-15.7135, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4993e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001675
Average total loss: 2.304260
tensor(-15.7142, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4982e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001674
Average total loss: 2.304259
tensor(-15.7149, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4972e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001673
Average total loss: 2.304258
tensor(-15.7156, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4961e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7163, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4950e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7170, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4940e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001670
Average total loss: 2.304254
tensor(-15.7177, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4929e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001669
Average total loss: 2.304254
tensor(-15.7178, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4928e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001669
Average total loss: 2.304254
tensor(-15.7179, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4927e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001669
Average total loss: 2.304253
tensor(-15.7180, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4925e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-15.7181, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4924e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-15.7182, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4922e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-15.7183, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4921e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-15.7184, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4920e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-15.7185, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4918e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304252
tensor(-15.7186, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4917e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7187, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4915e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
 Percentile value: -15.496134757995605
Non-zero model percentage: 0.8100091218948364%, Non-zero mask percentage: 0.8100091218948364%

--- Pruning Level [4/7]: ---
conv1.weight         | nonzeros =    1610 /    1728             ( 93.17%) | total_pruned =     118 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   11161 /   36864             ( 30.28%) | total_pruned =   25703 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      56 /      64             ( 87.50%) | total_pruned =       8 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   10841 /   36864             ( 29.41%) | total_pruned =   26023 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7647 /   36864             ( 20.74%) | total_pruned =   29217 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    6409 /   36864             ( 17.39%) | total_pruned =   30455 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    8886 /   73728             ( 12.05%) | total_pruned =   64842 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     125 /     128             ( 97.66%) | total_pruned =       3 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    6776 /  147456             (  4.60%) | total_pruned =  140680 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    5306 /    8192             ( 64.77%) | total_pruned =    2886 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    1614 /  147456             (  1.09%) | total_pruned =  145842 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =     125 /     128             ( 97.66%) | total_pruned =       3 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    1235 /  147456             (  0.84%) | total_pruned =  146221 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     127 /     128             ( 99.22%) | total_pruned =       1 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    2963 /  294912             (  1.00%) | total_pruned =  291949 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    1428 /  589824             (  0.24%) | total_pruned =  588396 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     254 /     256             ( 99.22%) | total_pruned =       2 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    7888 /   32768             ( 24.07%) | total_pruned =   24880 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     254 /     256             ( 99.22%) | total_pruned =       2 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     158 /  589824             (  0.03%) | total_pruned =  589666 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     165 /     256             ( 64.45%) | total_pruned =      91 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     217 /  589824             (  0.04%) | total_pruned =  589607 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     255 /     256             ( 99.61%) | total_pruned =       1 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     502 / 1179648             (  0.04%) | total_pruned = 1179146 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     461 /     512             ( 90.04%) | total_pruned =      51 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     505 / 2359296             (  0.02%) | total_pruned = 2358791 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     475 /     512             ( 92.77%) | total_pruned =      37 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    2485 /  131072             (  1.90%) | total_pruned =  128587 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     470 /     512             ( 91.80%) | total_pruned =      42 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     128 / 2359296             (  0.01%) | total_pruned = 2359168 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     488 /     512             ( 95.31%) | total_pruned =      24 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     242 / 2359296             (  0.01%) | total_pruned = 2359054 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =      77 /     512             ( 15.04%) | total_pruned =     435 | shape = torch.Size([512])
linear.weight        | nonzeros =    4186 /    5120             ( 81.76%) | total_pruned =     934 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       7 /      10             ( 70.00%) | total_pruned =       3 | shape = torch.Size([10])
alive: 90549, pruned : 11088213, total: 11178762, Compression rate :     123.46x  ( 99.19% pruned)
Train Epoch: 87/200 Loss: 0.010916 Accuracy: 73.84 100.00 % Best test Accuracy: 74.22%
tensor(-15.7188, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.4914e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-15.7260, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4807e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001649
Average total loss: 2.304234
tensor(-15.7331, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4702e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001638
Average total loss: 2.304223
tensor(-15.7402, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4598e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001626
Average total loss: 2.304211
tensor(-15.7472, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4495e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001615
Average total loss: 2.304200
tensor(-15.7542, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4394e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001604
Average total loss: 2.304189
tensor(-15.7611, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4295e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001593
Average total loss: 2.304177
tensor(-15.7680, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4197e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001582
Average total loss: 2.304167
tensor(-15.7749, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4100e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001571
Average total loss: 2.304156
tensor(-15.7817, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.4004e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001560
Average total loss: 2.304145
tensor(-15.7884, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(1.3910e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001550
Average total loss: 2.304135
tensor(-15.7951, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3817e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001540
Average total loss: 2.304124
tensor(-15.8018, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3725e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001534
Average total loss: 2.304119
tensor(-15.8024, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3716e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001533
Average total loss: 2.304118
tensor(-15.8031, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3707e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001532
Average total loss: 2.304117
tensor(-15.8037, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3698e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001531
Average total loss: 2.304116
tensor(-15.8044, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3689e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001530
Average total loss: 2.304115
tensor(-15.8051, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3680e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001529
Average total loss: 2.304114
tensor(-15.8057, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3671e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001528
Average total loss: 2.304113
tensor(-15.8064, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3662e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001527
Average total loss: 2.304112
tensor(-15.8070, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3653e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8077, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3644e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001525
Average total loss: 2.304110
tensor(-15.8083, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3635e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001524
Average total loss: 2.304109
tensor(-15.8090, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3626e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8090, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3626e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8091, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3625e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8091, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3624e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8092, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3624e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8092, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3623e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8093, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3622e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8093, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3622e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-15.8094, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3621e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304107
tensor(-15.8094, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3620e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3620e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
 Percentile value: -15.415421676635741
Non-zero model percentage: 0.2430054396390915%, Non-zero mask percentage: 0.2430054396390915%

--- Pruning Level [5/7]: ---
conv1.weight         | nonzeros =    1491 /    1728             ( 86.28%) | total_pruned =     237 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3753 /   36864             ( 10.18%) | total_pruned =   33111 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3049 /   36864             (  8.27%) | total_pruned =   33815 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1964 /   36864             (  5.33%) | total_pruned =   34900 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      62 /      64             ( 96.88%) | total_pruned =       2 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1134 /   36864             (  3.08%) | total_pruned =   35730 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     893 /   73728             (  1.21%) | total_pruned =   72835 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     434 /  147456             (  0.29%) | total_pruned =  147022 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      87 /     128             ( 67.97%) | total_pruned =      41 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2919 /    8192             ( 35.63%) | total_pruned =    5273 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      57 /  147456             (  0.04%) | total_pruned =  147399 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      56 /  147456             (  0.04%) | total_pruned =  147400 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     123 /     128             ( 96.09%) | total_pruned =       5 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     142 /  294912             (  0.05%) | total_pruned =  294770 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     220 /     256             ( 85.94%) | total_pruned =      36 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     108 /  589824             (  0.02%) | total_pruned =  589716 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     113 /     256             ( 44.14%) | total_pruned =     143 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1292 /   32768             (  3.94%) | total_pruned =   31476 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      16 /  589824             (  0.00%) | total_pruned =  589808 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       5 /     256             (  1.95%) | total_pruned =     251 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      28 /  589824             (  0.00%) | total_pruned =  589796 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =      76 / 1179648             (  0.01%) | total_pruned = 1179572 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     511 /     512             ( 99.80%) | total_pruned =       1 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =      60 /     512             ( 11.72%) | total_pruned =     452 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =      63 / 2359296             (  0.00%) | total_pruned = 2359233 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =      65 /     512             ( 12.70%) | total_pruned =     447 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     230 /  131072             (  0.18%) | total_pruned =  130842 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =      57 /     512             ( 11.13%) | total_pruned =     455 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =      10 / 2359296             (  0.00%) | total_pruned = 2359286 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =      38 / 2359296             (  0.00%) | total_pruned = 2359258 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    3251 /    5120             ( 63.50%) | total_pruned =    1869 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       2 /      10             ( 20.00%) | total_pruned =       8 | shape = torch.Size([10])
alive: 27165, pruned : 11151597, total: 11178762, Compression rate :     411.51x  ( 99.76% pruned)
Train Epoch: 196/200 Loss: 0.663328 Accuracy: 60.98 81.67 % Best test Accuracy: 62.44%
tensor(-15.8095, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3619e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001518
Average total loss: 2.304102
tensor(-15.8161, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3530e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001508
Average total loss: 2.304093
tensor(-15.8226, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3442e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001498
Average total loss: 2.304083
tensor(-15.8291, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3355e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001488
Average total loss: 2.304073
tensor(-15.8355, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3269e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001479
Average total loss: 2.304064
tensor(-15.8419, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3184e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001469
Average total loss: 2.304054
tensor(-15.8483, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3101e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001460
Average total loss: 2.304045
tensor(-15.8546, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.3018e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001451
Average total loss: 2.304036
tensor(-15.8609, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.2936e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001442
Average total loss: 2.304027
tensor(-15.8672, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.2856e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001433
Average total loss: 2.304018
tensor(-15.8734, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.2776e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001424
Average total loss: 2.304009
tensor(-15.8795, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2698e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001415
Average total loss: 2.304000
tensor(-15.8857, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2620e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001410
Average total loss: 2.303995
tensor(-15.8863, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2612e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001409
Average total loss: 2.303994
tensor(-15.8869, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2605e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001409
Average total loss: 2.303994
tensor(-15.8875, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2597e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001408
Average total loss: 2.303993
tensor(-15.8881, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2589e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001407
Average total loss: 2.303992
tensor(-15.8887, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2582e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001406
Average total loss: 2.303991
tensor(-15.8893, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2574e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001405
Average total loss: 2.303990
tensor(-15.8899, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2566e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8905, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2559e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001403
Average total loss: 2.303988
tensor(-15.8911, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2551e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001403
Average total loss: 2.303988
tensor(-15.8917, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2543e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001402
Average total loss: 2.303987
tensor(-15.8924, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2536e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8924, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2535e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8924, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2534e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8925, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2534e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8925, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2533e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8926, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2533e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8926, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2532e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8927, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2531e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8927, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2531e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8928, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2530e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8928, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2530e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-15.8929, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
 Percentile value: -15.363252353668212
Non-zero model percentage: 0.07290609925985336%, Non-zero mask percentage: 0.07290609925985336%

--- Pruning Level [6/7]: ---
conv1.weight         | nonzeros =    1194 /    1728             ( 69.10%) | total_pruned =     534 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =      35 /      64             ( 54.69%) | total_pruned =      29 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     535 /   36864             (  1.45%) | total_pruned =   36329 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     324 /   36864             (  0.88%) | total_pruned =   36540 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     236 /   36864             (  0.64%) | total_pruned =   36628 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =      43 /   36864             (  0.12%) | total_pruned =   36821 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =      32 /   73728             (  0.04%) | total_pruned =   73696 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =      14 /  147456             (  0.01%) | total_pruned =  147442 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     432 /    8192             (  5.27%) | total_pruned =    7760 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       4 /     128             (  3.12%) | total_pruned =     124 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       2 /  147456             (  0.00%) | total_pruned =  147454 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     126 /     128             ( 98.44%) | total_pruned =       2 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       6 /  147456             (  0.00%) | total_pruned =  147450 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      22 /     128             ( 17.19%) | total_pruned =     106 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =      10 /  294912             (  0.00%) | total_pruned =  294902 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       8 /     256             (  3.12%) | total_pruned =     248 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =      11 /  589824             (  0.00%) | total_pruned =  589813 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       1 /     256             (  0.39%) | total_pruned =     255 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =      21 /   32768             (  0.06%) | total_pruned =   32747 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       1 /     256             (  0.39%) | total_pruned =     255 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       3 /  589824             (  0.00%) | total_pruned =  589821 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     144 /     256             ( 56.25%) | total_pruned =     112 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       3 /  589824             (  0.00%) | total_pruned =  589821 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     228 /     256             ( 89.06%) | total_pruned =      28 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =       9 / 1179648             (  0.00%) | total_pruned = 1179639 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     325 /     512             ( 63.48%) | total_pruned =     187 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =       6 / 2359296             (  0.00%) | total_pruned = 2359290 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     443 /     512             ( 86.52%) | total_pruned =      69 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =      13 /  131072             (  0.01%) | total_pruned =  131059 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     419 /     512             ( 81.84%) | total_pruned =      93 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =       2 / 2359296             (  0.00%) | total_pruned = 2359294 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =      91 /     512             ( 17.77%) | total_pruned =     421 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =       0 / 2359296             (  0.00%) | total_pruned = 2359296 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     463 /     512             ( 90.43%) | total_pruned =      49 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    1247 /    5120             ( 24.36%) | total_pruned =    3873 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =       1 /      10             ( 10.00%) | total_pruned =       9 | shape = torch.Size([10])
alive: 8150, pruned : 11170612, total: 11178762, Compression rate :    1371.63x  ( 99.93% pruned)
Train Epoch: 63/200 Loss: 2.082263 Accuracy: 17.04 17.13 % Best test Accuracy: 17.31%
