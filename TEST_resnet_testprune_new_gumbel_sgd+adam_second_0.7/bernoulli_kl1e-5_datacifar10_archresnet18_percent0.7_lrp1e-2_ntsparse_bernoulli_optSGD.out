Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/7]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/200 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(2.4999e-06, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302930
Average KL loss: 49.295101
Average total loss: 51.598030
tensor(-0.4850, device='cuda:0') tensor(4.8069e-05, device='cuda:0') tensor(2.3585e-06, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.303218
Average KL loss: 36.830981
Average total loss: 39.134198
tensor(-0.9448, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.0155e-06, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.303080
Average KL loss: 26.944062
Average total loss: 29.247141
tensor(-1.3560, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(1.6291e-06, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.303138
Average KL loss: 19.873744
Average total loss: 22.176881
tensor(-1.7124, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(1.2948e-06, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.303197
Average KL loss: 15.013430
Average total loss: 17.316627
tensor(-2.0193, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(1.0345e-06, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302890
Average KL loss: 11.663687
Average total loss: 13.966577
tensor(-2.2851, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(8.3843e-07, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302922
Average KL loss: 9.304117
Average total loss: 11.607038
tensor(-2.5178, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(6.9055e-07, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302863
Average KL loss: 7.595405
Average total loss: 9.898268
tensor(-2.7240, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(5.7812e-07, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302863
Average KL loss: 6.323510
Average total loss: 8.626373
tensor(-2.9086, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(4.9076e-07, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302717
Average KL loss: 5.352638
Average total loss: 7.655355
tensor(-3.0757, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(4.2192e-07, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.303046
Average KL loss: 4.594841
Average total loss: 6.897888
tensor(-3.2282, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(3.6685e-07, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302800
Average KL loss: 3.991697
Average total loss: 6.294497
tensor(-3.3684, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(3.2215e-07, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302786
Average KL loss: 3.503394
Average total loss: 5.806180
tensor(-3.4982, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(2.8535e-07, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302809
Average KL loss: 3.102131
Average total loss: 5.404940
tensor(-3.6192, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.5450e-07, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302858
Average KL loss: 2.768051
Average total loss: 5.070908
tensor(-3.7324, device='cuda:0') tensor(0.0014, device='cuda:0') tensor(2.2858e-07, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302808
Average KL loss: 2.486679
Average total loss: 4.789487
tensor(-3.8388, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(2.0648e-07, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302866
Average KL loss: 2.247257
Average total loss: 4.550122
tensor(-3.9393, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(1.8751e-07, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302784
Average KL loss: 2.041661
Average total loss: 4.344445
tensor(-4.0345, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(1.7107e-07, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302799
Average KL loss: 1.863676
Average total loss: 4.166475
tensor(-4.1251, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.5682e-07, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302800
Average KL loss: 1.708448
Average total loss: 4.011249
tensor(-4.2114, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.4408e-07, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302597
Average KL loss: 1.572163
Average total loss: 3.874760
tensor(-4.2939, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.3309e-07, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302671
Average KL loss: 1.451785
Average total loss: 3.754455
tensor(-4.3730, device='cuda:0') tensor(0.0016, device='cuda:0') tensor(1.2306e-07, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302635
Average KL loss: 1.344870
Average total loss: 3.647504
tensor(-4.4489, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.1435e-07, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302782
Average KL loss: 1.249432
Average total loss: 3.552214
tensor(-4.5220, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(1.0651e-07, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302782
Average KL loss: 1.163844
Average total loss: 3.466626
tensor(-4.5925, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(9.9472e-08, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302641
Average KL loss: 1.086766
Average total loss: 3.389408
tensor(-4.6606, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(9.3128e-08, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302680
Average KL loss: 1.017077
Average total loss: 3.319757
tensor(-4.7264, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(8.7159e-08, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302713
Average KL loss: 0.953837
Average total loss: 3.256550
tensor(-4.7902, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(8.1938e-08, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302689
Average KL loss: 0.896255
Average total loss: 3.198945
tensor(-4.8521, device='cuda:0') tensor(0.0017, device='cuda:0') tensor(7.7151e-08, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302760
Average KL loss: 0.843665
Average total loss: 3.146426
tensor(-4.9122, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.2662e-08, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302583
Average KL loss: 0.795490
Average total loss: 3.098073
tensor(-4.9706, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.8548e-08, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302752
Average KL loss: 0.751240
Average total loss: 3.053992
tensor(-5.0275, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.4880e-08, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302727
Average KL loss: 0.710490
Average total loss: 3.013218
tensor(-5.0830, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.1361e-08, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302732
Average KL loss: 0.672871
Average total loss: 2.975603
tensor(-5.1371, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.8148e-08, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302695
Average KL loss: 0.638060
Average total loss: 2.940755
tensor(-5.1899, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.5224e-08, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302717
Average KL loss: 0.605784
Average total loss: 2.908501
tensor(-5.2416, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(5.2456e-08, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302615
Average KL loss: 0.575800
Average total loss: 2.878414
tensor(-5.2921, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.9910e-08, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302648
Average KL loss: 0.547891
Average total loss: 2.850538
tensor(-5.3415, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.7550e-08, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302629
Average KL loss: 0.521866
Average total loss: 2.824494
tensor(-5.3900, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(4.5381e-08, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302652
Average KL loss: 0.497553
Average total loss: 2.800205
tensor(-5.4375, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(4.3243e-08, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302664
Average KL loss: 0.474809
Average total loss: 2.777473
tensor(-5.4840, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(4.1259e-08, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302603
Average KL loss: 0.453493
Average total loss: 2.756096
tensor(-5.5298, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.9538e-08, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302667
Average KL loss: 0.433489
Average total loss: 2.736155
tensor(-5.5747, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.7800e-08, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302671
Average KL loss: 0.414691
Average total loss: 2.717362
tensor(-5.6188, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.6188e-08, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302671
Average KL loss: 0.397003
Average total loss: 2.699675
tensor(-5.6622, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.4652e-08, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302540
Average KL loss: 0.380341
Average total loss: 2.682880
tensor(-5.7050, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.3178e-08, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302610
Average KL loss: 0.364624
Average total loss: 2.667234
tensor(-5.7470, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.1828e-08, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302651
Average KL loss: 0.349781
Average total loss: 2.652432
tensor(-5.7884, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(3.0512e-08, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302640
Average KL loss: 0.335748
Average total loss: 2.638388
tensor(-5.8292, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.9320e-08, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302644
Average KL loss: 0.322468
Average total loss: 2.625111
tensor(-5.8694, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.8145e-08, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302668
Average KL loss: 0.309887
Average total loss: 2.612555
tensor(-5.9091, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.7093e-08, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302573
Average KL loss: 0.297958
Average total loss: 2.600531
tensor(-5.9482, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.6047e-08, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302664
Average KL loss: 0.286638
Average total loss: 2.589301
tensor(-5.9868, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.5082e-08, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302606
Average KL loss: 0.275882
Average total loss: 2.578488
tensor(-6.0249, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.4165e-08, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302612
Average KL loss: 0.265657
Average total loss: 2.568269
tensor(-6.0626, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.3243e-08, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302648
Average KL loss: 0.255927
Average total loss: 2.558575
tensor(-6.0998, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.2387e-08, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302587
Average KL loss: 0.246663
Average total loss: 2.549250
tensor(-6.1366, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.1604e-08, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302584
Average KL loss: 0.237834
Average total loss: 2.540417
tensor(-6.1729, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.0806e-08, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302644
Average KL loss: 0.229413
Average total loss: 2.532057
tensor(-6.2089, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(2.0100e-08, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302587
Average KL loss: 0.221375
Average total loss: 2.523963
tensor(-6.2444, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.9405e-08, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302630
Average KL loss: 0.213699
Average total loss: 2.516329
tensor(-6.2796, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8720e-08, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302609
Average KL loss: 0.206365
Average total loss: 2.508974
tensor(-6.3144, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8132e-08, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302614
Average KL loss: 0.199353
Average total loss: 2.501967
tensor(-6.3489, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7494e-08, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302614
Average KL loss: 0.192643
Average total loss: 2.495257
tensor(-6.3831, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6908e-08, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302632
Average KL loss: 0.186221
Average total loss: 2.488852
tensor(-6.4169, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6355e-08, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302614
Average KL loss: 0.180070
Average total loss: 2.482683
tensor(-6.4504, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5831e-08, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302601
Average KL loss: 0.174175
Average total loss: 2.476776
tensor(-6.4836, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5279e-08, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302645
Average KL loss: 0.168522
Average total loss: 2.471166
tensor(-6.5166, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4794e-08, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302591
Average KL loss: 0.163098
Average total loss: 2.465689
tensor(-6.5492, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4310e-08, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302553
Average KL loss: 0.157891
Average total loss: 2.460444
tensor(-6.5816, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3884e-08, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302621
Average KL loss: 0.152892
Average total loss: 2.455514
tensor(-6.6137, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3450e-08, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302584
Average KL loss: 0.148088
Average total loss: 2.450672
tensor(-6.6455, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2988e-08, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302551
Average KL loss: 0.143473
Average total loss: 2.446024
tensor(-6.6771, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2608e-08, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302593
Average KL loss: 0.139035
Average total loss: 2.441628
tensor(-6.7085, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2235e-08, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302608
Average KL loss: 0.134765
Average total loss: 2.437373
tensor(-6.7396, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1848e-08, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302605
Average KL loss: 0.130656
Average total loss: 2.433261
tensor(-6.7705, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1455e-08, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302604
Average KL loss: 0.126701
Average total loss: 2.429304
tensor(-6.8012, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1131e-08, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302603
Average KL loss: 0.122892
Average total loss: 2.425495
tensor(-6.8317, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0806e-08, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302639
Average KL loss: 0.119222
Average total loss: 2.421861
tensor(-6.8619, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0482e-08, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302590
Average KL loss: 0.115685
Average total loss: 2.418275
tensor(-6.8920, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.0155e-08, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302624
Average KL loss: 0.112276
Average total loss: 2.414900
tensor(-6.9219, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.8887e-09, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302606
Average KL loss: 0.108988
Average total loss: 2.411594
tensor(-6.9515, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.6017e-09, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302595
Average KL loss: 0.105816
Average total loss: 2.408411
tensor(-6.9810, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.3043e-09, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302564
Average KL loss: 0.102755
Average total loss: 2.405318
tensor(-7.0104, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(9.0306e-09, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302594
Average KL loss: 0.099800
Average total loss: 2.402395
tensor(-7.0395, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.7854e-09, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302613
Average KL loss: 0.096947
Average total loss: 2.399560
tensor(-7.0684, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.5362e-09, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302596
Average KL loss: 0.094191
Average total loss: 2.396787
tensor(-7.0972, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.3039e-09, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302618
Average KL loss: 0.091529
Average total loss: 2.394146
tensor(-7.1259, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(8.1057e-09, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302581
Average KL loss: 0.088956
Average total loss: 2.391537
tensor(-7.1543, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.8241e-09, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302595
Average KL loss: 0.086469
Average total loss: 2.389064
tensor(-7.1827, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.6256e-09, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302596
Average KL loss: 0.084064
Average total loss: 2.386661
tensor(-7.2108, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.3947e-09, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302598
Average KL loss: 0.081739
Average total loss: 2.384336
tensor(-7.2389, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(7.2141e-09, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302613
Average KL loss: 0.079489
Average total loss: 2.382102
tensor(-7.2667, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.9992e-09, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302588
Average KL loss: 0.077312
Average total loss: 2.379900
tensor(-7.2945, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.8016e-09, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302569
Average KL loss: 0.075206
Average total loss: 2.377775
tensor(-7.3220, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.6315e-09, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302583
Average KL loss: 0.073167
Average total loss: 2.375750
tensor(-7.3495, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.4330e-09, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302615
Average KL loss: 0.071193
Average total loss: 2.373808
tensor(-7.3768, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.2586e-09, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302630
Average KL loss: 0.069281
Average total loss: 2.371910
tensor(-7.4040, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(6.1140e-09, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302622
Average KL loss: 0.067428
Average total loss: 2.370051
tensor(-7.4311, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.9437e-09, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302617
Average KL loss: 0.065634
Average total loss: 2.368251
tensor(-7.4580, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.7923e-09, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302611
Average KL loss: 0.063895
Average total loss: 2.366506
tensor(-7.4848, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.6385e-09, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302590
Average KL loss: 0.062210
Average total loss: 2.364799
tensor(-7.5115, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.4809e-09, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302583
Average KL loss: 0.060576
Average total loss: 2.363159
tensor(-7.5381, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.3405e-09, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302599
Average KL loss: 0.058993
Average total loss: 2.361592
tensor(-7.5645, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.2020e-09, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302560
Average KL loss: 0.057458
Average total loss: 2.360018
tensor(-7.5909, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(5.0650e-09, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302576
Average KL loss: 0.055970
Average total loss: 2.358547
tensor(-7.6171, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.9455e-09, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302602
Average KL loss: 0.054526
Average total loss: 2.357127
tensor(-7.6432, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.8123e-09, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.053124
Average total loss: 2.355709
tensor(-7.6692, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.6906e-09, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302561
Average KL loss: 0.051765
Average total loss: 2.354325
tensor(-7.6951, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.5633e-09, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302580
Average KL loss: 0.050445
Average total loss: 2.353026
tensor(-7.7209, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.4857e-09, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302604
Average KL loss: 0.049165
Average total loss: 2.351769
tensor(-7.7466, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.3331e-09, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302589
Average KL loss: 0.047922
Average total loss: 2.350510
tensor(-7.7722, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.2348e-09, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302573
Average KL loss: 0.046715
Average total loss: 2.349288
tensor(-7.7977, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.1195e-09, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302595
Average KL loss: 0.045543
Average total loss: 2.348139
tensor(-7.8230, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(4.0182e-09, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302574
Average KL loss: 0.044406
Average total loss: 2.346980
tensor(-7.8483, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.9239e-09, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302609
Average KL loss: 0.043300
Average total loss: 2.345909
tensor(-7.8735, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.8378e-09, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302574
Average KL loss: 0.042227
Average total loss: 2.344801
tensor(-7.8986, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.7157e-09, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302595
Average KL loss: 0.041184
Average total loss: 2.343779
tensor(-7.9236, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.6356e-09, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302577
Average KL loss: 0.040171
Average total loss: 2.342748
tensor(-7.9484, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.5467e-09, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302600
Average KL loss: 0.039186
Average total loss: 2.341786
tensor(-7.9732, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.4322e-09, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.038229
Average total loss: 2.340814
tensor(-7.9979, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.3758e-09, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.037299
Average total loss: 2.339883
tensor(-8.0225, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.2967e-09, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302586
Average KL loss: 0.036394
Average total loss: 2.338980
tensor(-8.0470, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.2144e-09, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302578
Average KL loss: 0.035515
Average total loss: 2.338093
tensor(-8.0715, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.1346e-09, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302598
Average KL loss: 0.034661
Average total loss: 2.337259
tensor(-8.0958, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(3.0511e-09, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302559
Average KL loss: 0.033830
Average total loss: 2.336389
tensor(-8.1200, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.9866e-09, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302595
Average KL loss: 0.033022
Average total loss: 2.335617
tensor(-8.1442, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.9103e-09, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302592
Average KL loss: 0.032237
Average total loss: 2.334829
tensor(-8.1682, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.8476e-09, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302577
Average KL loss: 0.031472
Average total loss: 2.334050
tensor(-8.1922, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.7667e-09, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302582
Average KL loss: 0.030729
Average total loss: 2.333311
tensor(-8.2160, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.7147e-09, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302596
Average KL loss: 0.030006
Average total loss: 2.332602
tensor(-8.2398, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.6663e-09, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302581
Average KL loss: 0.029303
Average total loss: 2.331883
tensor(-8.2635, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.5889e-09, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302578
Average KL loss: 0.028618
Average total loss: 2.331196
tensor(-8.2871, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.5303e-09, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302596
Average KL loss: 0.027952
Average total loss: 2.330548
tensor(-8.3106, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.4698e-09, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302570
Average KL loss: 0.027304
Average total loss: 2.329874
tensor(-8.3341, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.4135e-09, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.026673
Average total loss: 2.329258
tensor(-8.3574, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.3573e-09, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302578
Average KL loss: 0.026060
Average total loss: 2.328638
tensor(-8.3806, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.3075e-09, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302581
Average KL loss: 0.025463
Average total loss: 2.328044
tensor(-8.4038, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.2536e-09, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302589
Average KL loss: 0.024881
Average total loss: 2.327470
tensor(-8.4269, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.2050e-09, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302584
Average KL loss: 0.024315
Average total loss: 2.326899
tensor(-8.4499, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.1520e-09, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302587
Average KL loss: 0.023764
Average total loss: 2.326351
tensor(-8.4728, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.1074e-09, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.023227
Average total loss: 2.325813
tensor(-8.4956, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.0648e-09, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302601
Average KL loss: 0.022705
Average total loss: 2.325306
tensor(-8.5183, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(2.0010e-09, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302589
Average KL loss: 0.022196
Average total loss: 2.324785
tensor(-8.5409, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.9636e-09, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302596
Average KL loss: 0.021700
Average total loss: 2.324297
tensor(-8.5635, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.9177e-09, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302588
Average KL loss: 0.021218
Average total loss: 2.323805
tensor(-8.5859, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8731e-09, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302576
Average KL loss: 0.020747
Average total loss: 2.323324
tensor(-8.6083, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.8417e-09, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302594
Average KL loss: 0.020289
Average total loss: 2.322883
tensor(-8.6306, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7938e-09, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302584
Average KL loss: 0.019843
Average total loss: 2.322427
tensor(-8.6528, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7546e-09, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.019409
Average total loss: 2.321993
tensor(-8.6749, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.7273e-09, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302583
Average KL loss: 0.018985
Average total loss: 2.321568
tensor(-8.6969, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6768e-09, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302588
Average KL loss: 0.018573
Average total loss: 2.321161
tensor(-8.7188, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6425e-09, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302588
Average KL loss: 0.018171
Average total loss: 2.320759
tensor(-8.7407, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.6063e-09, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302587
Average KL loss: 0.017779
Average total loss: 2.320366
tensor(-8.7624, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5717e-09, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302582
Average KL loss: 0.017398
Average total loss: 2.319979
tensor(-8.7841, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5380e-09, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302588
Average KL loss: 0.017026
Average total loss: 2.319613
tensor(-8.8056, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.5074e-09, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302586
Average KL loss: 0.016663
Average total loss: 2.319249
tensor(-8.8271, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4712e-09, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302581
Average KL loss: 0.016310
Average total loss: 2.318891
tensor(-8.8485, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4463e-09, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.015966
Average total loss: 2.318551
tensor(-8.8698, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.4124e-09, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302579
Average KL loss: 0.015630
Average total loss: 2.318209
tensor(-8.8910, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3877e-09, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302580
Average KL loss: 0.015303
Average total loss: 2.317883
tensor(-8.9121, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3556e-09, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302590
Average KL loss: 0.014984
Average total loss: 2.317574
tensor(-8.9331, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.3258e-09, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302588
Average KL loss: 0.014673
Average total loss: 2.317261
tensor(-8.9540, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2987e-09, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302583
Average KL loss: 0.014370
Average total loss: 2.316953
tensor(-8.9748, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2714e-09, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302584
Average KL loss: 0.014074
Average total loss: 2.316658
tensor(-8.9956, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2442e-09, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302579
Average KL loss: 0.013786
Average total loss: 2.316366
tensor(-9.0162, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.2220e-09, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302598
Average KL loss: 0.013505
Average total loss: 2.316103
tensor(-9.0367, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1933e-09, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 0.013231
Average total loss: 2.315818
tensor(-9.0572, device='cuda:0') tensor(0.0020, device='cuda:0') tensor(1.1701e-09, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302598
Average KL loss: 0.012964
Average total loss: 2.315562
tensor(-9.0775, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1399e-09, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302590
Average KL loss: 0.012703
Average total loss: 2.315294
tensor(-9.0978, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1217e-09, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302587
Average KL loss: 0.012449
Average total loss: 2.315036
tensor(-9.1179, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.1020e-09, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302586
Average KL loss: 0.012201
Average total loss: 2.314787
tensor(-9.1380, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0738e-09, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302575
Average KL loss: 0.011959
Average total loss: 2.314534
tensor(-9.1580, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0588e-09, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302591
Average KL loss: 0.011724
Average total loss: 2.314315
tensor(-9.1778, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0379e-09, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302576
Average KL loss: 0.011493
Average total loss: 2.314070
tensor(-9.1976, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(1.0178e-09, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302577
Average KL loss: 0.011269
Average total loss: 2.313846
tensor(-9.2173, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.9148e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.011050
Average total loss: 2.313635
tensor(-9.2368, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.7775e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.010836
Average total loss: 2.313421
tensor(-9.2563, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.5930e-10, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302584
Average KL loss: 0.010628
Average total loss: 2.313211
tensor(-9.2757, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.4086e-10, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302587
Average KL loss: 0.010424
Average total loss: 2.313012
tensor(-9.2950, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.2276e-10, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302573
Average KL loss: 0.010226
Average total loss: 2.312799
tensor(-9.3141, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(9.0605e-10, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302589
Average KL loss: 0.010032
Average total loss: 2.312620
tensor(-9.3332, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.9832e-10, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302591
Average KL loss: 0.009843
Average total loss: 2.312433
tensor(-9.3522, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.7271e-10, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302594
Average KL loss: 0.009658
Average total loss: 2.312252
tensor(-9.3710, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.5555e-10, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302591
Average KL loss: 0.009478
Average total loss: 2.312069
tensor(-9.3898, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.4659e-10, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302589
Average KL loss: 0.009302
Average total loss: 2.311890
tensor(-9.4085, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.2204e-10, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302590
Average KL loss: 0.009130
Average total loss: 2.311720
tensor(-9.4270, device='cuda:0') tensor(0.0019, device='cuda:0') tensor(8.1093e-10, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302581
Average KL loss: 0.008962
Average total loss: 2.311543
tensor(-9.4455, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.9411e-10, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302592
Average KL loss: 0.008799
Average total loss: 2.311391
tensor(-9.4639, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.7965e-10, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302581
Average KL loss: 0.008639
Average total loss: 2.311220
tensor(-9.4821, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.6558e-10, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302590
Average KL loss: 0.008483
Average total loss: 2.311073
tensor(-9.5003, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.5167e-10, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302589
Average KL loss: 0.008331
Average total loss: 2.310920
tensor(-9.5183, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.3831e-10, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302584
Average KL loss: 0.008182
Average total loss: 2.310766
tensor(-9.5362, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.2516e-10, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302578
Average KL loss: 0.008037
Average total loss: 2.310615
tensor(-9.5541, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.1233e-10, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302587
Average KL loss: 0.007895
Average total loss: 2.310482
tensor(-9.5718, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(7.0022e-10, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302587
Average KL loss: 0.007756
Average total loss: 2.310344
tensor(-9.5894, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.8756e-10, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302584
Average KL loss: 0.007621
Average total loss: 2.310205
tensor(-9.6070, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.7572e-10, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302593
Average KL loss: 0.007489
Average total loss: 2.310082
tensor(-9.6244, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.5031e-10, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302584
Average KL loss: 0.007360
Average total loss: 2.309944
tensor(-9.6417, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.5251e-10, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302580
Average KL loss: 0.007234
Average total loss: 2.309813
 Percentile value: -9.659708023071289
Non-zero model percentage: 30.000001907348633%, Non-zero mask percentage: 30.000001907348633%

--- Pruning Level [1/7]: ---
conv1.weight         | nonzeros =     550 /    1728             ( 31.83%) | total_pruned =    1178 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    5533 /   36864             ( 15.01%) | total_pruned =   31331 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   12337 /   36864             ( 33.47%) | total_pruned =   24527 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   13018 /   36864             ( 35.31%) | total_pruned =   23846 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      52 /      64             ( 81.25%) | total_pruned =      12 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      36 /      64             ( 56.25%) | total_pruned =      28 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   14543 /   36864             ( 39.45%) | total_pruned =   22321 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      59 /      64             ( 92.19%) | total_pruned =       5 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   33661 /   73728             ( 45.66%) | total_pruned =   40067 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     105 /     128             ( 82.03%) | total_pruned =      23 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   67494 /  147456             ( 45.77%) | total_pruned =   79962 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3976 /    8192             ( 48.54%) | total_pruned =    4216 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   51092 /  147456             ( 34.65%) | total_pruned =   96364 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      94 /     128             ( 73.44%) | total_pruned =      34 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   45151 /  147456             ( 30.62%) | total_pruned =  102305 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  138902 /  294912             ( 47.10%) | total_pruned =  156010 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     253 /     256             ( 98.83%) | total_pruned =       3 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  242314 /  589824             ( 41.08%) | total_pruned =  347510 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     238 /     256             ( 92.97%) | total_pruned =      18 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   13308 /   32768             ( 40.61%) | total_pruned =   19460 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     181 /     256             ( 70.70%) | total_pruned =      75 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  148904 /  589824             ( 25.25%) | total_pruned =  440920 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     171 /     256             ( 66.80%) | total_pruned =      85 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  114862 /  589824             ( 19.47%) | total_pruned =  474962 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     164 /     256             ( 64.06%) | total_pruned =      92 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     251 /     256             ( 98.05%) | total_pruned =       5 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  398290 / 1179648             ( 33.76%) | total_pruned =  781358 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  674815 / 2359296             ( 28.60%) | total_pruned = 1684481 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     391 /     512             ( 76.37%) | total_pruned =     121 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     474 /     512             ( 92.58%) | total_pruned =      38 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   39381 /  131072             ( 30.05%) | total_pruned =   91691 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     277 /     512             ( 54.10%) | total_pruned =     235 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     473 /     512             ( 92.38%) | total_pruned =      39 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  653224 / 2359296             ( 27.69%) | total_pruned = 1706072 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  669221 / 2359296             ( 28.37%) | total_pruned = 1690075 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5099 /    5120             ( 99.59%) | total_pruned =      21 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3353629, pruned : 7825133, total: 11178762, Compression rate :       3.33x  ( 70.00% pruned)
Train Epoch: 53/200 Loss: 0.014766 Accuracy: 88.38 100.00 % Best test Accuracy: 88.49%
tensor(-9.6589, device='cuda:0') tensor(0.0018, device='cuda:0') tensor(6.4137e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302582
Average KL loss: 0.007064
Average total loss: 2.309646
tensor(-9.6881, device='cuda:0') tensor(0.0015, device='cuda:0') tensor(6.2219e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302587
Average KL loss: 0.006857
Average total loss: 2.309444
tensor(-9.7164, device='cuda:0') tensor(0.0013, device='cuda:0') tensor(6.0435e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302587
Average KL loss: 0.006664
Average total loss: 2.309251
tensor(-9.7440, device='cuda:0') tensor(0.0012, device='cuda:0') tensor(5.8762e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302592
Average KL loss: 0.006482
Average total loss: 2.309074
tensor(-9.7709, device='cuda:0') tensor(0.0011, device='cuda:0') tensor(5.7772e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302586
Average KL loss: 0.006311
Average total loss: 2.308896
tensor(-9.7971, device='cuda:0') tensor(0.0010, device='cuda:0') tensor(5.5693e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.006148
Average total loss: 2.308733
tensor(-9.8226, device='cuda:0') tensor(0.0009, device='cuda:0') tensor(5.4278e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.005994
Average total loss: 2.308579
tensor(-9.8476, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(5.2936e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302590
Average KL loss: 0.005847
Average total loss: 2.308438
tensor(-9.8719, device='cuda:0') tensor(0.0008, device='cuda:0') tensor(5.1658e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302586
Average KL loss: 0.005708
Average total loss: 2.308294
tensor(-9.8956, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(4.9583e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302588
Average KL loss: 0.005575
Average total loss: 2.308163
tensor(-9.9189, device='cuda:0') tensor(0.0007, device='cuda:0') tensor(4.9471e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302588
Average KL loss: 0.005448
Average total loss: 2.308035
tensor(-9.9416, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.8169e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302587
Average KL loss: 0.005326
Average total loss: 2.307913
tensor(-9.9638, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.7108e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.005210
Average total loss: 2.307796
tensor(-9.9855, device='cuda:0') tensor(0.0006, device='cuda:0') tensor(4.6092e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.005099
Average total loss: 2.307684
tensor(-10.0068, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.5119e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302580
Average KL loss: 0.004993
Average total loss: 2.307573
tensor(-10.0277, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.4187e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302586
Average KL loss: 0.004890
Average total loss: 2.307477
tensor(-10.0482, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.3291e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302586
Average KL loss: 0.004792
Average total loss: 2.307378
tensor(-10.0682, device='cuda:0') tensor(0.0005, device='cuda:0') tensor(4.2198e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.004698
Average total loss: 2.307283
tensor(-10.0879, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.1603e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302586
Average KL loss: 0.004607
Average total loss: 2.307193
tensor(-10.1072, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.0831e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302581
Average KL loss: 0.004520
Average total loss: 2.307101
tensor(-10.1261, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(4.0040e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302584
Average KL loss: 0.004436
Average total loss: 2.307019
tensor(-10.1447, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.9302e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302586
Average KL loss: 0.004354
Average total loss: 2.306940
tensor(-10.1630, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.8590e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302584
Average KL loss: 0.004276
Average total loss: 2.306860
tensor(-10.1809, device='cuda:0') tensor(0.0004, device='cuda:0') tensor(3.7903e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302587
Average KL loss: 0.004201
Average total loss: 2.306788
tensor(-10.1986, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.7240e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302584
Average KL loss: 0.004128
Average total loss: 2.306711
tensor(-10.2159, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.6599e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302588
Average KL loss: 0.004057
Average total loss: 2.306645
tensor(-10.2330, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.5980e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302587
Average KL loss: 0.003989
Average total loss: 2.306576
tensor(-10.2498, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.5383e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302583
Average KL loss: 0.003923
Average total loss: 2.306507
tensor(-10.2663, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.4801e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302586
Average KL loss: 0.003860
Average total loss: 2.306445
tensor(-10.2825, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.4239e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302586
Average KL loss: 0.003798
Average total loss: 2.306384
tensor(-10.2985, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.3697e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302583
Average KL loss: 0.003738
Average total loss: 2.306321
tensor(-10.3143, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.3169e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302587
Average KL loss: 0.003680
Average total loss: 2.306267
tensor(-10.3298, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.2658e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302584
Average KL loss: 0.003624
Average total loss: 2.306208
tensor(-10.3451, device='cuda:0') tensor(0.0003, device='cuda:0') tensor(3.2162e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302583
Average KL loss: 0.003569
Average total loss: 2.306152
tensor(-10.3601, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.1682e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302582
Average KL loss: 0.003516
Average total loss: 2.306098
tensor(-10.3750, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.1211e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.003465
Average total loss: 2.306050
tensor(-10.3896, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.0761e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302584
Average KL loss: 0.003415
Average total loss: 2.305998
tensor(-10.4040, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(3.0320e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302587
Average KL loss: 0.003366
Average total loss: 2.305953
tensor(-10.4183, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9890e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302588
Average KL loss: 0.003319
Average total loss: 2.305907
tensor(-10.4323, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9474e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302589
Average KL loss: 0.003273
Average total loss: 2.305862
tensor(-10.4461, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.9070e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302591
Average KL loss: 0.003228
Average total loss: 2.305819
tensor(-10.4598, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.8674e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302583
Average KL loss: 0.003184
Average total loss: 2.305768
tensor(-10.4733, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.8290e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.003142
Average total loss: 2.305727
tensor(-10.4866, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7916e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302586
Average KL loss: 0.003101
Average total loss: 2.305687
tensor(-10.4997, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7551e-10, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.003060
Average total loss: 2.305645
tensor(-10.5127, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.7196e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302583
Average KL loss: 0.003021
Average total loss: 2.305605
tensor(-10.5255, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6850e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302584
Average KL loss: 0.002983
Average total loss: 2.305567
tensor(-10.5382, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6535e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.002946
Average total loss: 2.305530
tensor(-10.5507, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.6183e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302584
Average KL loss: 0.002909
Average total loss: 2.305493
tensor(-10.5630, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5861e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302586
Average KL loss: 0.002874
Average total loss: 2.305459
tensor(-10.5752, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5548e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.002839
Average total loss: 2.305424
tensor(-10.5873, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.5241e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302586
Average KL loss: 0.002805
Average total loss: 2.305391
tensor(-10.5992, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4942e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302584
Average KL loss: 0.002772
Average total loss: 2.305357
tensor(-10.6110, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4650e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302580
Average KL loss: 0.002740
Average total loss: 2.305320
tensor(-10.6227, device='cuda:0') tensor(0.0002, device='cuda:0') tensor(2.4364e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302583
Average KL loss: 0.002708
Average total loss: 2.305292
tensor(-10.6342, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.4085e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302582
Average KL loss: 0.002677
Average total loss: 2.305259
tensor(-10.6456, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3812e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302586
Average KL loss: 0.002647
Average total loss: 2.305233
tensor(-10.6568, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3545e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.002618
Average total loss: 2.305203
tensor(-10.6680, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3284e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.002589
Average total loss: 2.305174
tensor(-10.6790, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3029e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302586
Average KL loss: 0.002561
Average total loss: 2.305146
tensor(-10.6899, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2779e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302586
Average KL loss: 0.002533
Average total loss: 2.305119
tensor(-10.7007, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2534e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.002506
Average total loss: 2.305090
tensor(-10.7114, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2295e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302582
Average KL loss: 0.002479
Average total loss: 2.305061
tensor(-10.7220, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.2060e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302586
Average KL loss: 0.002453
Average total loss: 2.305040
tensor(-10.7325, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1850e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302584
Average KL loss: 0.002428
Average total loss: 2.305012
tensor(-10.7428, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.3060e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302586
Average KL loss: 0.002403
Average total loss: 2.304989
tensor(-10.7531, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1361e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.002379
Average total loss: 2.304964
tensor(-10.7632, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.1169e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.002355
Average total loss: 2.304940
tensor(-10.7733, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0957e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302588
Average KL loss: 0.002331
Average total loss: 2.304919
tensor(-10.7833, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0749e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.002308
Average total loss: 2.304893
tensor(-10.7931, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0546e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302588
Average KL loss: 0.002286
Average total loss: 2.304873
tensor(-10.8029, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0345e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302586
Average KL loss: 0.002264
Average total loss: 2.304850
tensor(-10.8126, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(2.0149e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302584
Average KL loss: 0.002242
Average total loss: 2.304826
tensor(-10.8222, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9957e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.002221
Average total loss: 2.304806
tensor(-10.8317, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9768e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302584
Average KL loss: 0.002200
Average total loss: 2.304784
tensor(-10.8411, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9583e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302587
Average KL loss: 0.002179
Average total loss: 2.304766
tensor(-10.8504, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9401e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302584
Average KL loss: 0.002159
Average total loss: 2.304743
tensor(-10.8596, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9222e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302584
Average KL loss: 0.002139
Average total loss: 2.304723
tensor(-10.8688, device='cuda:0') tensor(0.0001, device='cuda:0') tensor(1.9047e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302584
Average KL loss: 0.002120
Average total loss: 2.304704
tensor(-10.8779, device='cuda:0') tensor(9.8561e-05, device='cuda:0') tensor(1.8811e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302584
Average KL loss: 0.002101
Average total loss: 2.304685
tensor(-10.8869, device='cuda:0') tensor(9.7363e-05, device='cuda:0') tensor(1.8706e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302586
Average KL loss: 0.002082
Average total loss: 2.304668
tensor(-10.8958, device='cuda:0') tensor(9.5956e-05, device='cuda:0') tensor(1.8545e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.002072
Average total loss: 2.304657
tensor(-10.8967, device='cuda:0') tensor(9.5824e-05, device='cuda:0') tensor(1.8523e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302584
Average KL loss: 0.002070
Average total loss: 2.304654
tensor(-10.8976, device='cuda:0') tensor(9.5699e-05, device='cuda:0') tensor(1.8507e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302584
Average KL loss: 0.002068
Average total loss: 2.304652
tensor(-10.8985, device='cuda:0') tensor(9.5586e-05, device='cuda:0') tensor(1.8490e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302588
Average KL loss: 0.002066
Average total loss: 2.304654
tensor(-10.8994, device='cuda:0') tensor(9.5415e-05, device='cuda:0') tensor(1.8474e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302583
Average KL loss: 0.002064
Average total loss: 2.304647
tensor(-10.9002, device='cuda:0') tensor(9.5269e-05, device='cuda:0') tensor(1.8458e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302584
Average KL loss: 0.002062
Average total loss: 2.304647
tensor(-10.9011, device='cuda:0') tensor(9.5144e-05, device='cuda:0') tensor(1.8441e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302586
Average KL loss: 0.002061
Average total loss: 2.304647
tensor(-10.9020, device='cuda:0') tensor(9.5015e-05, device='cuda:0') tensor(1.8425e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.002059
Average total loss: 2.304643
tensor(-10.9029, device='cuda:0') tensor(9.4888e-05, device='cuda:0') tensor(1.8408e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.002057
Average total loss: 2.304642
tensor(-10.9038, device='cuda:0') tensor(9.4753e-05, device='cuda:0') tensor(1.8392e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302584
Average KL loss: 0.002055
Average total loss: 2.304639
tensor(-10.9047, device='cuda:0') tensor(9.4627e-05, device='cuda:0') tensor(1.8376e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.002053
Average total loss: 2.304638
tensor(-10.9056, device='cuda:0') tensor(9.4495e-05, device='cuda:0') tensor(1.8360e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 0.002052
Average total loss: 2.304637
tensor(-10.9065, device='cuda:0') tensor(9.4339e-05, device='cuda:0') tensor(1.8343e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302586
Average KL loss: 0.002050
Average total loss: 2.304637
tensor(-10.9066, device='cuda:0') tensor(9.4325e-05, device='cuda:0') tensor(1.8341e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302586
Average KL loss: 0.002050
Average total loss: 2.304636
tensor(-10.9067, device='cuda:0') tensor(9.4313e-05, device='cuda:0') tensor(1.8340e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.002050
Average total loss: 2.304635
tensor(-10.9067, device='cuda:0') tensor(9.4302e-05, device='cuda:0') tensor(1.8338e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302584
Average KL loss: 0.002050
Average total loss: 2.304634
tensor(-10.9068, device='cuda:0') tensor(9.4290e-05, device='cuda:0') tensor(1.8336e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.002050
Average total loss: 2.304635
tensor(-10.9069, device='cuda:0') tensor(9.4279e-05, device='cuda:0') tensor(1.8335e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302589
Average KL loss: 0.002050
Average total loss: 2.304638
tensor(-10.9070, device='cuda:0') tensor(9.4263e-05, device='cuda:0') tensor(1.8333e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302578
Average KL loss: 0.002049
Average total loss: 2.304627
tensor(-10.9071, device='cuda:0') tensor(9.4252e-05, device='cuda:0') tensor(1.8331e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302587
Average KL loss: 0.002049
Average total loss: 2.304636
tensor(-10.9072, device='cuda:0') tensor(9.4238e-05, device='cuda:0') tensor(1.8329e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302583
Average KL loss: 0.002049
Average total loss: 2.304632
tensor(-10.9073, device='cuda:0') tensor(9.4226e-05, device='cuda:0') tensor(1.8328e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.002049
Average total loss: 2.304634
tensor(-10.9074, device='cuda:0') tensor(9.4214e-05, device='cuda:0') tensor(1.8326e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302583
Average KL loss: 0.002049
Average total loss: 2.304631
tensor(-10.9075, device='cuda:0') tensor(9.4202e-05, device='cuda:0') tensor(1.8321e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304634
tensor(-10.9075, device='cuda:0') tensor(9.4202e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302584
Average KL loss: 0.002048
Average total loss: 2.304632
tensor(-10.9075, device='cuda:0') tensor(9.4201e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304633
tensor(-10.9075, device='cuda:0') tensor(9.4201e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302586
Average KL loss: 0.002048
Average total loss: 2.304634
tensor(-10.9075, device='cuda:0') tensor(9.4200e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304633
tensor(-10.9075, device='cuda:0') tensor(9.4200e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302583
Average KL loss: 0.002048
Average total loss: 2.304631
tensor(-10.9075, device='cuda:0') tensor(9.4199e-05, device='cuda:0') tensor(1.7771e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304633
tensor(-10.9075, device='cuda:0') tensor(9.4199e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.002048
Average total loss: 2.304633
tensor(-10.9075, device='cuda:0') tensor(9.4198e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302581
Average KL loss: 0.002048
Average total loss: 2.304630
tensor(-10.9075, device='cuda:0') tensor(9.4198e-05, device='cuda:0') tensor(1.8325e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302587
Average KL loss: 0.002048
Average total loss: 2.304635
tensor(-10.9075, device='cuda:0') tensor(9.4197e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
 Percentile value: -10.907543182373047
Non-zero model percentage: 9.000003814697266%, Non-zero mask percentage: 9.000003814697266%

--- Pruning Level [2/7]: ---
conv1.weight         | nonzeros =     537 /    1728             ( 31.08%) | total_pruned =    1191 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    3856 /   36864             ( 10.46%) | total_pruned =   33008 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      47 /      64             ( 73.44%) | total_pruned =      17 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    7275 /   36864             ( 19.73%) | total_pruned =   29589 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    7330 /   36864             ( 19.88%) | total_pruned =   29534 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    7776 /   36864             ( 21.09%) | total_pruned =   29088 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      60 /      64             ( 93.75%) | total_pruned =       4 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   20949 /   73728             ( 28.41%) | total_pruned =   52779 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =     103 /     128             ( 80.47%) | total_pruned =      25 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   37799 /  147456             ( 25.63%) | total_pruned =  109657 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    2860 /    8192             ( 34.91%) | total_pruned =    5332 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     102 /     128             ( 79.69%) | total_pruned =      26 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     116 /     128             ( 90.62%) | total_pruned =      12 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   12837 /  147456             (  8.71%) | total_pruned =  134619 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      90 /     128             ( 70.31%) | total_pruned =      38 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   10127 /  147456             (  6.87%) | total_pruned =  137329 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   82439 /  294912             ( 27.95%) | total_pruned =  212473 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  100227 /  589824             ( 16.99%) | total_pruned =  489597 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     249 /     256             ( 97.27%) | total_pruned =       7 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    6146 /   32768             ( 18.76%) | total_pruned =   26622 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     180 /     256             ( 70.31%) | total_pruned =      76 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     242 /     256             ( 94.53%) | total_pruned =      14 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   21293 /  589824             (  3.61%) | total_pruned =  568531 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   14635 /  589824             (  2.48%) | total_pruned =  575189 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     153 /     256             ( 59.77%) | total_pruned =     103 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     250 /     256             ( 97.66%) | total_pruned =       6 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  118264 / 1179648             ( 10.03%) | total_pruned = 1061384 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     425 /     512             ( 83.01%) | total_pruned =      87 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     469 /     512             ( 91.60%) | total_pruned =      43 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  164946 / 2359296             (  6.99%) | total_pruned = 2194350 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     389 /     512             ( 75.98%) | total_pruned =     123 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   11996 /  131072             (  9.15%) | total_pruned =  119076 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     262 /     512             ( 51.17%) | total_pruned =     250 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     472 /     512             ( 92.19%) | total_pruned =      40 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  186560 / 2359296             (  7.91%) | total_pruned = 2172736 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     376 /     512             ( 73.44%) | total_pruned =     136 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  175255 / 2359296             (  7.43%) | total_pruned = 2184041 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     480 /     512             ( 93.75%) | total_pruned =      32 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5095 /    5120             ( 99.51%) | total_pruned =      25 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1006089, pruned : 10172673, total: 11178762, Compression rate :      11.11x  ( 91.00% pruned)
Train Epoch: 71/200 Loss: 0.025128 Accuracy: 87.83 100.00 % Best test Accuracy: 87.95%
tensor(-10.9075, device='cuda:0') tensor(9.4196e-05, device='cuda:0') tensor(1.8324e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302586
Average KL loss: 0.002040
Average total loss: 2.304625
tensor(-10.9163, device='cuda:0') tensor(8.9193e-05, device='cuda:0') tensor(1.8163e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302587
Average KL loss: 0.002022
Average total loss: 2.304609
tensor(-10.9250, device='cuda:0') tensor(8.5021e-05, device='cuda:0') tensor(1.8005e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302586
Average KL loss: 0.002004
Average total loss: 2.304590
tensor(-10.9336, device='cuda:0') tensor(8.1649e-05, device='cuda:0') tensor(1.7849e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001987
Average total loss: 2.304572
tensor(-10.9422, device='cuda:0') tensor(7.8871e-05, device='cuda:0') tensor(1.7697e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001970
Average total loss: 2.304555
tensor(-10.9507, device='cuda:0') tensor(7.6666e-05, device='cuda:0') tensor(1.7547e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302586
Average KL loss: 0.001953
Average total loss: 2.304539
tensor(-10.9591, device='cuda:0') tensor(7.4520e-05, device='cuda:0') tensor(1.7400e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302586
Average KL loss: 0.001937
Average total loss: 2.304523
tensor(-10.9675, device='cuda:0') tensor(7.2568e-05, device='cuda:0') tensor(1.7255e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001921
Average total loss: 2.304506
tensor(-10.9758, device='cuda:0') tensor(7.0810e-05, device='cuda:0') tensor(1.7112e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302586
Average KL loss: 0.001905
Average total loss: 2.304492
tensor(-10.9840, device='cuda:0') tensor(6.8960e-05, device='cuda:0') tensor(1.6972e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001890
Average total loss: 2.304475
tensor(-10.9921, device='cuda:0') tensor(6.7357e-05, device='cuda:0') tensor(1.6834e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001874
Average total loss: 2.304459
tensor(-11.0002, device='cuda:0') tensor(6.5958e-05, device='cuda:0') tensor(1.6699e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001859
Average total loss: 2.304444
tensor(-11.0082, device='cuda:0') tensor(6.4523e-05, device='cuda:0') tensor(1.6565e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001851
Average total loss: 2.304436
tensor(-11.0090, device='cuda:0') tensor(6.4391e-05, device='cuda:0') tensor(1.6552e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001850
Average total loss: 2.304435
tensor(-11.0098, device='cuda:0') tensor(6.4260e-05, device='cuda:0') tensor(1.6539e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-11.0106, device='cuda:0') tensor(6.4120e-05, device='cuda:0') tensor(1.6526e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001847
Average total loss: 2.304432
tensor(-11.0114, device='cuda:0') tensor(6.3982e-05, device='cuda:0') tensor(1.6513e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001845
Average total loss: 2.304430
tensor(-11.0122, device='cuda:0') tensor(6.3838e-05, device='cuda:0') tensor(1.6500e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001844
Average total loss: 2.304429
tensor(-11.0130, device='cuda:0') tensor(6.3699e-05, device='cuda:0') tensor(1.6486e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001842
Average total loss: 2.304427
tensor(-11.0138, device='cuda:0') tensor(6.3560e-05, device='cuda:0') tensor(1.6473e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302581
Average KL loss: 0.001841
Average total loss: 2.304422
tensor(-11.0146, device='cuda:0') tensor(6.3432e-05, device='cuda:0') tensor(1.6460e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302586
Average KL loss: 0.001839
Average total loss: 2.304425
tensor(-11.0154, device='cuda:0') tensor(6.3279e-05, device='cuda:0') tensor(1.6447e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001838
Average total loss: 2.304423
tensor(-11.0162, device='cuda:0') tensor(6.3137e-05, device='cuda:0') tensor(1.6434e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302587
Average KL loss: 0.001836
Average total loss: 2.304423
tensor(-11.0170, device='cuda:0') tensor(6.2997e-05, device='cuda:0') tensor(1.6421e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001836
Average total loss: 2.304420
tensor(-11.0171, device='cuda:0') tensor(6.2988e-05, device='cuda:0') tensor(1.6420e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304420
tensor(-11.0172, device='cuda:0') tensor(6.2980e-05, device='cuda:0') tensor(1.6418e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304420
tensor(-11.0173, device='cuda:0') tensor(6.2973e-05, device='cuda:0') tensor(1.6416e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302584
Average KL loss: 0.001835
Average total loss: 2.304419
tensor(-11.0174, device='cuda:0') tensor(6.2965e-05, device='cuda:0') tensor(1.6415e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302586
Average KL loss: 0.001835
Average total loss: 2.304421
tensor(-11.0175, device='cuda:0') tensor(6.2957e-05, device='cuda:0') tensor(1.6413e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304420
tensor(-11.0175, device='cuda:0') tensor(6.2948e-05, device='cuda:0') tensor(1.6412e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001835
Average total loss: 2.304419
tensor(-11.0176, device='cuda:0') tensor(6.2941e-05, device='cuda:0') tensor(1.6410e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0177, device='cuda:0') tensor(6.2933e-05, device='cuda:0') tensor(1.6409e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0178, device='cuda:0') tensor(6.2925e-05, device='cuda:0') tensor(1.6407e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0179, device='cuda:0') tensor(6.2917e-05, device='cuda:0') tensor(1.6406e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2907e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2907e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2906e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2906e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2906e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302586
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2905e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2904e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302586
Average KL loss: 0.001834
Average total loss: 2.304420
tensor(-11.0180, device='cuda:0') tensor(6.2904e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2904e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304419
tensor(-11.0180, device='cuda:0') tensor(6.2903e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001834
Average total loss: 2.304418
tensor(-11.0180, device='cuda:0') tensor(6.2903e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
 Percentile value: -11.017661094665527
Non-zero model percentage: 2.7000038623809814%, Non-zero mask percentage: 2.7000038623809814%

--- Pruning Level [3/7]: ---
conv1.weight         | nonzeros =     522 /    1728             ( 30.21%) | total_pruned =    1206 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2297 /   36864             (  6.23%) | total_pruned =   34567 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      44 /      64             ( 68.75%) | total_pruned =      20 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3800 /   36864             ( 10.31%) | total_pruned =   33064 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      50 /      64             ( 78.12%) | total_pruned =      14 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3621 /   36864             (  9.82%) | total_pruned =   33243 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3690 /   36864             ( 10.01%) | total_pruned =   33174 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      57 /      64             ( 89.06%) | total_pruned =       7 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    9534 /   73728             ( 12.93%) | total_pruned =   64194 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     119 /     128             ( 92.97%) | total_pruned =       9 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      99 /     128             ( 77.34%) | total_pruned =      29 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   15033 /  147456             ( 10.19%) | total_pruned =  132423 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     118 /     128             ( 92.19%) | total_pruned =      10 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     113 /     128             ( 88.28%) | total_pruned =      15 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1851 /    8192             ( 22.60%) | total_pruned =    6341 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      97 /     128             ( 75.78%) | total_pruned =      31 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     114 /     128             ( 89.06%) | total_pruned =      14 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    2507 /  147456             (  1.70%) | total_pruned =  144949 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2088 /  147456             (  1.42%) | total_pruned =  145368 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   33005 /  294912             ( 11.19%) | total_pruned =  261907 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   35568 /  589824             (  6.03%) | total_pruned =  554256 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     233 /     256             ( 91.02%) | total_pruned =      23 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     248 /     256             ( 96.88%) | total_pruned =       8 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2635 /   32768             (  8.04%) | total_pruned =   30133 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     167 /     256             ( 65.23%) | total_pruned =      89 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     241 /     256             ( 94.14%) | total_pruned =      15 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    2824 /  589824             (  0.48%) | total_pruned =  587000 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     139 /     256             ( 54.30%) | total_pruned =     117 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     162 /     256             ( 63.28%) | total_pruned =      94 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    2039 /  589824             (  0.35%) | total_pruned =  587785 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     126 /     256             ( 49.22%) | total_pruned =     130 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     247 /     256             ( 96.48%) | total_pruned =       9 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   35085 / 1179648             (  2.97%) | total_pruned = 1144563 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     421 /     512             ( 82.23%) | total_pruned =      91 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     465 /     512             ( 90.82%) | total_pruned =      47 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   36591 / 2359296             (  1.55%) | total_pruned = 2322705 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     378 /     512             ( 73.83%) | total_pruned =     134 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     466 /     512             ( 91.02%) | total_pruned =      46 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3398 /  131072             (  2.59%) | total_pruned =  127674 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     230 /     512             ( 44.92%) | total_pruned =     282 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     467 /     512             ( 91.21%) | total_pruned =      45 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   49089 / 2359296             (  2.08%) | total_pruned = 2310207 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     365 /     512             ( 71.29%) | total_pruned =     147 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     386 /     512             ( 75.39%) | total_pruned =     126 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   43864 / 2359296             (  1.86%) | total_pruned = 2315432 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     478 /     512             ( 93.36%) | total_pruned =      34 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5090 /    5120             ( 99.41%) | total_pruned =      30 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 301827, pruned : 10876935, total: 11178762, Compression rate :      37.04x  ( 97.30% pruned)
Train Epoch: 89/200 Loss: 0.017920 Accuracy: 84.32 100.00 % Best test Accuracy: 85.30%
tensor(-11.0180, device='cuda:0') tensor(6.2902e-05, device='cuda:0') tensor(1.6404e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001827
Average total loss: 2.304412
tensor(-11.0259, device='cuda:0') tensor(6.0656e-05, device='cuda:0') tensor(1.6275e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001812
Average total loss: 2.304397
tensor(-11.0337, device='cuda:0') tensor(5.8780e-05, device='cuda:0') tensor(1.6149e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001798
Average total loss: 2.304383
tensor(-11.0415, device='cuda:0') tensor(5.7140e-05, device='cuda:0') tensor(1.6023e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302586
Average KL loss: 0.001784
Average total loss: 2.304371
tensor(-11.0492, device='cuda:0') tensor(5.5651e-05, device='cuda:0') tensor(1.5900e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001771
Average total loss: 2.304356
tensor(-11.0569, device='cuda:0') tensor(5.4427e-05, device='cuda:0') tensor(1.5779e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001757
Average total loss: 2.304342
tensor(-11.0644, device='cuda:0') tensor(5.3294e-05, device='cuda:0') tensor(1.5660e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302587
Average KL loss: 0.001744
Average total loss: 2.304331
tensor(-11.0720, device='cuda:0') tensor(5.2115e-05, device='cuda:0') tensor(1.5542e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001731
Average total loss: 2.304316
tensor(-11.0795, device='cuda:0') tensor(5.1065e-05, device='cuda:0') tensor(1.5426e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001718
Average total loss: 2.304303
tensor(-11.0869, device='cuda:0') tensor(5.0157e-05, device='cuda:0') tensor(1.5312e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001706
Average total loss: 2.304290
tensor(-11.0943, device='cuda:0') tensor(4.9256e-05, device='cuda:0') tensor(1.5200e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001693
Average total loss: 2.304278
tensor(-11.1016, device='cuda:0') tensor(4.8345e-05, device='cuda:0') tensor(1.5089e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001681
Average total loss: 2.304266
tensor(-11.1088, device='cuda:0') tensor(4.7456e-05, device='cuda:0') tensor(1.4980e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001674
Average total loss: 2.304259
tensor(-11.1095, device='cuda:0') tensor(4.7358e-05, device='cuda:0') tensor(1.4969e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001673
Average total loss: 2.304258
tensor(-11.1102, device='cuda:0') tensor(4.7257e-05, device='cuda:0') tensor(1.4953e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304256
tensor(-11.1109, device='cuda:0') tensor(4.7166e-05, device='cuda:0') tensor(1.4950e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001670
Average total loss: 2.304255
tensor(-11.1116, device='cuda:0') tensor(4.7079e-05, device='cuda:0') tensor(1.4938e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302581
Average KL loss: 0.001669
Average total loss: 2.304251
tensor(-11.1123, device='cuda:0') tensor(4.6991e-05, device='cuda:0') tensor(1.4927e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001668
Average total loss: 2.304253
tensor(-11.1130, device='cuda:0') tensor(4.6904e-05, device='cuda:0') tensor(1.4917e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001667
Average total loss: 2.304252
tensor(-11.1137, device='cuda:0') tensor(4.6815e-05, device='cuda:0') tensor(1.4906e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001666
Average total loss: 2.304251
tensor(-11.1144, device='cuda:0') tensor(4.6726e-05, device='cuda:0') tensor(1.4896e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001665
Average total loss: 2.304250
tensor(-11.1151, device='cuda:0') tensor(4.6638e-05, device='cuda:0') tensor(1.4885e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001663
Average total loss: 2.304248
tensor(-11.1158, device='cuda:0') tensor(4.6554e-05, device='cuda:0') tensor(1.4875e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001662
Average total loss: 2.304247
tensor(-11.1165, device='cuda:0') tensor(4.6468e-05, device='cuda:0') tensor(1.4864e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001662
Average total loss: 2.304247
tensor(-11.1166, device='cuda:0') tensor(4.6465e-05, device='cuda:0') tensor(1.4863e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1167, device='cuda:0') tensor(4.6462e-05, device='cuda:0') tensor(1.4862e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1168, device='cuda:0') tensor(4.6459e-05, device='cuda:0') tensor(1.4860e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1169, device='cuda:0') tensor(4.6456e-05, device='cuda:0') tensor(1.4859e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1170, device='cuda:0') tensor(4.6452e-05, device='cuda:0') tensor(1.4858e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1171, device='cuda:0') tensor(4.6449e-05, device='cuda:0') tensor(1.4856e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304246
tensor(-11.1172, device='cuda:0') tensor(4.6446e-05, device='cuda:0') tensor(1.4855e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001661
Average total loss: 2.304245
tensor(-11.1173, device='cuda:0') tensor(4.6443e-05, device='cuda:0') tensor(1.4853e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1174, device='cuda:0') tensor(4.6440e-05, device='cuda:0') tensor(1.4852e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1175, device='cuda:0') tensor(4.6438e-05, device='cuda:0') tensor(1.4851e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6434e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6434e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6434e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6434e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6433e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6433e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6433e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6433e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6433e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6432e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001660
Average total loss: 2.304245
tensor(-11.1176, device='cuda:0') tensor(4.6432e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
 Percentile value: -11.115622901916504
Non-zero model percentage: 0.8100091218948364%, Non-zero mask percentage: 0.8100091218948364%

--- Pruning Level [4/7]: ---
conv1.weight         | nonzeros =     496 /    1728             ( 28.70%) | total_pruned =    1232 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1227 /   36864             (  3.33%) | total_pruned =   35637 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      40 /      64             ( 62.50%) | total_pruned =      24 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    1837 /   36864             (  4.98%) | total_pruned =   35027 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      49 /      64             ( 76.56%) | total_pruned =      15 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      48 /      64             ( 75.00%) | total_pruned =      16 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1557 /   36864             (  4.22%) | total_pruned =   35307 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1546 /   36864             (  4.19%) | total_pruned =   35318 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      51 /      64             ( 79.69%) | total_pruned =      13 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    3129 /   73728             (  4.24%) | total_pruned =   70599 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    4368 /  147456             (  2.96%) | total_pruned =  143088 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     117 /     128             ( 91.41%) | total_pruned =      11 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =     110 /     128             ( 85.94%) | total_pruned =      18 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1116 /    8192             ( 13.62%) | total_pruned =    7076 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      88 /     128             ( 68.75%) | total_pruned =      40 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =     110 /     128             ( 85.94%) | total_pruned =      18 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     453 /  147456             (  0.31%) | total_pruned =  147003 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     376 /  147456             (  0.25%) | total_pruned =  147080 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      67 /     128             ( 52.34%) | total_pruned =      61 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     115 /     128             ( 89.84%) | total_pruned =      13 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    9373 /  294912             (  3.18%) | total_pruned =  285539 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     252 /     256             ( 98.44%) | total_pruned =       4 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     246 /     256             ( 96.09%) | total_pruned =      10 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   10922 /  589824             (  1.85%) | total_pruned =  578902 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     223 /     256             ( 87.11%) | total_pruned =      33 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     244 /     256             ( 95.31%) | total_pruned =      12 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    1045 /   32768             (  3.19%) | total_pruned =   31723 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     146 /     256             ( 57.03%) | total_pruned =     110 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     237 /     256             ( 92.58%) | total_pruned =      19 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =     280 /  589824             (  0.05%) | total_pruned =  589544 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      99 /     256             ( 38.67%) | total_pruned =     157 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     135 /     256             ( 52.73%) | total_pruned =     121 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =     237 /  589824             (  0.04%) | total_pruned =  589587 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      90 /     256             ( 35.16%) | total_pruned =     166 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     240 /     256             ( 93.75%) | total_pruned =      16 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    9609 / 1179648             (  0.81%) | total_pruned = 1170039 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     407 /     512             ( 79.49%) | total_pruned =     105 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     458 /     512             ( 89.45%) | total_pruned =      54 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    7442 / 2359296             (  0.32%) | total_pruned = 2351854 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     350 /     512             ( 68.36%) | total_pruned =     162 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     449 /     512             ( 87.70%) | total_pruned =      63 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     966 /  131072             (  0.74%) | total_pruned =  130106 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     174 /     512             ( 33.98%) | total_pruned =     338 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     445 /     512             ( 86.91%) | total_pruned =      67 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   11730 / 2359296             (  0.50%) | total_pruned = 2347566 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     338 /     512             ( 66.02%) | total_pruned =     174 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     334 /     512             ( 65.23%) | total_pruned =     178 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   10570 / 2359296             (  0.45%) | total_pruned = 2348726 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     462 /     512             ( 90.23%) | total_pruned =      50 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
linear.weight        | nonzeros =    5076 /    5120             ( 99.14%) | total_pruned =      44 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 90549, pruned : 11088213, total: 11178762, Compression rate :     123.46x  ( 99.19% pruned)
Train Epoch: 199/200 Loss: 0.138851 Accuracy: 77.26 98.75 % Best test Accuracy: 81.13%
tensor(-11.1176, device='cuda:0') tensor(4.6432e-05, device='cuda:0') tensor(1.4849e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001654
Average total loss: 2.304239
tensor(-11.1247, device='cuda:0') tensor(4.5266e-05, device='cuda:0') tensor(1.4743e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001642
Average total loss: 2.304227
tensor(-11.1318, device='cuda:0') tensor(4.4186e-05, device='cuda:0') tensor(1.4639e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001631
Average total loss: 2.304216
tensor(-11.1389, device='cuda:0') tensor(4.3241e-05, device='cuda:0') tensor(1.4536e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001619
Average total loss: 2.304204
tensor(-11.1459, device='cuda:0') tensor(4.2341e-05, device='cuda:0') tensor(1.4435e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001608
Average total loss: 2.304193
tensor(-11.1528, device='cuda:0') tensor(4.1544e-05, device='cuda:0') tensor(1.4335e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302583
Average KL loss: 0.001597
Average total loss: 2.304180
tensor(-11.1597, device='cuda:0') tensor(4.0870e-05, device='cuda:0') tensor(1.4236e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001586
Average total loss: 2.304171
tensor(-11.1666, device='cuda:0') tensor(4.0166e-05, device='cuda:0') tensor(1.4138e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001575
Average total loss: 2.304160
tensor(-11.1734, device='cuda:0') tensor(3.9479e-05, device='cuda:0') tensor(1.4042e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001565
Average total loss: 2.304149
tensor(-11.1802, device='cuda:0') tensor(3.8814e-05, device='cuda:0') tensor(1.3948e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001554
Average total loss: 2.304139
tensor(-11.1869, device='cuda:0') tensor(3.8170e-05, device='cuda:0') tensor(1.3854e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001544
Average total loss: 2.304129
tensor(-11.1936, device='cuda:0') tensor(3.7460e-05, device='cuda:0') tensor(1.3762e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302572
Average KL loss: 0.001533
Average total loss: 2.304105
tensor(-11.2002, device='cuda:0') tensor(3.6986e-05, device='cuda:0') tensor(1.3671e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001528
Average total loss: 2.304113
tensor(-11.2009, device='cuda:0') tensor(3.6926e-05, device='cuda:0') tensor(1.3662e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302583
Average KL loss: 0.001527
Average total loss: 2.304110
tensor(-11.2015, device='cuda:0') tensor(3.6879e-05, device='cuda:0') tensor(1.3653e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-11.2022, device='cuda:0') tensor(3.6820e-05, device='cuda:0') tensor(1.3644e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302575
Average KL loss: 0.001525
Average total loss: 2.304100
tensor(-11.2029, device='cuda:0') tensor(3.6766e-05, device='cuda:0') tensor(1.3635e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001524
Average total loss: 2.304109
tensor(-11.2035, device='cuda:0') tensor(3.6710e-05, device='cuda:0') tensor(1.3626e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001523
Average total loss: 2.304108
tensor(-11.2042, device='cuda:0') tensor(3.6645e-05, device='cuda:0') tensor(1.3617e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001522
Average total loss: 2.304107
tensor(-11.2048, device='cuda:0') tensor(3.6587e-05, device='cuda:0') tensor(1.3608e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302584
Average KL loss: 0.001521
Average total loss: 2.304105
tensor(-11.2055, device='cuda:0') tensor(3.6536e-05, device='cuda:0') tensor(1.3600e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001520
Average total loss: 2.304105
tensor(-11.2061, device='cuda:0') tensor(3.6483e-05, device='cuda:0') tensor(1.3591e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302584
Average KL loss: 0.001519
Average total loss: 2.304103
tensor(-11.2068, device='cuda:0') tensor(3.6440e-05, device='cuda:0') tensor(1.3582e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001518
Average total loss: 2.304103
tensor(-11.2074, device='cuda:0') tensor(3.6386e-05, device='cuda:0') tensor(1.3573e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2075, device='cuda:0') tensor(3.6375e-05, device='cuda:0') tensor(1.3572e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2075, device='cuda:0') tensor(3.6364e-05, device='cuda:0') tensor(1.3572e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2076, device='cuda:0') tensor(3.6352e-05, device='cuda:0') tensor(1.3571e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2076, device='cuda:0') tensor(3.6341e-05, device='cuda:0') tensor(1.3570e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2077, device='cuda:0') tensor(3.6330e-05, device='cuda:0') tensor(1.3570e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2077, device='cuda:0') tensor(3.6318e-05, device='cuda:0') tensor(1.3569e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2078, device='cuda:0') tensor(3.6307e-05, device='cuda:0') tensor(1.3568e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2078, device='cuda:0') tensor(3.6296e-05, device='cuda:0') tensor(1.3568e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304102
tensor(-11.2079, device='cuda:0') tensor(3.6285e-05, device='cuda:0') tensor(1.3567e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6273e-05, device='cuda:0') tensor(1.3567e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6262e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302607
Average KL loss: 0.001517
Average total loss: 2.304124
tensor(-11.2079, device='cuda:0') tensor(3.6262e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6262e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6262e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6262e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001517
Average total loss: 2.304101
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
 Percentile value: -11.177608489990234
Non-zero model percentage: 0.2430054396390915%, Non-zero mask percentage: 0.2430054396390915%

--- Pruning Level [5/7]: ---
conv1.weight         | nonzeros =     432 /    1728             ( 25.00%) | total_pruned =    1296 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     505 /   36864             (  1.37%) | total_pruned =   36359 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      38 /      64             ( 59.38%) | total_pruned =      26 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      34 /      64             ( 53.12%) | total_pruned =      30 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     659 /   36864             (  1.79%) | total_pruned =   36205 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      46 /      64             ( 71.88%) | total_pruned =      18 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      42 /      64             ( 65.62%) | total_pruned =      22 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     460 /   36864             (  1.25%) | total_pruned =   36404 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     488 /   36864             (  1.32%) | total_pruned =   36376 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      45 /      64             ( 70.31%) | total_pruned =      19 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     482 /   73728             (  0.65%) | total_pruned =   73246 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     110 /     128             ( 85.94%) | total_pruned =      18 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     760 /  147456             (  0.52%) | total_pruned =  146696 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     106 /     128             ( 82.81%) | total_pruned =      22 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     517 /    8192             (  6.31%) | total_pruned =    7675 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      95 /     128             ( 74.22%) | total_pruned =      33 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      75 /  147456             (  0.05%) | total_pruned =  147381 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      45 /     128             ( 35.16%) | total_pruned =      83 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      43 /     128             ( 33.59%) | total_pruned =      85 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      54 /  147456             (  0.04%) | total_pruned =  147402 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      37 /     128             ( 28.91%) | total_pruned =      91 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =     110 /     128             ( 85.94%) | total_pruned =      18 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    1579 /  294912             (  0.54%) | total_pruned =  293333 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     244 /     256             ( 95.31%) | total_pruned =      12 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     241 /     256             ( 94.14%) | total_pruned =      15 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    2289 /  589824             (  0.39%) | total_pruned =  587535 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     200 /     256             ( 78.12%) | total_pruned =      56 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     221 /     256             ( 86.33%) | total_pruned =      35 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     371 /   32768             (  1.13%) | total_pruned =   32397 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     222 /     256             ( 86.72%) | total_pruned =      34 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      17 /  589824             (  0.00%) | total_pruned =  589807 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      42 /     256             ( 16.41%) | total_pruned =     214 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      77 /     256             ( 30.08%) | total_pruned =     179 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      23 /  589824             (  0.00%) | total_pruned =  589801 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      42 /     256             ( 16.41%) | total_pruned =     214 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     224 /     256             ( 87.50%) | total_pruned =      32 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    1895 / 1179648             (  0.16%) | total_pruned = 1177753 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     351 /     512             ( 68.55%) | total_pruned =     161 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     415 /     512             ( 81.05%) | total_pruned =      97 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    1262 / 2359296             (  0.05%) | total_pruned = 2358034 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     288 /     512             ( 56.25%) | total_pruned =     224 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     396 /     512             ( 77.34%) | total_pruned =     116 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     193 /  131072             (  0.15%) | total_pruned =  130879 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      95 /     512             ( 18.55%) | total_pruned =     417 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     397 /     512             ( 77.54%) | total_pruned =     115 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    1975 / 2359296             (  0.08%) | total_pruned = 2357321 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     283 /     512             ( 55.27%) | total_pruned =     229 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     241 /     512             ( 47.07%) | total_pruned =     271 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    1944 / 2359296             (  0.08%) | total_pruned = 2357352 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     432 /     512             ( 84.38%) | total_pruned =      80 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     511 /     512             ( 99.80%) | total_pruned =       1 | shape = torch.Size([512])
linear.weight        | nonzeros =    5036 /    5120             ( 98.36%) | total_pruned =      84 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 27165, pruned : 11151597, total: 11178762, Compression rate :     411.51x  ( 99.76% pruned)
Train Epoch: 112/200 Loss: 1.039396 Accuracy: 64.97 67.59 % Best test Accuracy: 65.18%
tensor(-11.2079, device='cuda:0') tensor(3.6261e-05, device='cuda:0') tensor(1.3566e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001512
Average total loss: 2.304097
tensor(-11.2145, device='cuda:0') tensor(3.5477e-05, device='cuda:0') tensor(1.3477e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001502
Average total loss: 2.304087
tensor(-11.2210, device='cuda:0') tensor(3.4740e-05, device='cuda:0') tensor(1.3390e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001492
Average total loss: 2.304077
tensor(-11.2275, device='cuda:0') tensor(3.4055e-05, device='cuda:0') tensor(1.3304e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001483
Average total loss: 2.304067
tensor(-11.2339, device='cuda:0') tensor(3.3397e-05, device='cuda:0') tensor(1.3219e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001473
Average total loss: 2.304058
tensor(-11.2403, device='cuda:0') tensor(3.2770e-05, device='cuda:0') tensor(1.3135e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001464
Average total loss: 2.304049
tensor(-11.2466, device='cuda:0') tensor(3.2170e-05, device='cuda:0') tensor(1.3052e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001455
Average total loss: 2.304039
tensor(-11.2529, device='cuda:0') tensor(3.1595e-05, device='cuda:0') tensor(1.2970e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302591
Average KL loss: 0.001445
Average total loss: 2.304036
tensor(-11.2592, device='cuda:0') tensor(3.1014e-05, device='cuda:0') tensor(1.2889e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001436
Average total loss: 2.304021
tensor(-11.2654, device='cuda:0') tensor(3.0481e-05, device='cuda:0') tensor(1.2809e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001428
Average total loss: 2.304012
tensor(-11.2716, device='cuda:0') tensor(2.9968e-05, device='cuda:0') tensor(1.2730e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001419
Average total loss: 2.304004
tensor(-11.2777, device='cuda:0') tensor(2.9471e-05, device='cuda:0') tensor(1.2652e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001410
Average total loss: 2.303995
tensor(-11.2838, device='cuda:0') tensor(2.8991e-05, device='cuda:0') tensor(1.2575e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001405
Average total loss: 2.303990
tensor(-11.2844, device='cuda:0') tensor(2.8944e-05, device='cuda:0') tensor(1.2567e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-11.2850, device='cuda:0') tensor(2.8898e-05, device='cuda:0') tensor(1.2560e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-11.2856, device='cuda:0') tensor(2.8851e-05, device='cuda:0') tensor(1.2552e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001403
Average total loss: 2.303988
tensor(-11.2862, device='cuda:0') tensor(2.8805e-05, device='cuda:0') tensor(1.2544e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001402
Average total loss: 2.303987
tensor(-11.2868, device='cuda:0') tensor(2.8759e-05, device='cuda:0') tensor(1.2537e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001401
Average total loss: 2.303986
tensor(-11.2875, device='cuda:0') tensor(2.8712e-05, device='cuda:0') tensor(1.2529e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001400
Average total loss: 2.303985
tensor(-11.2881, device='cuda:0') tensor(2.8667e-05, device='cuda:0') tensor(1.2521e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001399
Average total loss: 2.303984
tensor(-11.2887, device='cuda:0') tensor(2.8632e-05, device='cuda:0') tensor(1.2514e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001398
Average total loss: 2.303983
tensor(-11.2893, device='cuda:0') tensor(2.8590e-05, device='cuda:0') tensor(1.2506e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001398
Average total loss: 2.303983
tensor(-11.2899, device='cuda:0') tensor(2.8547e-05, device='cuda:0') tensor(1.2499e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001397
Average total loss: 2.303982
tensor(-11.2905, device='cuda:0') tensor(2.8504e-05, device='cuda:0') tensor(1.2491e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2905, device='cuda:0') tensor(2.8497e-05, device='cuda:0') tensor(1.2490e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2906, device='cuda:0') tensor(2.8489e-05, device='cuda:0') tensor(1.2490e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2906, device='cuda:0') tensor(2.8482e-05, device='cuda:0') tensor(1.2489e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2907, device='cuda:0') tensor(2.8474e-05, device='cuda:0') tensor(1.2489e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2907, device='cuda:0') tensor(2.8467e-05, device='cuda:0') tensor(1.2488e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2908, device='cuda:0') tensor(2.8459e-05, device='cuda:0') tensor(1.2488e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2908, device='cuda:0') tensor(2.8452e-05, device='cuda:0') tensor(1.2487e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2909, device='cuda:0') tensor(2.8444e-05, device='cuda:0') tensor(1.2486e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2909, device='cuda:0') tensor(2.8437e-05, device='cuda:0') tensor(1.2486e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8429e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8422e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8421e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8421e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001396
Average total loss: 2.303981
tensor(-11.2910, device='cuda:0') tensor(2.8421e-05, device='cuda:0') tensor(1.2485e-10, device='cuda:0')
 Percentile value: -11.116487598419189
Non-zero model percentage: 0.07290609925985336%, Non-zero mask percentage: 0.07290609925985336%

--- Pruning Level [6/7]: ---
conv1.weight         | nonzeros =     167 /    1728             (  9.66%) | total_pruned =    1561 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =      22 /      64             ( 34.38%) | total_pruned =      42 | shape = torch.Size([64])
bn1.bias             | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =      42 /   36864             (  0.11%) | total_pruned =   36822 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =      36 /   36864             (  0.10%) | total_pruned =   36828 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =      21 /   36864             (  0.06%) | total_pruned =   36843 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       1 /      64             (  1.56%) | total_pruned =      63 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =      24 /   36864             (  0.07%) | total_pruned =   36840 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =       2 /   73728             (  0.00%) | total_pruned =   73726 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      47 /     128             ( 36.72%) | total_pruned =      81 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       4 /     128             (  3.12%) | total_pruned =     124 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =       6 /  147456             (  0.00%) | total_pruned =  147450 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      16 /     128             ( 12.50%) | total_pruned =     112 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =      70 /    8192             (  0.85%) | total_pruned =    8122 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      40 /     128             ( 31.25%) | total_pruned =      88 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      15 /     128             ( 11.72%) | total_pruned =     113 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =       3 /  147456             (  0.00%) | total_pruned =  147453 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       3 /     128             (  2.34%) | total_pruned =     125 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =       2 /  147456             (  0.00%) | total_pruned =  147454 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =       4 /     128             (  3.12%) | total_pruned =     124 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      31 /     128             ( 24.22%) | total_pruned =      97 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =      42 /  294912             (  0.01%) | total_pruned =  294870 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     125 /     256             ( 48.83%) | total_pruned =     131 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     122 /     256             ( 47.66%) | total_pruned =     134 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =      52 /  589824             (  0.01%) | total_pruned =  589772 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      92 /     256             ( 35.94%) | total_pruned =     164 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     112 /     256             ( 43.75%) | total_pruned =     144 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =      32 /   32768             (  0.10%) | total_pruned =   32736 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      23 /     256             (  8.98%) | total_pruned =     233 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       2 /     256             (  0.78%) | total_pruned =     254 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       6 /     256             (  2.34%) | total_pruned =     250 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       0 /  589824             (  0.00%) | total_pruned =  589824 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       3 /     256             (  1.17%) | total_pruned =     253 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     100 /     256             ( 39.06%) | total_pruned =     156 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =      53 / 1179648             (  0.00%) | total_pruned = 1179595 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     102 /     512             ( 19.92%) | total_pruned =     410 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     164 /     512             ( 32.03%) | total_pruned =     348 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =      49 / 2359296             (  0.00%) | total_pruned = 2359247 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =      98 /     512             ( 19.14%) | total_pruned =     414 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     260 /     512             ( 50.78%) | total_pruned =     252 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =       5 /  131072             (  0.00%) | total_pruned =  131067 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      11 /     512             (  2.15%) | total_pruned =     501 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     260 /     512             ( 50.78%) | total_pruned =     252 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =      27 / 2359296             (  0.00%) | total_pruned = 2359269 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     124 /     512             ( 24.22%) | total_pruned =     388 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      54 /     512             ( 10.55%) | total_pruned =     458 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =      51 / 2359296             (  0.00%) | total_pruned = 2359245 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     204 /     512             ( 39.84%) | total_pruned =     308 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     510 /     512             ( 99.61%) | total_pruned =       2 | shape = torch.Size([512])
linear.weight        | nonzeros =    4602 /    5120             ( 89.88%) | total_pruned =     518 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8150, pruned : 11170612, total: 11178762, Compression rate :    1371.63x  ( 99.93% pruned)
Train Epoch: 56/200 Loss: 2.302637 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
