Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Non-zero model percentage: 99.95706176757812%, Non-zero mask percentage: 99.99999237060547%

--- Pruning Level [0/7]: ---
conv1.weight         | nonzeros =    1728 /    1728             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.weight           | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
bn1.bias             | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   36864 /   36864             (100.00%) | total_pruned =       0 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      64 /      64             (100.00%) | total_pruned =       0 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   73728 /   73728             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =  147456 /  147456             (100.00%) | total_pruned =       0 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =     128 /     128             (100.00%) | total_pruned =       0 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  294912 /  294912             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =  589824 /  589824             (100.00%) | total_pruned =       0 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     256 /     256             (100.00%) | total_pruned =       0 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros = 1179648 / 1179648             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 2359296 / 2359296             (100.00%) | total_pruned =       0 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
linear.weight        | nonzeros =    5120 /    5120             (100.00%) | total_pruned =       0 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 11173962, pruned : 4800, total: 11178762, Compression rate :       1.00x  (  0.04% pruned)
Train Epoch: 57/200 Loss: 0.015782 Accuracy: 90.13 100.00 % Best test Accuracy: 90.50%
tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302932
Average KL loss: 4926.653735
Average total loss: 4928.956882
tensor(-0.4872, device='cuda:0') tensor(6.8873e-09, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302899
Average KL loss: 3675.506276
Average total loss: 3677.809336
tensor(-0.9489, device='cuda:0') tensor(2.0103e-08, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302798
Average KL loss: 2684.637036
Average total loss: 2686.939981
tensor(-1.3614, device='cuda:0') tensor(3.1459e-08, device='cuda:0') tensor(0.0002, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302733
Average KL loss: 1977.440036
Average total loss: 1979.742845
tensor(-1.7189, device='cuda:0') tensor(4.0279e-08, device='cuda:0') tensor(0.0001, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302631
Average KL loss: 1492.164821
Average total loss: 1494.467517
tensor(-2.0266, device='cuda:0') tensor(4.4611e-08, device='cuda:0') tensor(0.0001, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302620
Average KL loss: 1158.189523
Average total loss: 1160.492195
tensor(-2.2930, device='cuda:0') tensor(4.5368e-08, device='cuda:0') tensor(8.3298e-05, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302599
Average KL loss: 923.190991
Average total loss: 925.493637
tensor(-2.5261, device='cuda:0') tensor(4.5360e-08, device='cuda:0') tensor(6.8563e-05, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302610
Average KL loss: 753.156157
Average total loss: 755.458801
tensor(-2.7327, device='cuda:0') tensor(4.5056e-08, device='cuda:0') tensor(5.7342e-05, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302607
Average KL loss: 626.674186
Average total loss: 628.976825
tensor(-2.9177, device='cuda:0') tensor(4.4921e-08, device='cuda:0') tensor(4.8655e-05, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302565
Average KL loss: 530.178858
Average total loss: 532.481447
tensor(-3.0851, device='cuda:0') tensor(4.4703e-08, device='cuda:0') tensor(4.1814e-05, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302560
Average KL loss: 454.895132
Average total loss: 457.197713
tensor(-3.2379, device='cuda:0') tensor(4.4892e-08, device='cuda:0') tensor(3.6338e-05, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302602
Average KL loss: 394.999217
Average total loss: 397.301837
tensor(-3.3784, device='cuda:0') tensor(4.3959e-08, device='cuda:0') tensor(3.1888e-05, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302582
Average KL loss: 346.525001
Average total loss: 348.827596
tensor(-3.5086, device='cuda:0') tensor(4.2731e-08, device='cuda:0') tensor(2.8224e-05, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302574
Average KL loss: 306.704267
Average total loss: 309.006853
tensor(-3.6298, device='cuda:0') tensor(4.2322e-08, device='cuda:0') tensor(2.5169e-05, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302586
Average KL loss: 273.560479
Average total loss: 275.863080
tensor(-3.7433, device='cuda:0') tensor(4.1864e-08, device='cuda:0') tensor(2.2594e-05, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302584
Average KL loss: 245.653376
Average total loss: 247.955973
tensor(-3.8500, device='cuda:0') tensor(4.1663e-08, device='cuda:0') tensor(2.0402e-05, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302581
Average KL loss: 221.913308
Average total loss: 224.215900
tensor(-3.9508, device='cuda:0') tensor(4.2096e-08, device='cuda:0') tensor(1.8520e-05, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302586
Average KL loss: 201.532685
Average total loss: 203.835281
tensor(-4.0463, device='cuda:0') tensor(4.2340e-08, device='cuda:0') tensor(1.6891e-05, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302582
Average KL loss: 183.892526
Average total loss: 186.195118
tensor(-4.1371, device='cuda:0') tensor(4.2233e-08, device='cuda:0') tensor(1.5471e-05, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302584
Average KL loss: 168.511230
Average total loss: 170.813822
tensor(-4.2237, device='cuda:0') tensor(4.2141e-08, device='cuda:0') tensor(1.4224e-05, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302575
Average KL loss: 155.009953
Average total loss: 157.312535
tensor(-4.3065, device='cuda:0') tensor(4.2113e-08, device='cuda:0') tensor(1.3124e-05, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302587
Average KL loss: 143.086756
Average total loss: 145.389349
tensor(-4.3859, device='cuda:0') tensor(4.1759e-08, device='cuda:0') tensor(1.2147e-05, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302581
Average KL loss: 132.499738
Average total loss: 134.802325
tensor(-4.4622, device='cuda:0') tensor(4.1553e-08, device='cuda:0') tensor(1.1276e-05, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302589
Average KL loss: 123.051338
Average total loss: 125.353933
tensor(-4.5355, device='cuda:0') tensor(4.1345e-08, device='cuda:0') tensor(1.0495e-05, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302581
Average KL loss: 114.579886
Average total loss: 116.882472
tensor(-4.6063, device='cuda:0') tensor(4.1761e-08, device='cuda:0') tensor(9.7922e-06, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 106.952127
Average total loss: 109.254716
tensor(-4.6746, device='cuda:0') tensor(4.1627e-08, device='cuda:0') tensor(9.1572e-06, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302586
Average KL loss: 100.056968
Average total loss: 102.359557
tensor(-4.7408, device='cuda:0') tensor(4.1637e-08, device='cuda:0') tensor(8.5813e-06, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302593
Average KL loss: 93.801084
Average total loss: 96.103681
tensor(-4.8049, device='cuda:0') tensor(4.1377e-08, device='cuda:0') tensor(8.0573e-06, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 88.106168
Average total loss: 90.408757
tensor(-4.8670, device='cuda:0') tensor(4.1189e-08, device='cuda:0') tensor(7.5790e-06, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302587
Average KL loss: 82.905541
Average total loss: 85.208131
tensor(-4.9275, device='cuda:0') tensor(4.1044e-08, device='cuda:0') tensor(7.1411e-06, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302587
Average KL loss: 78.142375
Average total loss: 80.444964
tensor(-4.9862, device='cuda:0') tensor(4.0799e-08, device='cuda:0') tensor(6.7391e-06, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302584
Average KL loss: 73.767861
Average total loss: 76.070447
tensor(-5.0434, device='cuda:0') tensor(4.0696e-08, device='cuda:0') tensor(6.3691e-06, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302582
Average KL loss: 69.739905
Average total loss: 72.042488
tensor(-5.0992, device='cuda:0') tensor(4.0720e-08, device='cuda:0') tensor(6.0278e-06, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302586
Average KL loss: 66.022100
Average total loss: 68.324688
tensor(-5.1536, device='cuda:0') tensor(4.0645e-08, device='cuda:0') tensor(5.7121e-06, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302584
Average KL loss: 62.582803
Average total loss: 64.885391
tensor(-5.2068, device='cuda:0') tensor(4.0549e-08, device='cuda:0') tensor(5.4196e-06, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302583
Average KL loss: 59.394416
Average total loss: 61.697003
tensor(-5.2588, device='cuda:0') tensor(4.0348e-08, device='cuda:0') tensor(5.1480e-06, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302579
Average KL loss: 56.432723
Average total loss: 58.735304
tensor(-5.3096, device='cuda:0') tensor(4.0249e-08, device='cuda:0') tensor(4.8953e-06, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302584
Average KL loss: 53.676114
Average total loss: 55.978701
tensor(-5.3594, device='cuda:0') tensor(4.0174e-08, device='cuda:0') tensor(4.6597e-06, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302581
Average KL loss: 51.105934
Average total loss: 53.408518
tensor(-5.4082, device='cuda:0') tensor(4.0135e-08, device='cuda:0') tensor(4.4398e-06, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302584
Average KL loss: 48.705459
Average total loss: 51.008046
tensor(-5.4560, device='cuda:0') tensor(4.0179e-08, device='cuda:0') tensor(4.2342e-06, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302587
Average KL loss: 46.459900
Average total loss: 48.762490
tensor(-5.5030, device='cuda:0') tensor(4.0110e-08, device='cuda:0') tensor(4.0416e-06, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302584
Average KL loss: 44.356017
Average total loss: 46.658603
tensor(-5.5491, device='cuda:0') tensor(3.9992e-08, device='cuda:0') tensor(3.8609e-06, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302586
Average KL loss: 42.381976
Average total loss: 44.684565
tensor(-5.5944, device='cuda:0') tensor(3.9931e-08, device='cuda:0') tensor(3.6912e-06, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302590
Average KL loss: 40.527270
Average total loss: 42.829862
tensor(-5.6389, device='cuda:0') tensor(3.9821e-08, device='cuda:0') tensor(3.5316e-06, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302586
Average KL loss: 38.782279
Average total loss: 41.084867
tensor(-5.6827, device='cuda:0') tensor(4.0807e-08, device='cuda:0') tensor(3.3813e-06, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302584
Average KL loss: 37.138522
Average total loss: 39.441109
tensor(-5.7258, device='cuda:0') tensor(4.0690e-08, device='cuda:0') tensor(3.2396e-06, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302584
Average KL loss: 35.588221
Average total loss: 37.890808
tensor(-5.7682, device='cuda:0') tensor(4.1035e-08, device='cuda:0') tensor(3.1058e-06, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 34.124387
Average total loss: 36.426975
tensor(-5.8101, device='cuda:0') tensor(4.0857e-08, device='cuda:0') tensor(2.9794e-06, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302584
Average KL loss: 32.740700
Average total loss: 35.043286
tensor(-5.8513, device='cuda:0') tensor(4.0805e-08, device='cuda:0') tensor(2.8598e-06, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302587
Average KL loss: 31.431313
Average total loss: 33.733902
tensor(-5.8919, device='cuda:0') tensor(4.0828e-08, device='cuda:0') tensor(2.7465e-06, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 30.191146
Average total loss: 32.493732
tensor(-5.9320, device='cuda:0') tensor(4.0750e-08, device='cuda:0') tensor(2.6392e-06, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 29.015351
Average total loss: 31.317938
tensor(-5.9716, device='cuda:0') tensor(4.0683e-08, device='cuda:0') tensor(2.5373e-06, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302584
Average KL loss: 27.899546
Average total loss: 30.202133
tensor(-6.0106, device='cuda:0') tensor(4.0599e-08, device='cuda:0') tensor(2.4406e-06, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 26.839727
Average total loss: 29.142313
tensor(-6.0492, device='cuda:0') tensor(4.0580e-08, device='cuda:0') tensor(2.3487e-06, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 25.832238
Average total loss: 28.134825
tensor(-6.0873, device='cuda:0') tensor(4.0469e-08, device='cuda:0') tensor(2.2612e-06, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 24.873707
Average total loss: 27.176294
tensor(-6.1250, device='cuda:0') tensor(4.0259e-08, device='cuda:0') tensor(2.1780e-06, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302587
Average KL loss: 23.961033
Average total loss: 26.263621
tensor(-6.1622, device='cuda:0') tensor(4.0061e-08, device='cuda:0') tensor(2.0987e-06, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302587
Average KL loss: 23.091379
Average total loss: 25.393967
tensor(-6.1991, device='cuda:0') tensor(3.8839e-08, device='cuda:0') tensor(2.0231e-06, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 22.262106
Average total loss: 24.564693
tensor(-6.2355, device='cuda:0') tensor(3.8498e-08, device='cuda:0') tensor(1.9510e-06, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302584
Average KL loss: 21.470775
Average total loss: 23.773360
tensor(-6.2716, device='cuda:0') tensor(3.9217e-08, device='cuda:0') tensor(1.8821e-06, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 20.715187
Average total loss: 23.017773
tensor(-6.3073, device='cuda:0') tensor(3.9086e-08, device='cuda:0') tensor(1.8163e-06, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 19.993242
Average total loss: 22.295828
tensor(-6.3426, device='cuda:0') tensor(3.8953e-08, device='cuda:0') tensor(1.7535e-06, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302586
Average KL loss: 19.303000
Average total loss: 21.605588
tensor(-6.3777, device='cuda:0') tensor(4.0069e-08, device='cuda:0') tensor(1.6933e-06, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302586
Average KL loss: 18.642674
Average total loss: 20.945262
tensor(-6.4124, device='cuda:0') tensor(3.9864e-08, device='cuda:0') tensor(1.6357e-06, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 18.010595
Average total loss: 20.313182
tensor(-6.4468, device='cuda:0') tensor(3.9653e-08, device='cuda:0') tensor(1.5806e-06, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302586
Average KL loss: 17.405231
Average total loss: 19.707818
tensor(-6.4809, device='cuda:0') tensor(3.9578e-08, device='cuda:0') tensor(1.5278e-06, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 16.825145
Average total loss: 19.127731
tensor(-6.5147, device='cuda:0') tensor(3.9412e-08, device='cuda:0') tensor(1.4772e-06, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 16.268971
Average total loss: 18.571557
tensor(-6.5482, device='cuda:0') tensor(3.9193e-08, device='cuda:0') tensor(1.4286e-06, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302579
Average KL loss: 15.735453
Average total loss: 18.038033
tensor(-6.5814, device='cuda:0') tensor(3.9143e-08, device='cuda:0') tensor(1.3820e-06, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302582
Average KL loss: 15.223424
Average total loss: 17.526007
tensor(-6.6144, device='cuda:0') tensor(4.0107e-08, device='cuda:0') tensor(1.3373e-06, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 14.731788
Average total loss: 17.034374
tensor(-6.6472, device='cuda:0') tensor(4.0818e-08, device='cuda:0') tensor(1.2943e-06, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 14.259520
Average total loss: 16.562106
tensor(-6.6797, device='cuda:0') tensor(4.0675e-08, device='cuda:0') tensor(1.2530e-06, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 13.805654
Average total loss: 16.108240
tensor(-6.7119, device='cuda:0') tensor(4.0548e-08, device='cuda:0') tensor(1.2133e-06, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 13.369292
Average total loss: 15.671877
tensor(-6.7440, device='cuda:0') tensor(4.0420e-08, device='cuda:0') tensor(1.1752e-06, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302584
Average KL loss: 12.949569
Average total loss: 15.252154
tensor(-6.7758, device='cuda:0') tensor(4.0386e-08, device='cuda:0') tensor(1.1384e-06, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 12.545702
Average total loss: 14.848287
tensor(-6.8074, device='cuda:0') tensor(4.0382e-08, device='cuda:0') tensor(1.1031e-06, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 12.156943
Average total loss: 14.459528
tensor(-6.8388, device='cuda:0') tensor(4.0480e-08, device='cuda:0') tensor(1.0691e-06, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 11.782583
Average total loss: 14.085169
tensor(-6.8700, device='cuda:0') tensor(4.0408e-08, device='cuda:0') tensor(1.0363e-06, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302583
Average KL loss: 11.421960
Average total loss: 13.724543
tensor(-6.9011, device='cuda:0') tensor(4.0329e-08, device='cuda:0') tensor(1.0047e-06, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 11.074440
Average total loss: 13.377025
tensor(-6.9319, device='cuda:0') tensor(4.0259e-08, device='cuda:0') tensor(9.7424e-07, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302584
Average KL loss: 10.739436
Average total loss: 13.042021
tensor(-6.9626, device='cuda:0') tensor(4.0278e-08, device='cuda:0') tensor(9.4489e-07, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 10.416394
Average total loss: 12.718979
tensor(-6.9930, device='cuda:0') tensor(4.0299e-08, device='cuda:0') tensor(9.1657e-07, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302587
Average KL loss: 10.104784
Average total loss: 12.407371
tensor(-7.0233, device='cuda:0') tensor(4.0433e-08, device='cuda:0') tensor(8.8925e-07, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 9.804105
Average total loss: 12.106690
tensor(-7.0535, device='cuda:0') tensor(4.1052e-08, device='cuda:0') tensor(8.6289e-07, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 9.513892
Average total loss: 11.816477
tensor(-7.0835, device='cuda:0') tensor(4.0967e-08, device='cuda:0') tensor(8.3743e-07, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 9.233694
Average total loss: 11.536279
tensor(-7.1133, device='cuda:0') tensor(4.0853e-08, device='cuda:0') tensor(8.1285e-07, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302584
Average KL loss: 8.963087
Average total loss: 11.265672
tensor(-7.1430, device='cuda:0') tensor(4.1027e-08, device='cuda:0') tensor(7.8911e-07, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 8.701673
Average total loss: 11.004258
tensor(-7.1726, device='cuda:0') tensor(4.1351e-08, device='cuda:0') tensor(7.6617e-07, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302586
Average KL loss: 8.449074
Average total loss: 10.751660
tensor(-7.2020, device='cuda:0') tensor(4.1579e-08, device='cuda:0') tensor(7.4399e-07, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 8.204928
Average total loss: 10.507513
tensor(-7.2313, device='cuda:0') tensor(4.1232e-08, device='cuda:0') tensor(7.2256e-07, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 7.968890
Average total loss: 10.271475
tensor(-7.2604, device='cuda:0') tensor(4.0938e-08, device='cuda:0') tensor(7.0184e-07, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 7.740635
Average total loss: 10.043220
tensor(-7.2894, device='cuda:0') tensor(4.1283e-08, device='cuda:0') tensor(6.8179e-07, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 7.519859
Average total loss: 9.822445
tensor(-7.3183, device='cuda:0') tensor(4.1450e-08, device='cuda:0') tensor(6.6240e-07, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 7.306263
Average total loss: 9.608849
tensor(-7.3471, device='cuda:0') tensor(4.1265e-08, device='cuda:0') tensor(6.4364e-07, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 7.099568
Average total loss: 9.402153
tensor(-7.3757, device='cuda:0') tensor(4.1084e-08, device='cuda:0') tensor(6.2547e-07, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 6.899507
Average total loss: 9.202092
tensor(-7.4043, device='cuda:0') tensor(4.0919e-08, device='cuda:0') tensor(6.0789e-07, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 6.705827
Average total loss: 9.008412
tensor(-7.4327, device='cuda:0') tensor(4.0767e-08, device='cuda:0') tensor(5.9087e-07, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 6.518281
Average total loss: 8.820866
tensor(-7.4611, device='cuda:0') tensor(4.0632e-08, device='cuda:0') tensor(5.7439e-07, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 6.336640
Average total loss: 8.639225
tensor(-7.4893, device='cuda:0') tensor(4.0501e-08, device='cuda:0') tensor(5.5842e-07, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 6.160690
Average total loss: 8.463275
tensor(-7.5174, device='cuda:0') tensor(4.0485e-08, device='cuda:0') tensor(5.4295e-07, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 5.990215
Average total loss: 8.292800
tensor(-7.5454, device='cuda:0') tensor(4.0435e-08, device='cuda:0') tensor(5.2796e-07, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 5.825014
Average total loss: 8.127599
tensor(-7.5734, device='cuda:0') tensor(4.0328e-08, device='cuda:0') tensor(5.1343e-07, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 5.664897
Average total loss: 7.967482
tensor(-7.6012, device='cuda:0') tensor(4.0247e-08, device='cuda:0') tensor(4.9935e-07, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 5.509680
Average total loss: 7.812266
tensor(-7.6290, device='cuda:0') tensor(4.0468e-08, device='cuda:0') tensor(4.8570e-07, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302589
Average KL loss: 5.359187
Average total loss: 7.661776
tensor(-7.6566, device='cuda:0') tensor(4.7277e-08, device='cuda:0') tensor(4.7246e-07, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 5.213250
Average total loss: 7.515835
tensor(-7.6842, device='cuda:0') tensor(4.5207e-08, device='cuda:0') tensor(4.5962e-07, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 5.071708
Average total loss: 7.374292
tensor(-7.7117, device='cuda:0') tensor(4.5805e-08, device='cuda:0') tensor(4.4716e-07, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 4.934407
Average total loss: 7.236992
tensor(-7.7391, device='cuda:0') tensor(4.5324e-08, device='cuda:0') tensor(4.3508e-07, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 4.801200
Average total loss: 7.103785
tensor(-7.7664, device='cuda:0') tensor(4.4739e-08, device='cuda:0') tensor(4.2336e-07, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 4.671946
Average total loss: 6.974532
tensor(-7.7937, device='cuda:0') tensor(4.4199e-08, device='cuda:0') tensor(4.1198e-07, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 4.546510
Average total loss: 6.849095
tensor(-7.8209, device='cuda:0') tensor(4.3707e-08, device='cuda:0') tensor(4.0094e-07, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 4.424762
Average total loss: 6.727348
tensor(-7.8480, device='cuda:0') tensor(4.3209e-08, device='cuda:0') tensor(3.9022e-07, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 4.306577
Average total loss: 6.609162
tensor(-7.8751, device='cuda:0') tensor(4.2781e-08, device='cuda:0') tensor(3.7981e-07, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 4.191833
Average total loss: 6.494418
tensor(-7.9020, device='cuda:0') tensor(4.2421e-08, device='cuda:0') tensor(3.6971e-07, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 4.080419
Average total loss: 6.383004
tensor(-7.9290, device='cuda:0') tensor(4.2074e-08, device='cuda:0') tensor(3.5990e-07, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 3.972222
Average total loss: 6.274807
tensor(-7.9558, device='cuda:0') tensor(4.1796e-08, device='cuda:0') tensor(3.5037e-07, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302583
Average KL loss: 3.867139
Average total loss: 6.169722
tensor(-7.9826, device='cuda:0') tensor(4.2913e-08, device='cuda:0') tensor(3.4112e-07, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 3.765065
Average total loss: 6.067650
tensor(-8.0093, device='cuda:0') tensor(4.2800e-08, device='cuda:0') tensor(3.3213e-07, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 3.665906
Average total loss: 5.968491
tensor(-8.0360, device='cuda:0') tensor(4.2571e-08, device='cuda:0') tensor(3.2339e-07, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 3.569567
Average total loss: 5.872152
tensor(-8.0626, device='cuda:0') tensor(4.2371e-08, device='cuda:0') tensor(3.1491e-07, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 3.475957
Average total loss: 5.778542
tensor(-8.0892, device='cuda:0') tensor(4.2189e-08, device='cuda:0') tensor(3.0666e-07, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 3.384986
Average total loss: 5.687571
tensor(-8.1157, device='cuda:0') tensor(4.2052e-08, device='cuda:0') tensor(2.9864e-07, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 3.296576
Average total loss: 5.599161
tensor(-8.1421, device='cuda:0') tensor(4.1973e-08, device='cuda:0') tensor(2.9085e-07, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302588
Average KL loss: 3.210642
Average total loss: 5.513230
tensor(-8.1685, device='cuda:0') tensor(4.1473e-08, device='cuda:0') tensor(2.8328e-07, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 3.127106
Average total loss: 5.429691
tensor(-8.1948, device='cuda:0') tensor(4.1530e-08, device='cuda:0') tensor(2.7592e-07, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 3.045897
Average total loss: 5.348482
tensor(-8.2211, device='cuda:0') tensor(4.1509e-08, device='cuda:0') tensor(2.6876e-07, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 2.966939
Average total loss: 5.269524
tensor(-8.2474, device='cuda:0') tensor(4.1504e-08, device='cuda:0') tensor(2.6180e-07, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 2.890163
Average total loss: 5.192748
tensor(-8.2736, device='cuda:0') tensor(4.1518e-08, device='cuda:0') tensor(2.5504e-07, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 2.815503
Average total loss: 5.118088
tensor(-8.2997, device='cuda:0') tensor(4.1553e-08, device='cuda:0') tensor(2.4846e-07, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 2.742896
Average total loss: 5.045481
tensor(-8.3259, device='cuda:0') tensor(4.1606e-08, device='cuda:0') tensor(2.4206e-07, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 2.672277
Average total loss: 4.974862
tensor(-8.3519, device='cuda:0') tensor(4.1676e-08, device='cuda:0') tensor(2.3583e-07, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 2.603586
Average total loss: 4.906172
tensor(-8.3780, device='cuda:0') tensor(4.1138e-08, device='cuda:0') tensor(2.2977e-07, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 2.536766
Average total loss: 4.839351
tensor(-8.4039, device='cuda:0') tensor(4.1271e-08, device='cuda:0') tensor(2.2388e-07, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 2.471761
Average total loss: 4.774346
tensor(-8.4299, device='cuda:0') tensor(4.1400e-08, device='cuda:0') tensor(2.1815e-07, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 2.408515
Average total loss: 4.711099
tensor(-8.4558, device='cuda:0') tensor(4.1541e-08, device='cuda:0') tensor(2.1258e-07, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 2.346977
Average total loss: 4.649562
tensor(-8.4817, device='cuda:0') tensor(4.1699e-08, device='cuda:0') tensor(2.0715e-07, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 2.287100
Average total loss: 4.589685
tensor(-8.5075, device='cuda:0') tensor(4.1864e-08, device='cuda:0') tensor(2.0187e-07, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 2.228832
Average total loss: 4.531417
tensor(-8.5333, device='cuda:0') tensor(4.2046e-08, device='cuda:0') tensor(1.9673e-07, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 2.172122
Average total loss: 4.474707
tensor(-8.5590, device='cuda:0') tensor(4.2231e-08, device='cuda:0') tensor(1.9173e-07, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 2.116928
Average total loss: 4.419513
tensor(-8.5848, device='cuda:0') tensor(4.2496e-08, device='cuda:0') tensor(1.8686e-07, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 2.063210
Average total loss: 4.365795
tensor(-8.6105, device='cuda:0') tensor(4.2713e-08, device='cuda:0') tensor(1.8212e-07, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 2.010921
Average total loss: 4.313506
tensor(-8.6361, device='cuda:0') tensor(4.2922e-08, device='cuda:0') tensor(1.7751e-07, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 1.960014
Average total loss: 4.262599
tensor(-8.6617, device='cuda:0') tensor(4.3135e-08, device='cuda:0') tensor(1.7302e-07, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 1.910462
Average total loss: 4.213047
tensor(-8.6873, device='cuda:0') tensor(4.3350e-08, device='cuda:0') tensor(1.6865e-07, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 1.862222
Average total loss: 4.164807
tensor(-8.7129, device='cuda:0') tensor(4.3581e-08, device='cuda:0') tensor(1.6440e-07, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 1.815245
Average total loss: 4.117830
tensor(-8.7384, device='cuda:0') tensor(4.3816e-08, device='cuda:0') tensor(1.6025e-07, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 1.769515
Average total loss: 4.072100
tensor(-8.7639, device='cuda:0') tensor(4.4065e-08, device='cuda:0') tensor(1.5622e-07, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 1.724979
Average total loss: 4.027564
tensor(-8.7894, device='cuda:0') tensor(4.4256e-08, device='cuda:0') tensor(1.5229e-07, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 1.681611
Average total loss: 3.984196
tensor(-8.8149, device='cuda:0') tensor(4.4498e-08, device='cuda:0') tensor(1.4846e-07, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 1.639385
Average total loss: 3.941970
tensor(-8.8403, device='cuda:0') tensor(4.4755e-08, device='cuda:0') tensor(1.4474e-07, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 1.598249
Average total loss: 3.900834
tensor(-8.8657, device='cuda:0') tensor(4.5009e-08, device='cuda:0') tensor(1.4111e-07, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 1.558201
Average total loss: 3.860786
tensor(-8.8911, device='cuda:0') tensor(4.5276e-08, device='cuda:0') tensor(1.3757e-07, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 1.519179
Average total loss: 3.821764
tensor(-8.9164, device='cuda:0') tensor(4.5530e-08, device='cuda:0') tensor(1.3413e-07, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 1.481186
Average total loss: 3.783771
tensor(-8.9417, device='cuda:0') tensor(4.5786e-08, device='cuda:0') tensor(1.3078e-07, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 1.444164
Average total loss: 3.746749
tensor(-8.9670, device='cuda:0') tensor(4.6042e-08, device='cuda:0') tensor(1.2751e-07, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 1.408114
Average total loss: 3.710699
tensor(-8.9923, device='cuda:0') tensor(4.6317e-08, device='cuda:0') tensor(1.2433e-07, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 1.372982
Average total loss: 3.675567
tensor(-9.0176, device='cuda:0') tensor(4.6576e-08, device='cuda:0') tensor(1.2123e-07, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 1.338771
Average total loss: 3.641355
tensor(-9.0428, device='cuda:0') tensor(4.6883e-08, device='cuda:0') tensor(1.1821e-07, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 1.305425
Average total loss: 3.608010
tensor(-9.0680, device='cuda:0') tensor(4.7202e-08, device='cuda:0') tensor(1.1527e-07, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 1.272955
Average total loss: 3.575540
tensor(-9.0932, device='cuda:0') tensor(4.7469e-08, device='cuda:0') tensor(1.1240e-07, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 1.241301
Average total loss: 3.543886
tensor(-9.1184, device='cuda:0') tensor(4.7726e-08, device='cuda:0') tensor(1.0961e-07, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 1.210475
Average total loss: 3.513060
tensor(-9.1435, device='cuda:0') tensor(4.7992e-08, device='cuda:0') tensor(1.0689e-07, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 1.180426
Average total loss: 3.483011
tensor(-9.1686, device='cuda:0') tensor(4.8255e-08, device='cuda:0') tensor(1.0424e-07, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 1.151152
Average total loss: 3.453737
tensor(-9.1937, device='cuda:0') tensor(4.8508e-08, device='cuda:0') tensor(1.0165e-07, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 1.122628
Average total loss: 3.425213
tensor(-9.2188, device='cuda:0') tensor(4.8777e-08, device='cuda:0') tensor(9.9135e-08, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 1.094819
Average total loss: 3.397404
tensor(-9.2439, device='cuda:0') tensor(4.9023e-08, device='cuda:0') tensor(9.6682e-08, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 1.067737
Average total loss: 3.370322
tensor(-9.2689, device='cuda:0') tensor(4.9284e-08, device='cuda:0') tensor(9.4291e-08, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302586
Average KL loss: 1.041327
Average total loss: 3.343913
tensor(-9.2940, device='cuda:0') tensor(4.7192e-08, device='cuda:0') tensor(9.1959e-08, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 1.015596
Average total loss: 3.318181
tensor(-9.3190, device='cuda:0') tensor(4.7434e-08, device='cuda:0') tensor(8.9689e-08, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.990523
Average total loss: 3.293108
tensor(-9.3440, device='cuda:0') tensor(4.7709e-08, device='cuda:0') tensor(8.7475e-08, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.966070
Average total loss: 3.268655
tensor(-9.3690, device='cuda:0') tensor(4.7972e-08, device='cuda:0') tensor(8.5316e-08, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.942252
Average total loss: 3.244837
tensor(-9.3939, device='cuda:0') tensor(4.8247e-08, device='cuda:0') tensor(8.3214e-08, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.919032
Average total loss: 3.221617
tensor(-9.4189, device='cuda:0') tensor(4.8532e-08, device='cuda:0') tensor(8.1164e-08, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.896388
Average total loss: 3.198973
tensor(-9.4438, device='cuda:0') tensor(4.8801e-08, device='cuda:0') tensor(7.9165e-08, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302584
Average KL loss: 0.874332
Average total loss: 3.176917
tensor(-9.4687, device='cuda:0') tensor(5.1218e-08, device='cuda:0') tensor(7.7218e-08, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.852826
Average total loss: 3.155411
tensor(-9.4936, device='cuda:0') tensor(5.9250e-08, device='cuda:0') tensor(7.5319e-08, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.831851
Average total loss: 3.134436
tensor(-9.5185, device='cuda:0') tensor(6.0277e-08, device='cuda:0') tensor(7.3468e-08, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.811419
Average total loss: 3.114004
tensor(-9.5434, device='cuda:0') tensor(6.1265e-08, device='cuda:0') tensor(7.1664e-08, device='cuda:0')
Epoch 179
Average batch original loss after noise: 2.302585
Average KL loss: 0.791497
Average total loss: 3.094082
tensor(-9.5682, device='cuda:0') tensor(6.2273e-08, device='cuda:0') tensor(6.9905e-08, device='cuda:0')
Epoch 180
Average batch original loss after noise: 2.302585
Average KL loss: 0.772064
Average total loss: 3.074649
tensor(-9.5931, device='cuda:0') tensor(6.3278e-08, device='cuda:0') tensor(6.8189e-08, device='cuda:0')
Epoch 181
Average batch original loss after noise: 2.302585
Average KL loss: 0.753130
Average total loss: 3.055715
tensor(-9.6179, device='cuda:0') tensor(6.4286e-08, device='cuda:0') tensor(6.6518e-08, device='cuda:0')
Epoch 182
Average batch original loss after noise: 2.302585
Average KL loss: 0.734674
Average total loss: 3.037259
tensor(-9.6427, device='cuda:0') tensor(6.5319e-08, device='cuda:0') tensor(6.4888e-08, device='cuda:0')
Epoch 183
Average batch original loss after noise: 2.302585
Average KL loss: 0.716669
Average total loss: 3.019254
tensor(-9.6675, device='cuda:0') tensor(6.6367e-08, device='cuda:0') tensor(6.3298e-08, device='cuda:0')
Epoch 184
Average batch original loss after noise: 2.302585
Average KL loss: 0.699114
Average total loss: 3.001699
tensor(-9.6923, device='cuda:0') tensor(6.7382e-08, device='cuda:0') tensor(6.1749e-08, device='cuda:0')
Epoch 185
Average batch original loss after noise: 2.302585
Average KL loss: 0.682011
Average total loss: 2.984596
tensor(-9.7171, device='cuda:0') tensor(6.8430e-08, device='cuda:0') tensor(6.0238e-08, device='cuda:0')
Epoch 186
Average batch original loss after noise: 2.302585
Average KL loss: 0.665328
Average total loss: 2.967913
tensor(-9.7418, device='cuda:0') tensor(6.9493e-08, device='cuda:0') tensor(5.8765e-08, device='cuda:0')
Epoch 187
Average batch original loss after noise: 2.302585
Average KL loss: 0.649054
Average total loss: 2.951639
tensor(-9.7666, device='cuda:0') tensor(7.0553e-08, device='cuda:0') tensor(5.7328e-08, device='cuda:0')
Epoch 188
Average batch original loss after noise: 2.302585
Average KL loss: 0.633194
Average total loss: 2.935779
tensor(-9.7913, device='cuda:0') tensor(7.1592e-08, device='cuda:0') tensor(5.5928e-08, device='cuda:0')
Epoch 189
Average batch original loss after noise: 2.302585
Average KL loss: 0.617734
Average total loss: 2.920319
tensor(-9.8161, device='cuda:0') tensor(7.2663e-08, device='cuda:0') tensor(5.4563e-08, device='cuda:0')
Epoch 190
Average batch original loss after noise: 2.302585
Average KL loss: 0.602651
Average total loss: 2.905236
tensor(-9.8408, device='cuda:0') tensor(7.3748e-08, device='cuda:0') tensor(5.3231e-08, device='cuda:0')
Epoch 191
Average batch original loss after noise: 2.302585
Average KL loss: 0.587938
Average total loss: 2.890523
tensor(-9.8655, device='cuda:0') tensor(7.4808e-08, device='cuda:0') tensor(5.1932e-08, device='cuda:0')
Epoch 192
Average batch original loss after noise: 2.302585
Average KL loss: 0.573602
Average total loss: 2.876187
tensor(-9.8902, device='cuda:0') tensor(7.5861e-08, device='cuda:0') tensor(5.0666e-08, device='cuda:0')
Epoch 193
Average batch original loss after noise: 2.302585
Average KL loss: 0.559623
Average total loss: 2.862208
tensor(-9.9148, device='cuda:0') tensor(7.3775e-08, device='cuda:0') tensor(4.9431e-08, device='cuda:0')
Epoch 194
Average batch original loss after noise: 2.302585
Average KL loss: 0.545985
Average total loss: 2.848569
tensor(-9.9395, device='cuda:0') tensor(7.4844e-08, device='cuda:0') tensor(4.8227e-08, device='cuda:0')
Epoch 195
Average batch original loss after noise: 2.302585
Average KL loss: 0.532679
Average total loss: 2.835264
tensor(-9.9642, device='cuda:0') tensor(7.5959e-08, device='cuda:0') tensor(4.7052e-08, device='cuda:0')
Epoch 196
Average batch original loss after noise: 2.302585
Average KL loss: 0.519714
Average total loss: 2.822299
tensor(-9.9888, device='cuda:0') tensor(7.7057e-08, device='cuda:0') tensor(4.5907e-08, device='cuda:0')
Epoch 197
Average batch original loss after noise: 2.302585
Average KL loss: 0.507072
Average total loss: 2.809657
tensor(-10.0134, device='cuda:0') tensor(7.8182e-08, device='cuda:0') tensor(4.4791e-08, device='cuda:0')
Epoch 198
Average batch original loss after noise: 2.302585
Average KL loss: 0.494737
Average total loss: 2.797322
tensor(-10.0380, device='cuda:0') tensor(7.9301e-08, device='cuda:0') tensor(4.3701e-08, device='cuda:0')
Epoch 199
Average batch original loss after noise: 2.302585
Average KL loss: 0.482703
Average total loss: 2.785288
tensor(-10.0627, device='cuda:0') tensor(8.0425e-08, device='cuda:0') tensor(4.2638e-08, device='cuda:0')
Epoch 200
Average batch original loss after noise: 2.302582
Average KL loss: 0.470971
Average total loss: 2.773553
 Percentile value: -10.08725357055664
Non-zero model percentage: 30.000001907348633%, Non-zero mask percentage: 30.000001907348633%

--- Pruning Level [1/7]: ---
conv1.weight         | nonzeros =     303 /    1728             ( 17.53%) | total_pruned =    1425 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       5 /      64             (  7.81%) | total_pruned =      59 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    4542 /   36864             ( 12.32%) | total_pruned =   32322 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    9857 /   36864             ( 26.74%) | total_pruned =   27007 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      15 /      64             ( 23.44%) | total_pruned =      49 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =   10472 /   36864             ( 28.41%) | total_pruned =   26392 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      26 /      64             ( 40.62%) | total_pruned =      38 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =   11254 /   36864             ( 30.53%) | total_pruned =   25610 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      43 /      64             ( 67.19%) | total_pruned =      21 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   28747 /   73728             ( 38.99%) | total_pruned =   44981 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      25 /     128             ( 19.53%) | total_pruned =     103 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      75 /     128             ( 58.59%) | total_pruned =      53 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   55546 /  147456             ( 37.67%) | total_pruned =   91910 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      37 /     128             ( 28.91%) | total_pruned =      91 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    3163 /    8192             ( 38.61%) | total_pruned =    5029 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      85 /     128             ( 66.41%) | total_pruned =      43 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =   31445 /  147456             ( 21.33%) | total_pruned =  116011 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =   25770 /  147456             ( 17.48%) | total_pruned =  121686 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      57 /     128             ( 44.53%) | total_pruned =      71 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =  116029 /  294912             ( 39.34%) | total_pruned =  178883 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      81 /     256             ( 31.64%) | total_pruned =     175 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     169 /     256             ( 66.02%) | total_pruned =      87 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =  162597 /  589824             ( 27.57%) | total_pruned =  427227 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     113 /     256             ( 44.14%) | total_pruned =     143 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    9385 /   32768             ( 28.64%) | total_pruned =   23383 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     119 /     256             ( 46.48%) | total_pruned =     137 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     152 /     256             ( 59.38%) | total_pruned =     104 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =   66769 /  589824             ( 11.32%) | total_pruned =  523055 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     109 /     256             ( 42.58%) | total_pruned =     147 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     124 /     256             ( 48.44%) | total_pruned =     132 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =   48522 /  589824             (  8.23%) | total_pruned =  541302 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     135 /     256             ( 52.73%) | total_pruned =     121 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     150 /     256             ( 58.59%) | total_pruned =     106 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =  224662 / 1179648             ( 19.04%) | total_pruned =  954986 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     215 /     512             ( 41.99%) | total_pruned =     297 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     297 /     512             ( 58.01%) | total_pruned =     215 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =  383335 / 2359296             ( 16.25%) | total_pruned = 1975961 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     205 /     512             ( 40.04%) | total_pruned =     307 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     328 /     512             ( 64.06%) | total_pruned =     184 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =   22943 /  131072             ( 17.50%) | total_pruned =  108129 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     198 /     512             ( 38.67%) | total_pruned =     314 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     336 /     512             ( 65.62%) | total_pruned =     176 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  402286 / 2359296             ( 17.05%) | total_pruned = 1957010 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     264 /     512             ( 51.56%) | total_pruned =     248 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros = 1727644 / 2359296             ( 73.23%) | total_pruned =  631652 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =     512 /     512             (100.00%) | total_pruned =       0 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
linear.weight        | nonzeros =    2976 /    5120             ( 58.12%) | total_pruned =    2144 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 3353629, pruned : 7825133, total: 11178762, Compression rate :       3.33x  ( 70.00% pruned)
Train Epoch: 63/200 Loss: 0.023368 Accuracy: 83.26 100.00 % Best test Accuracy: 83.52%
tensor(-10.0873, device='cuda:0') tensor(8.1504e-08, device='cuda:0') tensor(4.1603e-08, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.388192
Average total loss: 2.690777
tensor(-10.4575, device='cuda:0') tensor(7.1127e-08, device='cuda:0') tensor(2.8731e-08, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.276465
Average total loss: 2.579050
tensor(-10.7610, device='cuda:0') tensor(6.3653e-08, device='cuda:0') tensor(2.1210e-08, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.209429
Average total loss: 2.512013
tensor(-11.0125, device='cuda:0') tensor(5.8201e-08, device='cuda:0') tensor(1.6494e-08, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.165886
Average total loss: 2.468471
tensor(-11.2269, device='cuda:0') tensor(5.6379e-08, device='cuda:0') tensor(1.3311e-08, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.135726
Average total loss: 2.438311
tensor(-11.4136, device='cuda:0') tensor(5.5288e-08, device='cuda:0') tensor(1.1044e-08, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.113815
Average total loss: 2.416400
tensor(-11.5788, device='cuda:0') tensor(5.4461e-08, device='cuda:0') tensor(9.3623e-09, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.097300
Average total loss: 2.399885
tensor(-11.7269, device='cuda:0') tensor(5.3759e-08, device='cuda:0') tensor(8.0732e-09, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.084482
Average total loss: 2.387067
tensor(-11.8612, device='cuda:0') tensor(5.3076e-08, device='cuda:0') tensor(7.0593e-09, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.074295
Average total loss: 2.376879
tensor(-11.9838, device='cuda:0') tensor(5.2377e-08, device='cuda:0') tensor(6.2444e-09, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.066036
Average total loss: 2.368621
tensor(-12.0967, device='cuda:0') tensor(5.1653e-08, device='cuda:0') tensor(5.5777e-09, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.059228
Average total loss: 2.361813
tensor(-12.2013, device='cuda:0') tensor(5.0895e-08, device='cuda:0') tensor(5.0239e-09, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.053536
Average total loss: 2.356121
tensor(-12.2987, device='cuda:0') tensor(5.0112e-08, device='cuda:0') tensor(4.5576e-09, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.048718
Average total loss: 2.351303
tensor(-12.3898, device='cuda:0') tensor(4.9315e-08, device='cuda:0') tensor(4.1607e-09, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.044595
Average total loss: 2.347180
tensor(-12.4755, device='cuda:0') tensor(4.8512e-08, device='cuda:0') tensor(3.8192e-09, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.041035
Average total loss: 2.343620
tensor(-12.5562, device='cuda:0') tensor(4.7695e-08, device='cuda:0') tensor(3.5230e-09, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.037934
Average total loss: 2.340518
tensor(-12.6325, device='cuda:0') tensor(4.6883e-08, device='cuda:0') tensor(3.2640e-09, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.035212
Average total loss: 2.337797
tensor(-12.7050, device='cuda:0') tensor(4.6073e-08, device='cuda:0') tensor(3.0359e-09, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.032808
Average total loss: 2.335393
tensor(-12.7739, device='cuda:0') tensor(4.5273e-08, device='cuda:0') tensor(2.8336e-09, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.030671
Average total loss: 2.333256
tensor(-12.8397, device='cuda:0') tensor(4.4478e-08, device='cuda:0') tensor(2.6534e-09, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.028761
Average total loss: 2.331346
tensor(-12.9025, device='cuda:0') tensor(4.3688e-08, device='cuda:0') tensor(2.4919e-09, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.027046
Average total loss: 2.329631
tensor(-12.9626, device='cuda:0') tensor(4.2911e-08, device='cuda:0') tensor(2.3464e-09, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.025498
Average total loss: 2.328083
tensor(-13.0203, device='cuda:0') tensor(4.2145e-08, device='cuda:0') tensor(2.2149e-09, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.024096
Average total loss: 2.326681
tensor(-13.0757, device='cuda:0') tensor(4.1384e-08, device='cuda:0') tensor(2.0955e-09, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.022820
Average total loss: 2.325405
tensor(-13.1290, device='cuda:0') tensor(4.0642e-08, device='cuda:0') tensor(1.9867e-09, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.021656
Average total loss: 2.324241
tensor(-13.1804, device='cuda:0') tensor(3.9912e-08, device='cuda:0') tensor(1.8872e-09, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.020589
Average total loss: 2.323174
tensor(-13.2300, device='cuda:0') tensor(3.9197e-08, device='cuda:0') tensor(1.7959e-09, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.019609
Average total loss: 2.322194
tensor(-13.2779, device='cuda:0') tensor(3.8491e-08, device='cuda:0') tensor(1.7119e-09, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.018706
Average total loss: 2.321291
tensor(-13.3243, device='cuda:0') tensor(3.7800e-08, device='cuda:0') tensor(1.6343e-09, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.017872
Average total loss: 2.320457
tensor(-13.3691, device='cuda:0') tensor(3.7121e-08, device='cuda:0') tensor(1.5626e-09, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.017099
Average total loss: 2.319684
tensor(-13.4126, device='cuda:0') tensor(3.6455e-08, device='cuda:0') tensor(1.4961e-09, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.016382
Average total loss: 2.318967
tensor(-13.4548, device='cuda:0') tensor(3.5801e-08, device='cuda:0') tensor(1.4343e-09, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.015715
Average total loss: 2.318299
tensor(-13.4958, device='cuda:0') tensor(3.5161e-08, device='cuda:0') tensor(1.3767e-09, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.015092
Average total loss: 2.317677
tensor(-13.5356, device='cuda:0') tensor(3.4527e-08, device='cuda:0') tensor(1.3230e-09, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.014511
Average total loss: 2.317096
tensor(-13.5744, device='cuda:0') tensor(3.3908e-08, device='cuda:0') tensor(1.2727e-09, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.013967
Average total loss: 2.316551
tensor(-13.6121, device='cuda:0') tensor(3.3301e-08, device='cuda:0') tensor(1.2256e-09, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.013456
Average total loss: 2.316041
tensor(-13.6488, device='cuda:0') tensor(3.2707e-08, device='cuda:0') tensor(1.1814e-09, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.012977
Average total loss: 2.315562
tensor(-13.6846, device='cuda:0') tensor(3.2120e-08, device='cuda:0') tensor(1.1399e-09, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.012526
Average total loss: 2.315111
tensor(-13.7195, device='cuda:0') tensor(3.1543e-08, device='cuda:0') tensor(1.1008e-09, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.012102
Average total loss: 2.314687
tensor(-13.7535, device='cuda:0') tensor(3.0980e-08, device='cuda:0') tensor(1.0639e-09, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.011701
Average total loss: 2.314286
tensor(-13.7868, device='cuda:0') tensor(3.0434e-08, device='cuda:0') tensor(1.0291e-09, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.011323
Average total loss: 2.313908
tensor(-13.8193, device='cuda:0') tensor(2.9890e-08, device='cuda:0') tensor(9.9623e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.010965
Average total loss: 2.313549
tensor(-13.8510, device='cuda:0') tensor(2.9356e-08, device='cuda:0') tensor(9.6510e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.010626
Average total loss: 2.313210
tensor(-13.8821, device='cuda:0') tensor(2.8833e-08, device='cuda:0') tensor(9.3558e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.010304
Average total loss: 2.312889
tensor(-13.9125, device='cuda:0') tensor(2.8320e-08, device='cuda:0') tensor(9.0759e-10, device='cuda:0')
Epoch 45
Average batch original loss after noise: 2.302585
Average KL loss: 0.009999
Average total loss: 2.312584
tensor(-13.9422, device='cuda:0') tensor(2.7816e-08, device='cuda:0') tensor(8.8099e-10, device='cuda:0')
Epoch 46
Average batch original loss after noise: 2.302585
Average KL loss: 0.009709
Average total loss: 2.312294
tensor(-13.9713, device='cuda:0') tensor(2.7322e-08, device='cuda:0') tensor(8.5570e-10, device='cuda:0')
Epoch 47
Average batch original loss after noise: 2.302585
Average KL loss: 0.009433
Average total loss: 2.312018
tensor(-13.9999, device='cuda:0') tensor(2.6836e-08, device='cuda:0') tensor(8.3164e-10, device='cuda:0')
Epoch 48
Average batch original loss after noise: 2.302585
Average KL loss: 0.009170
Average total loss: 2.311755
tensor(-14.0278, device='cuda:0') tensor(2.6359e-08, device='cuda:0') tensor(8.0871e-10, device='cuda:0')
Epoch 49
Average batch original loss after noise: 2.302585
Average KL loss: 0.008920
Average total loss: 2.311505
tensor(-14.0552, device='cuda:0') tensor(2.5892e-08, device='cuda:0') tensor(7.8684e-10, device='cuda:0')
Epoch 50
Average batch original loss after noise: 2.302585
Average KL loss: 0.008681
Average total loss: 2.311266
tensor(-14.0821, device='cuda:0') tensor(2.5434e-08, device='cuda:0') tensor(7.6597e-10, device='cuda:0')
Epoch 51
Average batch original loss after noise: 2.302585
Average KL loss: 0.008453
Average total loss: 2.311037
tensor(-14.1085, device='cuda:0') tensor(2.4982e-08, device='cuda:0') tensor(7.4603e-10, device='cuda:0')
Epoch 52
Average batch original loss after noise: 2.302585
Average KL loss: 0.008235
Average total loss: 2.310819
tensor(-14.1344, device='cuda:0') tensor(2.4540e-08, device='cuda:0') tensor(7.2697e-10, device='cuda:0')
Epoch 53
Average batch original loss after noise: 2.302585
Average KL loss: 0.008026
Average total loss: 2.310611
tensor(-14.1598, device='cuda:0') tensor(2.4106e-08, device='cuda:0') tensor(7.0872e-10, device='cuda:0')
Epoch 54
Average batch original loss after noise: 2.302585
Average KL loss: 0.007826
Average total loss: 2.310411
tensor(-14.1848, device='cuda:0') tensor(2.3680e-08, device='cuda:0') tensor(6.9126e-10, device='cuda:0')
Epoch 55
Average batch original loss after noise: 2.302585
Average KL loss: 0.007635
Average total loss: 2.310220
tensor(-14.2093, device='cuda:0') tensor(2.3262e-08, device='cuda:0') tensor(6.7452e-10, device='cuda:0')
Epoch 56
Average batch original loss after noise: 2.302585
Average KL loss: 0.007452
Average total loss: 2.310037
tensor(-14.2333, device='cuda:0') tensor(2.2851e-08, device='cuda:0') tensor(6.5847e-10, device='cuda:0')
Epoch 57
Average batch original loss after noise: 2.302585
Average KL loss: 0.007276
Average total loss: 2.309861
tensor(-14.2570, device='cuda:0') tensor(2.2448e-08, device='cuda:0') tensor(6.4306e-10, device='cuda:0')
Epoch 58
Average batch original loss after noise: 2.302585
Average KL loss: 0.007107
Average total loss: 2.309692
tensor(-14.2803, device='cuda:0') tensor(2.2051e-08, device='cuda:0') tensor(6.2827e-10, device='cuda:0')
Epoch 59
Average batch original loss after noise: 2.302585
Average KL loss: 0.006945
Average total loss: 2.309530
tensor(-14.3032, device='cuda:0') tensor(2.1663e-08, device='cuda:0') tensor(6.1405e-10, device='cuda:0')
Epoch 60
Average batch original loss after noise: 2.302585
Average KL loss: 0.006789
Average total loss: 2.309374
tensor(-14.3257, device='cuda:0') tensor(2.1280e-08, device='cuda:0') tensor(6.0038e-10, device='cuda:0')
Epoch 61
Average batch original loss after noise: 2.302585
Average KL loss: 0.006639
Average total loss: 2.309224
tensor(-14.3478, device='cuda:0') tensor(2.0904e-08, device='cuda:0') tensor(5.8723e-10, device='cuda:0')
Epoch 62
Average batch original loss after noise: 2.302585
Average KL loss: 0.006495
Average total loss: 2.309080
tensor(-14.3696, device='cuda:0') tensor(2.0537e-08, device='cuda:0') tensor(5.7457e-10, device='cuda:0')
Epoch 63
Average batch original loss after noise: 2.302585
Average KL loss: 0.006356
Average total loss: 2.308941
tensor(-14.3911, device='cuda:0') tensor(2.0176e-08, device='cuda:0') tensor(5.6238e-10, device='cuda:0')
Epoch 64
Average batch original loss after noise: 2.302585
Average KL loss: 0.006222
Average total loss: 2.308807
tensor(-14.4122, device='cuda:0') tensor(1.9820e-08, device='cuda:0') tensor(5.5063e-10, device='cuda:0')
Epoch 65
Average batch original loss after noise: 2.302585
Average KL loss: 0.006093
Average total loss: 2.308678
tensor(-14.4330, device='cuda:0') tensor(1.9472e-08, device='cuda:0') tensor(5.3929e-10, device='cuda:0')
Epoch 66
Average batch original loss after noise: 2.302585
Average KL loss: 0.005968
Average total loss: 2.308553
tensor(-14.4535, device='cuda:0') tensor(1.9129e-08, device='cuda:0') tensor(5.2836e-10, device='cuda:0')
Epoch 67
Average batch original loss after noise: 2.302585
Average KL loss: 0.005848
Average total loss: 2.308433
tensor(-14.4737, device='cuda:0') tensor(1.8793e-08, device='cuda:0') tensor(5.1781e-10, device='cuda:0')
Epoch 68
Average batch original loss after noise: 2.302585
Average KL loss: 0.005732
Average total loss: 2.308317
tensor(-14.4935, device='cuda:0') tensor(1.8463e-08, device='cuda:0') tensor(5.0762e-10, device='cuda:0')
Epoch 69
Average batch original loss after noise: 2.302585
Average KL loss: 0.005620
Average total loss: 2.308205
tensor(-14.5131, device='cuda:0') tensor(1.8138e-08, device='cuda:0') tensor(4.9777e-10, device='cuda:0')
Epoch 70
Average batch original loss after noise: 2.302585
Average KL loss: 0.005512
Average total loss: 2.308097
tensor(-14.5324, device='cuda:0') tensor(1.7820e-08, device='cuda:0') tensor(4.8825e-10, device='cuda:0')
Epoch 71
Average batch original loss after noise: 2.302585
Average KL loss: 0.005407
Average total loss: 2.307992
tensor(-14.5515, device='cuda:0') tensor(1.7507e-08, device='cuda:0') tensor(4.7905e-10, device='cuda:0')
Epoch 72
Average batch original loss after noise: 2.302585
Average KL loss: 0.005306
Average total loss: 2.307891
tensor(-14.5702, device='cuda:0') tensor(1.7201e-08, device='cuda:0') tensor(4.7014e-10, device='cuda:0')
Epoch 73
Average batch original loss after noise: 2.302585
Average KL loss: 0.005208
Average total loss: 2.307793
tensor(-14.5887, device='cuda:0') tensor(1.6900e-08, device='cuda:0') tensor(4.6152e-10, device='cuda:0')
Epoch 74
Average batch original loss after noise: 2.302585
Average KL loss: 0.005113
Average total loss: 2.307698
tensor(-14.6070, device='cuda:0') tensor(1.6604e-08, device='cuda:0') tensor(4.5317e-10, device='cuda:0')
Epoch 75
Average batch original loss after noise: 2.302585
Average KL loss: 0.005021
Average total loss: 2.307606
tensor(-14.6250, device='cuda:0') tensor(1.6314e-08, device='cuda:0') tensor(4.4509e-10, device='cuda:0')
Epoch 76
Average batch original loss after noise: 2.302585
Average KL loss: 0.004932
Average total loss: 2.307517
tensor(-14.6428, device='cuda:0') tensor(1.6030e-08, device='cuda:0') tensor(4.3725e-10, device='cuda:0')
Epoch 77
Average batch original loss after noise: 2.302585
Average KL loss: 0.004846
Average total loss: 2.307431
tensor(-14.6603, device='cuda:0') tensor(1.5750e-08, device='cuda:0') tensor(4.2965e-10, device='cuda:0')
Epoch 78
Average batch original loss after noise: 2.302585
Average KL loss: 0.004762
Average total loss: 2.307347
tensor(-14.6776, device='cuda:0') tensor(1.5475e-08, device='cuda:0') tensor(4.2228e-10, device='cuda:0')
Epoch 79
Average batch original loss after noise: 2.302585
Average KL loss: 0.004681
Average total loss: 2.307266
tensor(-14.6946, device='cuda:0') tensor(1.5205e-08, device='cuda:0') tensor(4.1514e-10, device='cuda:0')
Epoch 80
Average batch original loss after noise: 2.302585
Average KL loss: 0.004603
Average total loss: 2.307187
tensor(-14.7115, device='cuda:0') tensor(1.4942e-08, device='cuda:0') tensor(4.0820e-10, device='cuda:0')
Epoch 81
Average batch original loss after noise: 2.302585
Average KL loss: 0.004526
Average total loss: 2.307111
tensor(-14.7281, device='cuda:0') tensor(1.4682e-08, device='cuda:0') tensor(4.0146e-10, device='cuda:0')
Epoch 82
Average batch original loss after noise: 2.302585
Average KL loss: 0.004452
Average total loss: 2.307037
tensor(-14.7446, device='cuda:0') tensor(1.4427e-08, device='cuda:0') tensor(3.9492e-10, device='cuda:0')
Epoch 83
Average batch original loss after noise: 2.302585
Average KL loss: 0.004380
Average total loss: 2.306965
tensor(-14.7608, device='cuda:0') tensor(1.4177e-08, device='cuda:0') tensor(3.8857e-10, device='cuda:0')
Epoch 84
Average batch original loss after noise: 2.302585
Average KL loss: 0.004310
Average total loss: 2.306895
tensor(-14.7768, device='cuda:0') tensor(1.3934e-08, device='cuda:0') tensor(3.8239e-10, device='cuda:0')
Epoch 85
Average batch original loss after noise: 2.302585
Average KL loss: 0.004242
Average total loss: 2.306827
tensor(-14.7926, device='cuda:0') tensor(1.3693e-08, device='cuda:0') tensor(3.7639e-10, device='cuda:0')
Epoch 86
Average batch original loss after noise: 2.302585
Average KL loss: 0.004175
Average total loss: 2.306760
tensor(-14.8083, device='cuda:0') tensor(1.3459e-08, device='cuda:0') tensor(3.7055e-10, device='cuda:0')
Epoch 87
Average batch original loss after noise: 2.302585
Average KL loss: 0.004111
Average total loss: 2.306696
tensor(-14.8237, device='cuda:0') tensor(1.3227e-08, device='cuda:0') tensor(3.6487e-10, device='cuda:0')
Epoch 88
Average batch original loss after noise: 2.302585
Average KL loss: 0.004048
Average total loss: 2.306633
tensor(-14.8390, device='cuda:0') tensor(1.2999e-08, device='cuda:0') tensor(3.5934e-10, device='cuda:0')
Epoch 89
Average batch original loss after noise: 2.302585
Average KL loss: 0.003987
Average total loss: 2.306572
tensor(-14.8541, device='cuda:0') tensor(1.2776e-08, device='cuda:0') tensor(3.5397e-10, device='cuda:0')
Epoch 90
Average batch original loss after noise: 2.302585
Average KL loss: 0.003928
Average total loss: 2.306513
tensor(-14.8690, device='cuda:0') tensor(1.2557e-08, device='cuda:0') tensor(3.4873e-10, device='cuda:0')
Epoch 91
Average batch original loss after noise: 2.302585
Average KL loss: 0.003870
Average total loss: 2.306455
tensor(-14.8837, device='cuda:0') tensor(1.2342e-08, device='cuda:0') tensor(3.4363e-10, device='cuda:0')
Epoch 92
Average batch original loss after noise: 2.302585
Average KL loss: 0.003814
Average total loss: 2.306399
tensor(-14.8983, device='cuda:0') tensor(1.2132e-08, device='cuda:0') tensor(3.3866e-10, device='cuda:0')
Epoch 93
Average batch original loss after noise: 2.302585
Average KL loss: 0.003759
Average total loss: 2.306344
tensor(-14.9127, device='cuda:0') tensor(1.1926e-08, device='cuda:0') tensor(3.3382e-10, device='cuda:0')
Epoch 94
Average batch original loss after noise: 2.302585
Average KL loss: 0.003706
Average total loss: 2.306291
tensor(-14.9269, device='cuda:0') tensor(1.1722e-08, device='cuda:0') tensor(3.2910e-10, device='cuda:0')
Epoch 95
Average batch original loss after noise: 2.302585
Average KL loss: 0.003654
Average total loss: 2.306239
tensor(-14.9410, device='cuda:0') tensor(1.1525e-08, device='cuda:0') tensor(3.2450e-10, device='cuda:0')
Epoch 96
Average batch original loss after noise: 2.302585
Average KL loss: 0.003603
Average total loss: 2.306188
tensor(-14.9549, device='cuda:0') tensor(1.1329e-08, device='cuda:0') tensor(3.2002e-10, device='cuda:0')
Epoch 97
Average batch original loss after noise: 2.302585
Average KL loss: 0.003553
Average total loss: 2.306138
tensor(-14.9687, device='cuda:0') tensor(1.1138e-08, device='cuda:0') tensor(3.1564e-10, device='cuda:0')
Epoch 98
Average batch original loss after noise: 2.302585
Average KL loss: 0.003505
Average total loss: 2.306090
tensor(-14.9823, device='cuda:0') tensor(1.0951e-08, device='cuda:0') tensor(3.1137e-10, device='cuda:0')
Epoch 99
Average batch original loss after noise: 2.302585
Average KL loss: 0.003458
Average total loss: 2.306043
tensor(-14.9957, device='cuda:0') tensor(1.0766e-08, device='cuda:0') tensor(3.0721e-10, device='cuda:0')
Epoch 100
Average batch original loss after noise: 2.302585
Average KL loss: 0.003412
Average total loss: 2.305997
tensor(-15.0091, device='cuda:0') tensor(1.0586e-08, device='cuda:0') tensor(3.0314e-10, device='cuda:0')
Epoch 101
Average batch original loss after noise: 2.302585
Average KL loss: 0.003367
Average total loss: 2.305952
tensor(-15.0222, device='cuda:0') tensor(1.0407e-08, device='cuda:0') tensor(2.9917e-10, device='cuda:0')
Epoch 102
Average batch original loss after noise: 2.302585
Average KL loss: 0.003323
Average total loss: 2.305908
tensor(-15.0353, device='cuda:0') tensor(1.0233e-08, device='cuda:0') tensor(2.9530e-10, device='cuda:0')
Epoch 103
Average batch original loss after noise: 2.302585
Average KL loss: 0.003280
Average total loss: 2.305865
tensor(-15.0482, device='cuda:0') tensor(1.0062e-08, device='cuda:0') tensor(2.9151e-10, device='cuda:0')
Epoch 104
Average batch original loss after noise: 2.302585
Average KL loss: 0.003238
Average total loss: 2.305823
tensor(-15.0610, device='cuda:0') tensor(9.8944e-09, device='cuda:0') tensor(2.8781e-10, device='cuda:0')
Epoch 105
Average batch original loss after noise: 2.302585
Average KL loss: 0.003198
Average total loss: 2.305782
tensor(-15.0736, device='cuda:0') tensor(9.7295e-09, device='cuda:0') tensor(2.8420e-10, device='cuda:0')
Epoch 106
Average batch original loss after noise: 2.302585
Average KL loss: 0.003158
Average total loss: 2.305742
tensor(-15.0861, device='cuda:0') tensor(9.5681e-09, device='cuda:0') tensor(2.8066e-10, device='cuda:0')
Epoch 107
Average batch original loss after noise: 2.302585
Average KL loss: 0.003118
Average total loss: 2.305703
tensor(-15.0985, device='cuda:0') tensor(9.4110e-09, device='cuda:0') tensor(2.7721e-10, device='cuda:0')
Epoch 108
Average batch original loss after noise: 2.302585
Average KL loss: 0.003080
Average total loss: 2.305665
tensor(-15.1107, device='cuda:0') tensor(9.2551e-09, device='cuda:0') tensor(2.7383e-10, device='cuda:0')
Epoch 109
Average batch original loss after noise: 2.302585
Average KL loss: 0.003043
Average total loss: 2.305628
tensor(-15.1229, device='cuda:0') tensor(9.1024e-09, device='cuda:0') tensor(2.7053e-10, device='cuda:0')
Epoch 110
Average batch original loss after noise: 2.302585
Average KL loss: 0.003006
Average total loss: 2.305591
tensor(-15.1349, device='cuda:0') tensor(8.9538e-09, device='cuda:0') tensor(2.6730e-10, device='cuda:0')
Epoch 111
Average batch original loss after noise: 2.302585
Average KL loss: 0.002971
Average total loss: 2.305556
tensor(-15.1468, device='cuda:0') tensor(8.8063e-09, device='cuda:0') tensor(2.6414e-10, device='cuda:0')
Epoch 112
Average batch original loss after noise: 2.302585
Average KL loss: 0.002936
Average total loss: 2.305521
tensor(-15.1586, device='cuda:0') tensor(8.6635e-09, device='cuda:0') tensor(2.6105e-10, device='cuda:0')
Epoch 113
Average batch original loss after noise: 2.302585
Average KL loss: 0.002902
Average total loss: 2.305486
tensor(-15.1702, device='cuda:0') tensor(8.5229e-09, device='cuda:0') tensor(2.5802e-10, device='cuda:0')
Epoch 114
Average batch original loss after noise: 2.302585
Average KL loss: 0.002868
Average total loss: 2.305453
tensor(-15.1818, device='cuda:0') tensor(8.3837e-09, device='cuda:0') tensor(2.5506e-10, device='cuda:0')
Epoch 115
Average batch original loss after noise: 2.302585
Average KL loss: 0.002835
Average total loss: 2.305420
tensor(-15.1932, device='cuda:0') tensor(8.2492e-09, device='cuda:0') tensor(2.5216e-10, device='cuda:0')
Epoch 116
Average batch original loss after noise: 2.302585
Average KL loss: 0.002803
Average total loss: 2.305388
tensor(-15.2045, device='cuda:0') tensor(8.1147e-09, device='cuda:0') tensor(2.4932e-10, device='cuda:0')
Epoch 117
Average batch original loss after noise: 2.302585
Average KL loss: 0.002772
Average total loss: 2.305357
tensor(-15.2158, device='cuda:0') tensor(7.9838e-09, device='cuda:0') tensor(2.4654e-10, device='cuda:0')
Epoch 118
Average batch original loss after noise: 2.302585
Average KL loss: 0.002741
Average total loss: 2.305326
tensor(-15.2269, device='cuda:0') tensor(7.8558e-09, device='cuda:0') tensor(2.4381e-10, device='cuda:0')
Epoch 119
Average batch original loss after noise: 2.302585
Average KL loss: 0.002711
Average total loss: 2.305296
tensor(-15.2379, device='cuda:0') tensor(7.7312e-09, device='cuda:0') tensor(2.4114e-10, device='cuda:0')
Epoch 120
Average batch original loss after noise: 2.302585
Average KL loss: 0.002681
Average total loss: 2.305266
tensor(-15.2488, device='cuda:0') tensor(7.6073e-09, device='cuda:0') tensor(2.3852e-10, device='cuda:0')
Epoch 121
Average batch original loss after noise: 2.302585
Average KL loss: 0.002652
Average total loss: 2.305237
tensor(-15.2596, device='cuda:0') tensor(7.4846e-09, device='cuda:0') tensor(2.3596e-10, device='cuda:0')
Epoch 122
Average batch original loss after noise: 2.302585
Average KL loss: 0.002624
Average total loss: 2.305209
tensor(-15.2703, device='cuda:0') tensor(7.3664e-09, device='cuda:0') tensor(2.3344e-10, device='cuda:0')
Epoch 123
Average batch original loss after noise: 2.302585
Average KL loss: 0.002596
Average total loss: 2.305181
tensor(-15.2809, device='cuda:0') tensor(7.2492e-09, device='cuda:0') tensor(2.3098e-10, device='cuda:0')
Epoch 124
Average batch original loss after noise: 2.302585
Average KL loss: 0.002569
Average total loss: 2.305154
tensor(-15.2914, device='cuda:0') tensor(7.1346e-09, device='cuda:0') tensor(2.2856e-10, device='cuda:0')
Epoch 125
Average batch original loss after noise: 2.302585
Average KL loss: 0.002542
Average total loss: 2.305127
tensor(-15.3019, device='cuda:0') tensor(7.0227e-09, device='cuda:0') tensor(2.2619e-10, device='cuda:0')
Epoch 126
Average batch original loss after noise: 2.302585
Average KL loss: 0.002516
Average total loss: 2.305101
tensor(-15.3122, device='cuda:0') tensor(6.9137e-09, device='cuda:0') tensor(2.2387e-10, device='cuda:0')
Epoch 127
Average batch original loss after noise: 2.302585
Average KL loss: 0.002490
Average total loss: 2.305075
tensor(-15.3224, device='cuda:0') tensor(6.8052e-09, device='cuda:0') tensor(2.2159e-10, device='cuda:0')
Epoch 128
Average batch original loss after noise: 2.302585
Average KL loss: 0.002465
Average total loss: 2.305050
tensor(-15.3326, device='cuda:0') tensor(6.6992e-09, device='cuda:0') tensor(2.1935e-10, device='cuda:0')
Epoch 129
Average batch original loss after noise: 2.302585
Average KL loss: 0.002440
Average total loss: 2.305025
tensor(-15.3427, device='cuda:0') tensor(6.5958e-09, device='cuda:0') tensor(2.1715e-10, device='cuda:0')
Epoch 130
Average batch original loss after noise: 2.302585
Average KL loss: 0.002416
Average total loss: 2.305001
tensor(-15.3526, device='cuda:0') tensor(6.4954e-09, device='cuda:0') tensor(2.1500e-10, device='cuda:0')
Epoch 131
Average batch original loss after noise: 2.302585
Average KL loss: 0.002392
Average total loss: 2.304977
tensor(-15.3625, device='cuda:0') tensor(6.3949e-09, device='cuda:0') tensor(2.1288e-10, device='cuda:0')
Epoch 132
Average batch original loss after noise: 2.302585
Average KL loss: 0.002368
Average total loss: 2.304953
tensor(-15.3723, device='cuda:0') tensor(6.2962e-09, device='cuda:0') tensor(2.1081e-10, device='cuda:0')
Epoch 133
Average batch original loss after noise: 2.302585
Average KL loss: 0.002345
Average total loss: 2.304930
tensor(-15.3820, device='cuda:0') tensor(6.1989e-09, device='cuda:0') tensor(2.0877e-10, device='cuda:0')
Epoch 134
Average batch original loss after noise: 2.302585
Average KL loss: 0.002323
Average total loss: 2.304908
tensor(-15.3917, device='cuda:0') tensor(6.1060e-09, device='cuda:0') tensor(2.0677e-10, device='cuda:0')
Epoch 135
Average batch original loss after noise: 2.302585
Average KL loss: 0.002301
Average total loss: 2.304885
tensor(-15.4012, device='cuda:0') tensor(6.0137e-09, device='cuda:0') tensor(2.0480e-10, device='cuda:0')
Epoch 136
Average batch original loss after noise: 2.302585
Average KL loss: 0.002279
Average total loss: 2.304864
tensor(-15.4107, device='cuda:0') tensor(5.9210e-09, device='cuda:0') tensor(2.0287e-10, device='cuda:0')
Epoch 137
Average batch original loss after noise: 2.302585
Average KL loss: 0.002257
Average total loss: 2.304842
tensor(-15.4201, device='cuda:0') tensor(5.8325e-09, device='cuda:0') tensor(2.0097e-10, device='cuda:0')
Epoch 138
Average batch original loss after noise: 2.302585
Average KL loss: 0.002236
Average total loss: 2.304821
tensor(-15.4294, device='cuda:0') tensor(5.7459e-09, device='cuda:0') tensor(1.9911e-10, device='cuda:0')
Epoch 139
Average batch original loss after noise: 2.302585
Average KL loss: 0.002216
Average total loss: 2.304801
tensor(-15.4387, device='cuda:0') tensor(5.6584e-09, device='cuda:0') tensor(1.9728e-10, device='cuda:0')
Epoch 140
Average batch original loss after noise: 2.302585
Average KL loss: 0.002195
Average total loss: 2.304780
tensor(-15.4478, device='cuda:0') tensor(5.5749e-09, device='cuda:0') tensor(1.9547e-10, device='cuda:0')
Epoch 141
Average batch original loss after noise: 2.302585
Average KL loss: 0.002175
Average total loss: 2.304760
tensor(-15.4569, device='cuda:0') tensor(5.4917e-09, device='cuda:0') tensor(1.9371e-10, device='cuda:0')
Epoch 142
Average batch original loss after noise: 2.302585
Average KL loss: 0.002156
Average total loss: 2.304741
tensor(-15.4659, device='cuda:0') tensor(5.4099e-09, device='cuda:0') tensor(1.9197e-10, device='cuda:0')
Epoch 143
Average batch original loss after noise: 2.302585
Average KL loss: 0.002137
Average total loss: 2.304721
tensor(-15.4749, device='cuda:0') tensor(5.3306e-09, device='cuda:0') tensor(1.9026e-10, device='cuda:0')
Epoch 144
Average batch original loss after noise: 2.302585
Average KL loss: 0.002118
Average total loss: 2.304702
tensor(-15.4838, device='cuda:0') tensor(5.2518e-09, device='cuda:0') tensor(1.8857e-10, device='cuda:0')
Epoch 145
Average batch original loss after noise: 2.302585
Average KL loss: 0.002099
Average total loss: 2.304684
tensor(-15.4926, device='cuda:0') tensor(5.1749e-09, device='cuda:0') tensor(1.8692e-10, device='cuda:0')
Epoch 146
Average batch original loss after noise: 2.302585
Average KL loss: 0.002089
Average total loss: 2.304674
tensor(-15.4935, device='cuda:0') tensor(5.1699e-09, device='cuda:0') tensor(1.8675e-10, device='cuda:0')
Epoch 147
Average batch original loss after noise: 2.302585
Average KL loss: 0.002087
Average total loss: 2.304672
tensor(-15.4944, device='cuda:0') tensor(5.1644e-09, device='cuda:0') tensor(1.8659e-10, device='cuda:0')
Epoch 148
Average batch original loss after noise: 2.302585
Average KL loss: 0.002085
Average total loss: 2.304670
tensor(-15.4952, device='cuda:0') tensor(5.1594e-09, device='cuda:0') tensor(1.8642e-10, device='cuda:0')
Epoch 149
Average batch original loss after noise: 2.302585
Average KL loss: 0.002083
Average total loss: 2.304668
tensor(-15.4961, device='cuda:0') tensor(5.1539e-09, device='cuda:0') tensor(1.8626e-10, device='cuda:0')
Epoch 150
Average batch original loss after noise: 2.302585
Average KL loss: 0.002081
Average total loss: 2.304666
tensor(-15.4970, device='cuda:0') tensor(5.1490e-09, device='cuda:0') tensor(1.8609e-10, device='cuda:0')
Epoch 151
Average batch original loss after noise: 2.302585
Average KL loss: 0.002079
Average total loss: 2.304664
tensor(-15.4979, device='cuda:0') tensor(5.1434e-09, device='cuda:0') tensor(1.8593e-10, device='cuda:0')
Epoch 152
Average batch original loss after noise: 2.302585
Average KL loss: 0.002078
Average total loss: 2.304662
tensor(-15.4988, device='cuda:0') tensor(5.1385e-09, device='cuda:0') tensor(1.8576e-10, device='cuda:0')
Epoch 153
Average batch original loss after noise: 2.302585
Average KL loss: 0.002076
Average total loss: 2.304661
tensor(-15.4997, device='cuda:0') tensor(5.1330e-09, device='cuda:0') tensor(1.8560e-10, device='cuda:0')
Epoch 154
Average batch original loss after noise: 2.302585
Average KL loss: 0.002074
Average total loss: 2.304659
tensor(-15.5006, device='cuda:0') tensor(5.1281e-09, device='cuda:0') tensor(1.8543e-10, device='cuda:0')
Epoch 155
Average batch original loss after noise: 2.302585
Average KL loss: 0.002072
Average total loss: 2.304657
tensor(-15.5015, device='cuda:0') tensor(5.1226e-09, device='cuda:0') tensor(1.8527e-10, device='cuda:0')
Epoch 156
Average batch original loss after noise: 2.302585
Average KL loss: 0.002070
Average total loss: 2.304655
tensor(-15.5023, device='cuda:0') tensor(5.1176e-09, device='cuda:0') tensor(1.8510e-10, device='cuda:0')
Epoch 157
Average batch original loss after noise: 2.302585
Average KL loss: 0.002068
Average total loss: 2.304653
tensor(-15.5032, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8494e-10, device='cuda:0')
Epoch 158
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304652
tensor(-15.5033, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8492e-10, device='cuda:0')
Epoch 159
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304652
tensor(-15.5034, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8491e-10, device='cuda:0')
Epoch 160
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304652
tensor(-15.5035, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8489e-10, device='cuda:0')
Epoch 161
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304652
tensor(-15.5036, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8487e-10, device='cuda:0')
Epoch 162
Average batch original loss after noise: 2.302585
Average KL loss: 0.002067
Average total loss: 2.304651
tensor(-15.5037, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8485e-10, device='cuda:0')
Epoch 163
Average batch original loss after noise: 2.302585
Average KL loss: 0.002066
Average total loss: 2.304651
tensor(-15.5038, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8484e-10, device='cuda:0')
Epoch 164
Average batch original loss after noise: 2.302585
Average KL loss: 0.002066
Average total loss: 2.304651
tensor(-15.5039, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8482e-10, device='cuda:0')
Epoch 165
Average batch original loss after noise: 2.302585
Average KL loss: 0.002066
Average total loss: 2.304651
tensor(-15.5040, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8480e-10, device='cuda:0')
Epoch 166
Average batch original loss after noise: 2.302585
Average KL loss: 0.002066
Average total loss: 2.304651
tensor(-15.5041, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8478e-10, device='cuda:0')
Epoch 167
Average batch original loss after noise: 2.302585
Average KL loss: 0.002066
Average total loss: 2.304650
tensor(-15.5042, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8477e-10, device='cuda:0')
Epoch 168
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 169
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 170
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 171
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 172
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 173
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 174
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 175
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 176
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 177
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 178
Average batch original loss after noise: 2.302585
Average KL loss: 0.002065
Average total loss: 2.304650
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
 Percentile value: -15.504262924194336
Non-zero model percentage: 9.000003814697266%, Non-zero mask percentage: 9.000003814697266%

--- Pruning Level [2/7]: ---
conv1.weight         | nonzeros =     285 /    1728             ( 16.49%) | total_pruned =    1443 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       4 /      64             (  6.25%) | total_pruned =      60 | shape = torch.Size([64])
bn1.bias             | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    2136 /   36864             (  5.79%) | total_pruned =   34728 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      25 /      64             ( 39.06%) | total_pruned =      39 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    3801 /   36864             ( 10.31%) | total_pruned =   33063 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    3847 /   36864             ( 10.44%) | total_pruned =   33017 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      20 /      64             ( 31.25%) | total_pruned =      44 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    3817 /   36864             ( 10.35%) | total_pruned =   33047 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      39 /      64             ( 60.94%) | total_pruned =      25 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =   10417 /   73728             ( 14.13%) | total_pruned =   63311 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      25 /     128             ( 19.53%) | total_pruned =     103 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      70 /     128             ( 54.69%) | total_pruned =      58 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =   16863 /  147456             ( 11.44%) | total_pruned =  130593 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      36 /     128             ( 28.12%) | total_pruned =      92 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      84 /     128             ( 65.62%) | total_pruned =      44 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =    1587 /    8192             ( 19.37%) | total_pruned =    6605 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      36 /     128             ( 28.12%) | total_pruned =      92 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      83 /     128             ( 64.84%) | total_pruned =      45 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =    3641 /  147456             (  2.47%) | total_pruned =  143815 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      63 /     128             ( 49.22%) | total_pruned =      65 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      59 /     128             ( 46.09%) | total_pruned =      69 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =    2856 /  147456             (  1.94%) | total_pruned =  144600 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      92 /     128             ( 71.88%) | total_pruned =      36 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   36344 /  294912             ( 12.32%) | total_pruned =  258568 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      81 /     256             ( 31.64%) | total_pruned =     175 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     167 /     256             ( 65.23%) | total_pruned =      89 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   37343 /  589824             (  6.33%) | total_pruned =  552481 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2599 /   32768             (  7.93%) | total_pruned =   30169 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4220 /  589824             (  0.72%) | total_pruned =  585604 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     100 /     256             ( 39.06%) | total_pruned =     156 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3120 /  589824             (  0.53%) | total_pruned =  586704 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   39367 / 1179648             (  3.34%) | total_pruned = 1140281 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     207 /     512             ( 40.43%) | total_pruned =     305 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     291 /     512             ( 56.84%) | total_pruned =     221 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   48836 / 2359296             (  2.07%) | total_pruned = 2310460 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     193 /     512             ( 37.70%) | total_pruned =     319 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     319 /     512             ( 62.30%) | total_pruned =     193 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3872 /  131072             (  2.95%) | total_pruned =  127200 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     155 /     512             ( 30.27%) | total_pruned =     357 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     326 /     512             ( 63.67%) | total_pruned =     186 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =  361278 / 2359296             ( 15.31%) | total_pruned = 1998018 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     217 /     512             ( 42.38%) | total_pruned =     295 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     264 /     512             ( 51.56%) | total_pruned =     248 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =  412223 / 2359296             ( 17.47%) | total_pruned = 1947073 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     227 /     512             ( 44.34%) | total_pruned =     285 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
linear.weight        | nonzeros =    2973 /    5120             ( 58.07%) | total_pruned =    2147 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 1006089, pruned : 10172673, total: 11178762, Compression rate :      11.11x  ( 91.00% pruned)
Train Epoch: 106/200 Loss: 0.091791 Accuracy: 78.54 99.93 % Best test Accuracy: 79.91%
tensor(-15.5043, device='cuda:0') tensor(5.1121e-09, device='cuda:0') tensor(1.8475e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.002056
Average total loss: 2.304641
tensor(-15.5131, device='cuda:0') tensor(5.0197e-09, device='cuda:0') tensor(1.8312e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.002038
Average total loss: 2.304623
tensor(-15.5219, device='cuda:0') tensor(4.9294e-09, device='cuda:0') tensor(1.8152e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.002020
Average total loss: 2.304605
tensor(-15.5306, device='cuda:0') tensor(4.8423e-09, device='cuda:0') tensor(1.7994e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.002003
Average total loss: 2.304588
tensor(-15.5393, device='cuda:0') tensor(4.7555e-09, device='cuda:0') tensor(1.7840e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001986
Average total loss: 2.304571
tensor(-15.5478, device='cuda:0') tensor(4.6737e-09, device='cuda:0') tensor(1.7687e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001969
Average total loss: 2.304554
tensor(-15.5563, device='cuda:0') tensor(4.5911e-09, device='cuda:0') tensor(1.7538e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001952
Average total loss: 2.304537
tensor(-15.5647, device='cuda:0') tensor(4.5127e-09, device='cuda:0') tensor(1.7391e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001936
Average total loss: 2.304521
tensor(-15.5731, device='cuda:0') tensor(4.4340e-09, device='cuda:0') tensor(1.7246e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001920
Average total loss: 2.304505
tensor(-15.5814, device='cuda:0') tensor(4.3600e-09, device='cuda:0') tensor(1.7104e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001904
Average total loss: 2.304489
tensor(-15.5896, device='cuda:0') tensor(4.2854e-09, device='cuda:0') tensor(1.6964e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001889
Average total loss: 2.304474
tensor(-15.5977, device='cuda:0') tensor(4.2152e-09, device='cuda:0') tensor(1.6826e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001874
Average total loss: 2.304458
tensor(-15.6058, device='cuda:0') tensor(4.1444e-09, device='cuda:0') tensor(1.6691e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001865
Average total loss: 2.304450
tensor(-15.6066, device='cuda:0') tensor(4.1383e-09, device='cuda:0') tensor(1.6678e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001864
Average total loss: 2.304449
tensor(-15.6074, device='cuda:0') tensor(4.1314e-09, device='cuda:0') tensor(1.6664e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001862
Average total loss: 2.304447
tensor(-15.6082, device='cuda:0') tensor(4.1254e-09, device='cuda:0') tensor(1.6651e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001861
Average total loss: 2.304446
tensor(-15.6090, device='cuda:0') tensor(4.1185e-09, device='cuda:0') tensor(1.6638e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001859
Average total loss: 2.304444
tensor(-15.6098, device='cuda:0') tensor(4.1129e-09, device='cuda:0') tensor(1.6625e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001858
Average total loss: 2.304443
tensor(-15.6106, device='cuda:0') tensor(4.1063e-09, device='cuda:0') tensor(1.6612e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001856
Average total loss: 2.304441
tensor(-15.6114, device='cuda:0') tensor(4.1007e-09, device='cuda:0') tensor(1.6598e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001855
Average total loss: 2.304440
tensor(-15.6122, device='cuda:0') tensor(4.0940e-09, device='cuda:0') tensor(1.6585e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001853
Average total loss: 2.304438
tensor(-15.6130, device='cuda:0') tensor(4.0886e-09, device='cuda:0') tensor(1.6572e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001852
Average total loss: 2.304437
tensor(-15.6138, device='cuda:0') tensor(4.0819e-09, device='cuda:0') tensor(1.6559e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001850
Average total loss: 2.304435
tensor(-15.6146, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6546e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001850
Average total loss: 2.304434
tensor(-15.6146, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6544e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001849
Average total loss: 2.304434
tensor(-15.6147, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6543e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001849
Average total loss: 2.304434
tensor(-15.6148, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6541e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001849
Average total loss: 2.304434
tensor(-15.6149, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6540e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001849
Average total loss: 2.304434
tensor(-15.6150, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6538e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001849
Average total loss: 2.304434
tensor(-15.6151, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6536e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6152, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6535e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6153, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6533e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6154, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6532e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6155, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6530e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001848
Average total loss: 2.304433
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
 Percentile value: -15.615577697753906
Non-zero model percentage: 2.7000038623809814%, Non-zero mask percentage: 2.7000038623809814%

--- Pruning Level [3/7]: ---
conv1.weight         | nonzeros =     266 /    1728             ( 15.39%) | total_pruned =    1462 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       4 /      64             (  6.25%) | total_pruned =      60 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =    1232 /   36864             (  3.34%) | total_pruned =   35632 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =    2028 /   36864             (  5.50%) | total_pruned =   34836 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      31 /      64             ( 48.44%) | total_pruned =      33 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =    1922 /   36864             (  5.21%) | total_pruned =   34942 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =      18 /      64             ( 28.12%) | total_pruned =      46 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =    1857 /   36864             (  5.04%) | total_pruned =   35007 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      27 /      64             ( 42.19%) | total_pruned =      37 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      37 /      64             ( 57.81%) | total_pruned =      27 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    4713 /   73728             (  6.39%) | total_pruned =   69015 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      62 /     128             ( 48.44%) | total_pruned =      66 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    6819 /  147456             (  4.62%) | total_pruned =  140637 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      34 /     128             ( 26.56%) | total_pruned =      94 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     968 /    8192             ( 11.82%) | total_pruned =    7224 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      30 /     128             ( 23.44%) | total_pruned =      98 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      79 /     128             ( 61.72%) | total_pruned =      49 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     956 /  147456             (  0.65%) | total_pruned =  146500 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      53 /     128             ( 41.41%) | total_pruned =      75 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      49 /     128             ( 38.28%) | total_pruned =      79 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     792 /  147456             (  0.54%) | total_pruned =  146664 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      89 /     128             ( 69.53%) | total_pruned =      39 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =   14486 /  294912             (  4.91%) | total_pruned =  280426 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      80 /     256             ( 31.25%) | total_pruned =     176 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     166 /     256             ( 64.84%) | total_pruned =      90 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =   35548 /  589824             (  6.03%) | total_pruned =  554276 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =     110 /     256             ( 42.97%) | total_pruned =     146 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =    2599 /   32768             (  7.93%) | total_pruned =   30169 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =    4220 /  589824             (  0.72%) | total_pruned =  585604 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =     100 /     256             ( 39.06%) | total_pruned =     156 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =     117 /     256             ( 45.70%) | total_pruned =     139 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =    3120 /  589824             (  0.53%) | total_pruned =  586704 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =     106 /     256             ( 41.41%) | total_pruned =     150 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     149 /     256             ( 58.20%) | total_pruned =     107 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   39367 / 1179648             (  3.34%) | total_pruned = 1140281 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     207 /     512             ( 40.43%) | total_pruned =     305 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     291 /     512             ( 56.84%) | total_pruned =     221 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   48836 / 2359296             (  2.07%) | total_pruned = 2310460 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     193 /     512             ( 37.70%) | total_pruned =     319 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     319 /     512             ( 62.30%) | total_pruned =     193 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    3872 /  131072             (  2.95%) | total_pruned =  127200 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     155 /     512             ( 30.27%) | total_pruned =     357 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     326 /     512             ( 63.67%) | total_pruned =     186 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   63702 / 2359296             (  2.70%) | total_pruned = 2295594 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     207 /     512             ( 40.43%) | total_pruned =     305 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     250 /     512             ( 48.83%) | total_pruned =     262 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   57003 / 2359296             (  2.42%) | total_pruned = 2302293 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     226 /     512             ( 44.14%) | total_pruned =     286 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
linear.weight        | nonzeros =    2957 /    5120             ( 57.75%) | total_pruned =    2163 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 301827, pruned : 10876935, total: 11178762, Compression rate :      37.04x  ( 97.30% pruned)
Train Epoch: 199/200 Loss: 0.379041 Accuracy: 74.29 93.36 % Best test Accuracy: 76.11%
tensor(-15.6156, device='cuda:0') tensor(4.0765e-09, device='cuda:0') tensor(1.6529e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001841
Average total loss: 2.304425
tensor(-15.6235, device='cuda:0') tensor(4.0079e-09, device='cuda:0') tensor(1.6398e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001826
Average total loss: 2.304411
tensor(-15.6314, device='cuda:0') tensor(3.9422e-09, device='cuda:0') tensor(1.6269e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001812
Average total loss: 2.304397
tensor(-15.6392, device='cuda:0') tensor(3.8801e-09, device='cuda:0') tensor(1.6142e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001798
Average total loss: 2.304383
tensor(-15.6470, device='cuda:0') tensor(3.8191e-09, device='cuda:0') tensor(1.6018e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001784
Average total loss: 2.304369
tensor(-15.6547, device='cuda:0') tensor(3.7573e-09, device='cuda:0') tensor(1.5895e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001770
Average total loss: 2.304355
tensor(-15.6623, device='cuda:0') tensor(3.6993e-09, device='cuda:0') tensor(1.5774e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001757
Average total loss: 2.304342
tensor(-15.6699, device='cuda:0') tensor(3.6421e-09, device='cuda:0') tensor(1.5654e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001744
Average total loss: 2.304328
tensor(-15.6775, device='cuda:0') tensor(3.5867e-09, device='cuda:0') tensor(1.5537e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001730
Average total loss: 2.304315
tensor(-15.6849, device='cuda:0') tensor(3.5318e-09, device='cuda:0') tensor(1.5421e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001718
Average total loss: 2.304303
tensor(-15.6924, device='cuda:0') tensor(3.4777e-09, device='cuda:0') tensor(1.5307e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001705
Average total loss: 2.304290
tensor(-15.6997, device='cuda:0') tensor(3.4253e-09, device='cuda:0') tensor(1.5195e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001693
Average total loss: 2.304277
tensor(-15.7070, device='cuda:0') tensor(3.3738e-09, device='cuda:0') tensor(1.5084e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001686
Average total loss: 2.304270
tensor(-15.7078, device='cuda:0') tensor(3.3704e-09, device='cuda:0') tensor(1.5073e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001684
Average total loss: 2.304269
tensor(-15.7085, device='cuda:0') tensor(3.3672e-09, device='cuda:0') tensor(1.5061e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001683
Average total loss: 2.304268
tensor(-15.7093, device='cuda:0') tensor(3.3643e-09, device='cuda:0') tensor(1.5050e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001682
Average total loss: 2.304267
tensor(-15.7100, device='cuda:0') tensor(3.3614e-09, device='cuda:0') tensor(1.5039e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001681
Average total loss: 2.304265
tensor(-15.7108, device='cuda:0') tensor(3.3584e-09, device='cuda:0') tensor(1.5028e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001679
Average total loss: 2.304264
tensor(-15.7115, device='cuda:0') tensor(3.3552e-09, device='cuda:0') tensor(1.5017e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001678
Average total loss: 2.304263
tensor(-15.7123, device='cuda:0') tensor(3.3459e-09, device='cuda:0') tensor(1.5005e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001677
Average total loss: 2.304262
tensor(-15.7130, device='cuda:0') tensor(3.3265e-09, device='cuda:0') tensor(1.4995e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001676
Average total loss: 2.304261
tensor(-15.7137, device='cuda:0') tensor(3.3091e-09, device='cuda:0') tensor(1.4984e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001674
Average total loss: 2.304259
tensor(-15.7144, device='cuda:0') tensor(3.2946e-09, device='cuda:0') tensor(1.4974e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001673
Average total loss: 2.304258
tensor(-15.7151, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4963e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001673
Average total loss: 2.304258
tensor(-15.7152, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4962e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7153, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4961e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7154, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4959e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7154, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4958e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7155, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4956e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7156, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4955e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304257
tensor(-15.7157, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4954e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001672
Average total loss: 2.304256
tensor(-15.7158, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4952e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7159, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4951e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7160, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4949e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001671
Average total loss: 2.304256
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
 Percentile value: -15.716094970703125
Non-zero model percentage: 0.8100091218948364%, Non-zero mask percentage: 0.8100091218948364%

--- Pruning Level [4/7]: ---
conv1.weight         | nonzeros =     232 /    1728             ( 13.43%) | total_pruned =    1496 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     552 /   36864             (  1.50%) | total_pruned =   36312 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      19 /      64             ( 29.69%) | total_pruned =      45 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     850 /   36864             (  2.31%) | total_pruned =   36014 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      29 /      64             ( 45.31%) | total_pruned =      35 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     739 /   36864             (  2.00%) | total_pruned =   36125 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       9 /      64             ( 14.06%) | total_pruned =      55 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     704 /   36864             (  1.91%) | total_pruned =   36160 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      23 /      64             ( 35.94%) | total_pruned =      41 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      33 /      64             ( 51.56%) | total_pruned =      31 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =    1276 /   73728             (  1.73%) | total_pruned =   72452 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      52 /     128             ( 40.62%) | total_pruned =      76 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =    1838 /  147456             (  1.25%) | total_pruned =  145618 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      29 /     128             ( 22.66%) | total_pruned =      99 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      72 /     128             ( 56.25%) | total_pruned =      56 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     477 /    8192             (  5.82%) | total_pruned =    7715 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      24 /     128             ( 18.75%) | total_pruned =     104 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      68 /     128             ( 53.12%) | total_pruned =      60 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =     187 /  147456             (  0.13%) | total_pruned =  147269 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      35 /     128             ( 27.34%) | total_pruned =      93 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      33 /     128             ( 25.78%) | total_pruned =      95 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =     160 /  147456             (  0.11%) | total_pruned =  147296 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      30 /     128             ( 23.44%) | total_pruned =      98 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      86 /     128             ( 67.19%) | total_pruned =      42 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    3652 /  294912             (  1.24%) | total_pruned =  291260 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      70 /     256             ( 27.34%) | total_pruned =     186 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     158 /     256             ( 61.72%) | total_pruned =      98 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    4542 /  589824             (  0.77%) | total_pruned =  585282 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      91 /     256             ( 35.55%) | total_pruned =     165 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     146 /     256             ( 57.03%) | total_pruned =     110 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     439 /   32768             (  1.34%) | total_pruned =   32329 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      70 /     256             ( 27.34%) | total_pruned =     186 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     133 /     256             ( 51.95%) | total_pruned =     123 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      82 /  589824             (  0.01%) | total_pruned =  589742 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      45 /     256             ( 17.58%) | total_pruned =     211 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      71 /     256             ( 27.73%) | total_pruned =     185 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      88 /  589824             (  0.01%) | total_pruned =  589736 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      41 /     256             ( 16.02%) | total_pruned =     215 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     142 /     256             ( 55.47%) | total_pruned =     114 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =   11746 / 1179648             (  1.00%) | total_pruned = 1167902 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     198 /     512             ( 38.67%) | total_pruned =     314 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     285 /     512             ( 55.66%) | total_pruned =     227 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =   14751 / 2359296             (  0.63%) | total_pruned = 2344545 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     173 /     512             ( 33.79%) | total_pruned =     339 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     301 /     512             ( 58.79%) | total_pruned =     211 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =    1434 /  131072             (  1.09%) | total_pruned =  129638 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =     121 /     512             ( 23.63%) | total_pruned =     391 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     315 /     512             ( 61.52%) | total_pruned =     197 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =   21581 / 2359296             (  0.91%) | total_pruned = 2337715 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     184 /     512             ( 35.94%) | total_pruned =     328 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     229 /     512             ( 44.73%) | total_pruned =     283 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =   18270 / 2359296             (  0.77%) | total_pruned = 2341026 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     219 /     512             ( 42.77%) | total_pruned =     293 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     403 /     512             ( 78.71%) | total_pruned =     109 | shape = torch.Size([512])
linear.weight        | nonzeros =    2933 /    5120             ( 57.29%) | total_pruned =    2187 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 90549, pruned : 11088213, total: 11178762, Compression rate :     123.46x  ( 99.19% pruned)
Train Epoch: 138/200 Loss: 0.831475 Accuracy: 63.99 68.00 % Best test Accuracy: 64.14%
tensor(-15.7161, device='cuda:0') tensor(3.2818e-09, device='cuda:0') tensor(1.4948e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001665
Average total loss: 2.304250
tensor(-15.7233, device='cuda:0') tensor(3.2334e-09, device='cuda:0') tensor(1.4841e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001653
Average total loss: 2.304238
tensor(-15.7304, device='cuda:0') tensor(3.1863e-09, device='cuda:0') tensor(1.4735e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001641
Average total loss: 2.304226
tensor(-15.7375, device='cuda:0') tensor(3.1394e-09, device='cuda:0') tensor(1.4631e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001630
Average total loss: 2.304215
tensor(-15.7446, device='cuda:0') tensor(3.0941e-09, device='cuda:0') tensor(1.4528e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001619
Average total loss: 2.304203
tensor(-15.7516, device='cuda:0') tensor(3.0498e-09, device='cuda:0') tensor(1.4427e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001607
Average total loss: 2.304192
tensor(-15.7585, device='cuda:0') tensor(3.0066e-09, device='cuda:0') tensor(1.4327e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001596
Average total loss: 2.304181
tensor(-15.7654, device='cuda:0') tensor(2.9643e-09, device='cuda:0') tensor(1.4228e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001585
Average total loss: 2.304170
tensor(-15.7723, device='cuda:0') tensor(2.9226e-09, device='cuda:0') tensor(1.4131e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001574
Average total loss: 2.304159
tensor(-15.7791, device='cuda:0') tensor(2.8820e-09, device='cuda:0') tensor(1.4035e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001564
Average total loss: 2.304149
tensor(-15.7859, device='cuda:0') tensor(2.8423e-09, device='cuda:0') tensor(1.3940e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001553
Average total loss: 2.304138
tensor(-15.7926, device='cuda:0') tensor(2.8039e-09, device='cuda:0') tensor(1.3847e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001543
Average total loss: 2.304128
tensor(-15.7993, device='cuda:0') tensor(2.7658e-09, device='cuda:0') tensor(1.3755e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001537
Average total loss: 2.304122
tensor(-15.7999, device='cuda:0') tensor(2.7623e-09, device='cuda:0') tensor(1.3746e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001536
Average total loss: 2.304121
tensor(-15.8006, device='cuda:0') tensor(2.7590e-09, device='cuda:0') tensor(1.3737e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001535
Average total loss: 2.304120
tensor(-15.8012, device='cuda:0') tensor(2.7557e-09, device='cuda:0') tensor(1.3728e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001534
Average total loss: 2.304119
tensor(-15.8019, device='cuda:0') tensor(2.7524e-09, device='cuda:0') tensor(1.3719e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001533
Average total loss: 2.304118
tensor(-15.8026, device='cuda:0') tensor(2.7491e-09, device='cuda:0') tensor(1.3710e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001532
Average total loss: 2.304117
tensor(-15.8032, device='cuda:0') tensor(2.7458e-09, device='cuda:0') tensor(1.3701e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001531
Average total loss: 2.304116
tensor(-15.8039, device='cuda:0') tensor(2.7425e-09, device='cuda:0') tensor(1.3692e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001530
Average total loss: 2.304115
tensor(-15.8045, device='cuda:0') tensor(2.7393e-09, device='cuda:0') tensor(1.3683e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001529
Average total loss: 2.304114
tensor(-15.8052, device='cuda:0') tensor(2.7362e-09, device='cuda:0') tensor(1.3674e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001528
Average total loss: 2.304113
tensor(-15.8058, device='cuda:0') tensor(2.7331e-09, device='cuda:0') tensor(1.3665e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001527
Average total loss: 2.304112
tensor(-15.8065, device='cuda:0') tensor(2.7300e-09, device='cuda:0') tensor(1.3656e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001527
Average total loss: 2.304111
tensor(-15.8065, device='cuda:0') tensor(2.7287e-09, device='cuda:0') tensor(1.3656e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001527
Average total loss: 2.304111
tensor(-15.8066, device='cuda:0') tensor(2.7267e-09, device='cuda:0') tensor(1.3655e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8066, device='cuda:0') tensor(2.7254e-09, device='cuda:0') tensor(1.3654e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8067, device='cuda:0') tensor(2.7233e-09, device='cuda:0') tensor(1.3654e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8067, device='cuda:0') tensor(2.7221e-09, device='cuda:0') tensor(1.3653e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8068, device='cuda:0') tensor(2.7200e-09, device='cuda:0') tensor(1.3652e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8068, device='cuda:0') tensor(2.7187e-09, device='cuda:0') tensor(1.3652e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8069, device='cuda:0') tensor(2.7166e-09, device='cuda:0') tensor(1.3651e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8069, device='cuda:0') tensor(2.7154e-09, device='cuda:0') tensor(1.3651e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8069, device='cuda:0') tensor(2.7133e-09, device='cuda:0') tensor(1.3650e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001526
Average total loss: 2.304111
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
 Percentile value: -15.806974411010742
Non-zero model percentage: 0.2430054396390915%, Non-zero mask percentage: 0.2430054396390915%

--- Pruning Level [5/7]: ---
conv1.weight         | nonzeros =     218 /    1728             ( 12.62%) | total_pruned =    1510 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     394 /   36864             (  1.07%) | total_pruned =   36470 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =      17 /      64             ( 26.56%) | total_pruned =      47 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     556 /   36864             (  1.51%) | total_pruned =   36308 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =      10 /      64             ( 15.62%) | total_pruned =      54 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      28 /      64             ( 43.75%) | total_pruned =      36 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =     436 /   36864             (  1.18%) | total_pruned =   36428 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      14 /      64             ( 21.88%) | total_pruned =      50 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     462 /   36864             (  1.25%) | total_pruned =   36402 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      30 /      64             ( 46.88%) | total_pruned =      34 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =     686 /   73728             (  0.93%) | total_pruned =   73042 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      20 /     128             ( 15.62%) | total_pruned =     108 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      41 /     128             ( 32.03%) | total_pruned =      87 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =     988 /  147456             (  0.67%) | total_pruned =  146468 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      28 /     128             ( 21.88%) | total_pruned =     100 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      66 /     128             ( 51.56%) | total_pruned =      62 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =     321 /    8192             (  3.92%) | total_pruned =    7871 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      23 /     128             ( 17.97%) | total_pruned =     105 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      60 /     128             ( 46.88%) | total_pruned =      68 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      92 /  147456             (  0.06%) | total_pruned =  147364 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      29 /     128             ( 22.66%) | total_pruned =      99 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      28 /     128             ( 21.88%) | total_pruned =     100 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      83 /  147456             (  0.06%) | total_pruned =  147373 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      28 /     128             ( 21.88%) | total_pruned =     100 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      80 /     128             ( 62.50%) | total_pruned =      48 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =    1925 /  294912             (  0.65%) | total_pruned =  292987 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      67 /     256             ( 26.17%) | total_pruned =     189 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     155 /     256             ( 60.55%) | total_pruned =     101 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =    2657 /  589824             (  0.45%) | total_pruned =  587167 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      84 /     256             ( 32.81%) | total_pruned =     172 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     137 /     256             ( 53.52%) | total_pruned =     119 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =     287 /   32768             (  0.88%) | total_pruned =   32481 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =     128 /     256             ( 50.00%) | total_pruned =     128 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =      27 /  589824             (  0.00%) | total_pruned =  589797 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =      30 /     256             ( 11.72%) | total_pruned =     226 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      53 /     256             ( 20.70%) | total_pruned =     203 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =      41 /  589824             (  0.01%) | total_pruned =  589783 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =      30 /     256             ( 11.72%) | total_pruned =     226 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     136 /     256             ( 53.12%) | total_pruned =     120 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =    2737 / 1179648             (  0.23%) | total_pruned = 1176911 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     161 /     512             ( 31.45%) | total_pruned =     351 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     256 /     512             ( 50.00%) | total_pruned =     256 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =    2262 / 2359296             (  0.10%) | total_pruned = 2357034 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =     139 /     512             ( 27.15%) | total_pruned =     373 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     263 /     512             ( 51.37%) | total_pruned =     249 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =     294 /  131072             (  0.22%) | total_pruned =  130778 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      60 /     512             ( 11.72%) | total_pruned =     452 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     278 /     512             ( 54.30%) | total_pruned =     234 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =    3586 / 2359296             (  0.15%) | total_pruned = 2355710 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     158 /     512             ( 30.86%) | total_pruned =     354 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =     168 /     512             ( 32.81%) | total_pruned =     344 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =    2785 / 2359296             (  0.12%) | total_pruned = 2356511 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     188 /     512             ( 36.72%) | total_pruned =     324 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     401 /     512             ( 78.32%) | total_pruned =     111 | shape = torch.Size([512])
linear.weight        | nonzeros =    2812 /    5120             ( 54.92%) | total_pruned =    2308 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 27165, pruned : 11151597, total: 11178762, Compression rate :     411.51x  ( 99.76% pruned)
Train Epoch: 110/200 Loss: 1.293144 Accuracy: 49.31 49.40 % Best test Accuracy: 49.53%
tensor(-15.8070, device='cuda:0') tensor(2.7121e-09, device='cuda:0') tensor(1.3649e-10, device='cuda:0')
Epoch 1
Average batch original loss after noise: 2.302585
Average KL loss: 0.001521
Average total loss: 2.304106
tensor(-15.8136, device='cuda:0') tensor(2.6772e-09, device='cuda:0') tensor(1.3560e-10, device='cuda:0')
Epoch 2
Average batch original loss after noise: 2.302585
Average KL loss: 0.001511
Average total loss: 2.304096
tensor(-15.8201, device='cuda:0') tensor(2.6420e-09, device='cuda:0') tensor(1.3471e-10, device='cuda:0')
Epoch 3
Average batch original loss after noise: 2.302585
Average KL loss: 0.001501
Average total loss: 2.304086
tensor(-15.8266, device='cuda:0') tensor(2.6060e-09, device='cuda:0') tensor(1.3384e-10, device='cuda:0')
Epoch 4
Average batch original loss after noise: 2.302585
Average KL loss: 0.001491
Average total loss: 2.304076
tensor(-15.8331, device='cuda:0') tensor(2.5712e-09, device='cuda:0') tensor(1.3298e-10, device='cuda:0')
Epoch 5
Average batch original loss after noise: 2.302585
Average KL loss: 0.001482
Average total loss: 2.304067
tensor(-15.8395, device='cuda:0') tensor(2.5373e-09, device='cuda:0') tensor(1.3213e-10, device='cuda:0')
Epoch 6
Average batch original loss after noise: 2.302585
Average KL loss: 0.001472
Average total loss: 2.304057
tensor(-15.8459, device='cuda:0') tensor(2.5041e-09, device='cuda:0') tensor(1.3129e-10, device='cuda:0')
Epoch 7
Average batch original loss after noise: 2.302585
Average KL loss: 0.001463
Average total loss: 2.304048
tensor(-15.8522, device='cuda:0') tensor(2.4717e-09, device='cuda:0') tensor(1.3046e-10, device='cuda:0')
Epoch 8
Average batch original loss after noise: 2.302585
Average KL loss: 0.001454
Average total loss: 2.304039
tensor(-15.8585, device='cuda:0') tensor(2.4402e-09, device='cuda:0') tensor(1.2964e-10, device='cuda:0')
Epoch 9
Average batch original loss after noise: 2.302585
Average KL loss: 0.001445
Average total loss: 2.304030
tensor(-15.8648, device='cuda:0') tensor(2.4109e-09, device='cuda:0') tensor(1.2883e-10, device='cuda:0')
Epoch 10
Average batch original loss after noise: 2.302585
Average KL loss: 0.001436
Average total loss: 2.304021
tensor(-15.8710, device='cuda:0') tensor(2.3796e-09, device='cuda:0') tensor(1.2803e-10, device='cuda:0')
Epoch 11
Average batch original loss after noise: 2.302585
Average KL loss: 0.001427
Average total loss: 2.304012
tensor(-15.8772, device='cuda:0') tensor(2.3490e-09, device='cuda:0') tensor(1.2724e-10, device='cuda:0')
Epoch 12
Average batch original loss after noise: 2.302585
Average KL loss: 0.001418
Average total loss: 2.304003
tensor(-15.8833, device='cuda:0') tensor(2.3189e-09, device='cuda:0') tensor(1.2647e-10, device='cuda:0')
Epoch 13
Average batch original loss after noise: 2.302585
Average KL loss: 0.001413
Average total loss: 2.303998
tensor(-15.8839, device='cuda:0') tensor(2.3165e-09, device='cuda:0') tensor(1.2639e-10, device='cuda:0')
Epoch 14
Average batch original loss after noise: 2.302585
Average KL loss: 0.001412
Average total loss: 2.303997
tensor(-15.8845, device='cuda:0') tensor(2.3132e-09, device='cuda:0') tensor(1.2631e-10, device='cuda:0')
Epoch 15
Average batch original loss after noise: 2.302585
Average KL loss: 0.001412
Average total loss: 2.303996
tensor(-15.8851, device='cuda:0') tensor(2.3117e-09, device='cuda:0') tensor(1.2624e-10, device='cuda:0')
Epoch 16
Average batch original loss after noise: 2.302585
Average KL loss: 0.001411
Average total loss: 2.303996
tensor(-15.8857, device='cuda:0') tensor(2.3093e-09, device='cuda:0') tensor(1.2616e-10, device='cuda:0')
Epoch 17
Average batch original loss after noise: 2.302585
Average KL loss: 0.001410
Average total loss: 2.303995
tensor(-15.8863, device='cuda:0') tensor(2.3078e-09, device='cuda:0') tensor(1.2608e-10, device='cuda:0')
Epoch 18
Average batch original loss after noise: 2.302585
Average KL loss: 0.001409
Average total loss: 2.303994
tensor(-15.8869, device='cuda:0') tensor(2.3054e-09, device='cuda:0') tensor(1.2601e-10, device='cuda:0')
Epoch 19
Average batch original loss after noise: 2.302585
Average KL loss: 0.001408
Average total loss: 2.303993
tensor(-15.8875, device='cuda:0') tensor(2.3039e-09, device='cuda:0') tensor(1.2593e-10, device='cuda:0')
Epoch 20
Average batch original loss after noise: 2.302585
Average KL loss: 0.001407
Average total loss: 2.303992
tensor(-15.8882, device='cuda:0') tensor(2.3015e-09, device='cuda:0') tensor(1.2585e-10, device='cuda:0')
Epoch 21
Average batch original loss after noise: 2.302585
Average KL loss: 0.001406
Average total loss: 2.303991
tensor(-15.8888, device='cuda:0') tensor(2.3000e-09, device='cuda:0') tensor(1.2578e-10, device='cuda:0')
Epoch 22
Average batch original loss after noise: 2.302585
Average KL loss: 0.001406
Average total loss: 2.303990
tensor(-15.8894, device='cuda:0') tensor(2.2976e-09, device='cuda:0') tensor(1.2570e-10, device='cuda:0')
Epoch 23
Average batch original loss after noise: 2.302585
Average KL loss: 0.001405
Average total loss: 2.303990
tensor(-15.8900, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2562e-10, device='cuda:0')
Epoch 24
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8900, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2562e-10, device='cuda:0')
Epoch 25
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8901, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2561e-10, device='cuda:0')
Epoch 26
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8901, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2561e-10, device='cuda:0')
Epoch 27
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8902, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2560e-10, device='cuda:0')
Epoch 28
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8902, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2559e-10, device='cuda:0')
Epoch 29
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8903, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2559e-10, device='cuda:0')
Epoch 30
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8903, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2558e-10, device='cuda:0')
Epoch 31
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8904, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2558e-10, device='cuda:0')
Epoch 32
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8904, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2557e-10, device='cuda:0')
Epoch 33
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303989
tensor(-15.8904, device='cuda:0') tensor(2.2961e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 34
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 35
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 36
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 37
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 38
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 39
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 40
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 41
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 42
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 43
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
Epoch 44
Average batch original loss after noise: 2.302585
Average KL loss: 0.001404
Average total loss: 2.303988
tensor(-15.8905, device='cuda:0') tensor(2.2956e-09, device='cuda:0') tensor(1.2556e-10, device='cuda:0')
 Percentile value: -15.890381813049316
Non-zero model percentage: 0.07290609925985336%, Non-zero mask percentage: 0.07290609925985336%

--- Pruning Level [6/7]: ---
conv1.weight         | nonzeros =     148 /    1728             (  8.56%) | total_pruned =    1580 | shape = torch.Size([64, 3, 3, 3])
conv1.bias           | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
bn1.weight           | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
bn1.bias             | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv1.weight | nonzeros =     119 /   36864             (  0.32%) | total_pruned =   36745 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn1.weight  | nonzeros =       7 /      64             ( 10.94%) | total_pruned =      57 | shape = torch.Size([64])
layer1.0.bn1.bias    | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.conv2.weight | nonzeros =     143 /   36864             (  0.39%) | total_pruned =   36721 | shape = torch.Size([64, 64, 3, 3])
layer1.0.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.0.bn2.weight  | nonzeros =       8 /      64             ( 12.50%) | total_pruned =      56 | shape = torch.Size([64])
layer1.0.bn2.bias    | nonzeros =      16 /      64             ( 25.00%) | total_pruned =      48 | shape = torch.Size([64])
layer1.1.conv1.weight | nonzeros =      80 /   36864             (  0.22%) | total_pruned =   36784 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv1.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn1.weight  | nonzeros =      11 /      64             ( 17.19%) | total_pruned =      53 | shape = torch.Size([64])
layer1.1.bn1.bias    | nonzeros =       3 /      64             (  4.69%) | total_pruned =      61 | shape = torch.Size([64])
layer1.1.conv2.weight | nonzeros =     115 /   36864             (  0.31%) | total_pruned =   36749 | shape = torch.Size([64, 64, 3, 3])
layer1.1.conv2.bias  | nonzeros =       0 /      64             (  0.00%) | total_pruned =      64 | shape = torch.Size([64])
layer1.1.bn2.weight  | nonzeros =      13 /      64             ( 20.31%) | total_pruned =      51 | shape = torch.Size([64])
layer1.1.bn2.bias    | nonzeros =      21 /      64             ( 32.81%) | total_pruned =      43 | shape = torch.Size([64])
layer2.0.conv1.weight | nonzeros =      63 /   73728             (  0.09%) | total_pruned =   73665 | shape = torch.Size([128, 64, 3, 3])
layer2.0.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn1.weight  | nonzeros =      17 /     128             ( 13.28%) | total_pruned =     111 | shape = torch.Size([128])
layer2.0.bn1.bias    | nonzeros =      22 /     128             ( 17.19%) | total_pruned =     106 | shape = torch.Size([128])
layer2.0.conv2.weight | nonzeros =      99 /  147456             (  0.07%) | total_pruned =  147357 | shape = torch.Size([128, 128, 3, 3])
layer2.0.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.bn2.weight  | nonzeros =      17 /     128             ( 13.28%) | total_pruned =     111 | shape = torch.Size([128])
layer2.0.bn2.bias    | nonzeros =      44 /     128             ( 34.38%) | total_pruned =      84 | shape = torch.Size([128])
layer2.0.shortcut.0.weight | nonzeros =      99 /    8192             (  1.21%) | total_pruned =    8093 | shape = torch.Size([128, 64, 1, 1])
layer2.0.shortcut.0.bias | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.0.shortcut.1.weight | nonzeros =      18 /     128             ( 14.06%) | total_pruned =     110 | shape = torch.Size([128])
layer2.0.shortcut.1.bias | nonzeros =      38 /     128             ( 29.69%) | total_pruned =      90 | shape = torch.Size([128])
layer2.1.conv1.weight | nonzeros =      14 /  147456             (  0.01%) | total_pruned =  147442 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv1.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn1.weight  | nonzeros =      12 /     128             (  9.38%) | total_pruned =     116 | shape = torch.Size([128])
layer2.1.bn1.bias    | nonzeros =      11 /     128             (  8.59%) | total_pruned =     117 | shape = torch.Size([128])
layer2.1.conv2.weight | nonzeros =      12 /  147456             (  0.01%) | total_pruned =  147444 | shape = torch.Size([128, 128, 3, 3])
layer2.1.conv2.bias  | nonzeros =       0 /     128             (  0.00%) | total_pruned =     128 | shape = torch.Size([128])
layer2.1.bn2.weight  | nonzeros =      13 /     128             ( 10.16%) | total_pruned =     115 | shape = torch.Size([128])
layer2.1.bn2.bias    | nonzeros =      61 /     128             ( 47.66%) | total_pruned =      67 | shape = torch.Size([128])
layer3.0.conv1.weight | nonzeros =     247 /  294912             (  0.08%) | total_pruned =  294665 | shape = torch.Size([256, 128, 3, 3])
layer3.0.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn1.weight  | nonzeros =      53 /     256             ( 20.70%) | total_pruned =     203 | shape = torch.Size([256])
layer3.0.bn1.bias    | nonzeros =     129 /     256             ( 50.39%) | total_pruned =     127 | shape = torch.Size([256])
layer3.0.conv2.weight | nonzeros =     396 /  589824             (  0.07%) | total_pruned =  589428 | shape = torch.Size([256, 256, 3, 3])
layer3.0.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.bn2.weight  | nonzeros =      61 /     256             ( 23.83%) | total_pruned =     195 | shape = torch.Size([256])
layer3.0.bn2.bias    | nonzeros =     112 /     256             ( 43.75%) | total_pruned =     144 | shape = torch.Size([256])
layer3.0.shortcut.0.weight | nonzeros =      73 /   32768             (  0.22%) | total_pruned =   32695 | shape = torch.Size([256, 128, 1, 1])
layer3.0.shortcut.0.bias | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.0.shortcut.1.weight | nonzeros =      29 /     256             ( 11.33%) | total_pruned =     227 | shape = torch.Size([256])
layer3.0.shortcut.1.bias | nonzeros =      98 /     256             ( 38.28%) | total_pruned =     158 | shape = torch.Size([256])
layer3.1.conv1.weight | nonzeros =       1 /  589824             (  0.00%) | total_pruned =  589823 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv1.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn1.weight  | nonzeros =       6 /     256             (  2.34%) | total_pruned =     250 | shape = torch.Size([256])
layer3.1.bn1.bias    | nonzeros =      18 /     256             (  7.03%) | total_pruned =     238 | shape = torch.Size([256])
layer3.1.conv2.weight | nonzeros =       3 /  589824             (  0.00%) | total_pruned =  589821 | shape = torch.Size([256, 256, 3, 3])
layer3.1.conv2.bias  | nonzeros =       0 /     256             (  0.00%) | total_pruned =     256 | shape = torch.Size([256])
layer3.1.bn2.weight  | nonzeros =       9 /     256             (  3.52%) | total_pruned =     247 | shape = torch.Size([256])
layer3.1.bn2.bias    | nonzeros =     105 /     256             ( 41.02%) | total_pruned =     151 | shape = torch.Size([256])
layer4.0.conv1.weight | nonzeros =     381 / 1179648             (  0.03%) | total_pruned = 1179267 | shape = torch.Size([512, 256, 3, 3])
layer4.0.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn1.weight  | nonzeros =     104 /     512             ( 20.31%) | total_pruned =     408 | shape = torch.Size([512])
layer4.0.bn1.bias    | nonzeros =     182 /     512             ( 35.55%) | total_pruned =     330 | shape = torch.Size([512])
layer4.0.conv2.weight | nonzeros =     261 / 2359296             (  0.01%) | total_pruned = 2359035 | shape = torch.Size([512, 512, 3, 3])
layer4.0.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.bn2.weight  | nonzeros =      95 /     512             ( 18.55%) | total_pruned =     417 | shape = torch.Size([512])
layer4.0.bn2.bias    | nonzeros =     198 /     512             ( 38.67%) | total_pruned =     314 | shape = torch.Size([512])
layer4.0.shortcut.0.weight | nonzeros =      35 /  131072             (  0.03%) | total_pruned =  131037 | shape = torch.Size([512, 256, 1, 1])
layer4.0.shortcut.0.bias | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.0.shortcut.1.weight | nonzeros =      16 /     512             (  3.12%) | total_pruned =     496 | shape = torch.Size([512])
layer4.0.shortcut.1.bias | nonzeros =     209 /     512             ( 40.82%) | total_pruned =     303 | shape = torch.Size([512])
layer4.1.conv1.weight | nonzeros =     453 / 2359296             (  0.02%) | total_pruned = 2358843 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv1.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn1.weight  | nonzeros =     114 /     512             ( 22.27%) | total_pruned =     398 | shape = torch.Size([512])
layer4.1.bn1.bias    | nonzeros =      80 /     512             ( 15.62%) | total_pruned =     432 | shape = torch.Size([512])
layer4.1.conv2.weight | nonzeros =     357 / 2359296             (  0.02%) | total_pruned = 2358939 | shape = torch.Size([512, 512, 3, 3])
layer4.1.conv2.bias  | nonzeros =       0 /     512             (  0.00%) | total_pruned =     512 | shape = torch.Size([512])
layer4.1.bn2.weight  | nonzeros =     139 /     512             ( 27.15%) | total_pruned =     373 | shape = torch.Size([512])
layer4.1.bn2.bias    | nonzeros =     394 /     512             ( 76.95%) | total_pruned =     118 | shape = torch.Size([512])
linear.weight        | nonzeros =    2539 /    5120             ( 49.59%) | total_pruned =    2581 | shape = torch.Size([10, 512])
linear.bias          | nonzeros =      10 /      10             (100.00%) | total_pruned =       0 | shape = torch.Size([10])
alive: 8150, pruned : 11170612, total: 11178762, Compression rate :    1371.63x  ( 99.93% pruned)
Train Epoch: 56/200 Loss: 2.302546 Accuracy: 10.00 10.00 % Best test Accuracy: 10.00%
